{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-xolvinfo","title":"Welcome to xolv.info","text":"<p>This site provides a wiki-like approach to topic of interest to:</p> <ul> <li>Microsoft Azure</li> <li>Data science</li> <li>IBM Cloud</li> </ul> <p>References are included to original source material. </p>"},{"location":"#stack-choices-for-this-site","title":"Stack choices for this site","text":"<p>The technologies supported in this site are opinionated, in that they express choices that have been made through experience or recommendation. It is a mix of Microsoft and Open Source technologies and can change as needed by the project.</p> Category Primary technology Secondary technology Operating System WSL2 on Windows (Ubuntu) Red Hat Linux Container Container Device Interface - Podman Docker Container Orchestration OpenShift Container Platform Kubernetes Developer container orchestration OpenShift Local GPU NVIDIA CUDA cuDNN Scripting language Bash PowerShell Development language Python 3 (Conda) TypeScript React NodeJS C# Neural Networks PyTorch  Keras TensorFlow Cloud Azure IBM Cloud Dev environment Visual Studio CodeJupyter Notebook DevOps Azure DevOps Pipeline Cloud Native CI/CD Tekton  Jenkins GitOps Argo Documentation Markdown MKDocs WordPress <p>Python libraries:</p> <ul> <li>NumPy</li> <li>Pandas</li> <li>matplotlib</li> <li>seaborn statistical data visualization</li> <li>scikit-learn</li> <li>transformers</li> </ul> <p>Open Source datasets:</p> <ul> <li>Hugging Face datasets</li> </ul> <p>Large language models:</p> <ul> <li>[]</li> </ul>"},{"location":"portfolioarchitectures/","title":"Welcome to Cloud Native Architectures","text":"<p>This site describes reference architectures for Cloud Native applications. </p> <p>Cloud-native applications are designed to run on a cloud-based infrastructure. A cloud-native application takes advantage of cloud computing models to increase speed, flexibility, and quality and to reduce deployment risks.</p> <p></p> <p>Photo via Good Free Photos.</p> <p>Go beyond the basic concepts of Kubernetes and learn how to harness its full capabilities with patterns in this e-book from O\u2019Reilly and Red Hat.</p> <p>The speed of of app development in the digital era is vital to competitive advantage. Download this e-book and discover the 8 fundamental steps to cloud-native apps--and accelerate your time to market.</p> <p>[Cloud-native development is] DevOps. It\u2019s containers. It\u2019s microservices. It\u2019s hybrid cloud. It\u2019s really about changing your mindset to embrace what\u2019s important with regard to building application services quicker.</p> <p>Ashesh Badani, Senior vice president of cloud platforms, Red Hat</p>"},{"location":"ai-ml-datascience/","title":"Overview","text":""},{"location":"ai-ml-datascience/#aimldata-science","title":"AI/ML/Data Science","text":"<p>This section provides patterns and setup for Data Science, AI, and ML.</p>"},{"location":"ai-ml-datascience/#setup","title":"Setup","text":"<p>This section provides information about how to set up your local development environment for Jupyter Notebooks, containers, and use your GPU.</p>"},{"location":"ai-ml-datascience/#gpu","title":"GPU","text":"<p>See:</p> <ol> <li>Install NVIDIA GPU display driver</li> <li>How to set up GPU development for containers with Podman</li> </ol>"},{"location":"ai-ml-datascience/#jupyter-notebook","title":"Jupyter Notebook","text":"<p>See:</p> <ol> <li>Start Jupyter Notebook using environment</li> <li>Set up tokens and use as environment variables</li> </ol>"},{"location":"ai-ml-datascience/#containers","title":"Containers","text":"<p>See:</p> <ul> <li>About Jupyter Docker Stacks</li> <li>Set up development computer with Docker, Jupyter</li> <li>Set up your tokens, passwords, secrets for Podman</li> </ul>"},{"location":"ai-ml-datascience/#patterns","title":"Patterns","text":"<ul> <li>Data Analysis and Preparation</li> <li>Decision Tree</li> <li>Bagging, Random Forest, AdaBoost, GradientBoost</li> <li>Neural Network</li> </ul>"},{"location":"ai-ml-datascience/containers/jupyterdockerstacks/","title":"About Jupyter Docker Stacks","text":"<p>Jupyter Docker Stacks are a set of ready-to-run Docker images containing Jupyter applications and interactive computing tools. You can use a stack image to do any of the following (and more):</p> <ul> <li>Start a personal Jupyter Server with the JupyterLab frontend (default)</li> <li>Run JupyterLab for a team using JupyterHub</li> <li>Start a personal Jupyter Server with the Jupyter Notebook frontend in a local Docker container</li> <li>Write your own project Dockerfile</li> </ul> <p>Important</p> <p>Images hosted on Docker Hub are no longer updated. Use quay.io image.</p>"},{"location":"ai-ml-datascience/containers/jupyterdockerstacks/#where-to-find","title":"Where to find","text":"<p>All Jupyter Docker Stack images are available on Quay.io registry. We provide CUDA accelerated versions of images are available for tensorflow-notebook and pytorch-notebook.</p> <p>To use such an image, you have to specify a special prefix tag to the image: versioned CUDA prefix like <code>cuda11-</code> or <code>cuda12-</code> for <code>pytorch-notebook</code> or just <code>cuda-</code> for <code>tensorflow-notebook</code>.</p>"},{"location":"ai-ml-datascience/containers/jupyterdockerstacks/#run-images","title":"Run images","text":"<p>You will need:</p> <ul> <li>a  compatible NVIDIA GPU</li> <li>NVIDIA Linux driver installed</li> <li>add <code>--gpus all</code> (or <code>--gpus '\"device=all\"'</code>) flag to if you\u2019re using Docker add <code>--device 'nvidia.com/gpu=all'</code> flag if you\u2019re using Podman</li> </ul> <p>You can also enable GPU support on Windows using Docker or Podman.</p>"},{"location":"ai-ml-datascience/containers/jupyterdockerstacks/#core-stacks","title":"Core stacks","text":"<ul> <li>CUDA enabled variant</li> <li>jupyter/docker-stacks-foundation</li> <li>jupyter/base-notebook</li> <li>jupyter/minimal-notebook</li> <li>jupyter/r-notebook</li> <li>jupyter/julia-notebook</li> <li>jupyter/scipy-notebook</li> <li>jupyter/tensorflow-notebook</li> <li>jupyter/pytorch-notebook</li> <li>jupyter/datascience-notebook</li> <li>jupyter/pyspark-notebook</li> <li>jupyter/all-spark-notebook</li> </ul> <p>Source code for all of the notebooks are available on Github docker-stacks</p>"},{"location":"ai-ml-datascience/containers/jupyterdockerstacks/#running-on-openshift","title":"Running on OpenShift","text":"<p>Examples provides templates for deploying the Jupyter Project docker-stacks images to OpenShift.</p> <p>See OpenShift example.</p>"},{"location":"ai-ml-datascience/containers/jupyterdockerstacks/#source-to-image","title":"Source to image","text":"<p>Source-to-Image (S2I) is an open source project which provides a tool for creating container images. It works by taking a base image, injecting additional source code or files into a running container created from the base image, and running a builder script in the container to process the source code or files to prepare the new image.</p>"},{"location":"ai-ml-datascience/containers/jupyterdockerstacks/#reference","title":"Reference","text":"<ul> <li>Juptyer Docker Stacks</li> <li>CUDA enabled Jupyter Docker Image</li> </ul>"},{"location":"ai-ml-datascience/containers/setupdocker/","title":"Set up development computer with Docker, Jupyter","text":"<p>This article assumes you are using python to start a generic Jupyter notebook and then loading the requirements using requirements.txt file.</p> <p>We recommend using conda as base and adding your dependencies using environment.yml. The conda packaging provides a better dependency than using pip.</p> <p>| The first two bullet points of conda are really what make it advantageous over pip for many packages. Since pip installs from source, it can be painful to install things with it if you are unable to compile the source code (this is especially true on Windows, but it can even be true on Linux if the packages have some difficult C or FORTRAN library dependencies). conda installs from binary, meaning that someone (e.g., Continuum) has already done the hard work of compiling the package, and so the installation is easy.</p>"},{"location":"ai-ml-datascience/containers/setupdocker/#prerequisites","title":"Prerequisites","text":"<p>You will need:</p> <ul> <li>Docker installed</li> <li>Visual Studio Code installed</li> <li>WSL terminal</li> </ul> <p>If you want to run a GPU on your notebook, you will need the GPU drivers installed.</p>"},{"location":"ai-ml-datascience/containers/setupdocker/#project-setup","title":"Project setup","text":"<p>Start WSL and then create a directory where you want to put the project.</p> <pre><code>cd ~\nmkdir dockerproject\ncd dockerproject\ncode .\n</code></pre> <p>Create a readme.md for your project.</p>"},{"location":"ai-ml-datascience/containers/setupdocker/#test-docker","title":"Test Docker","text":"<p>Start Docker</p> <pre><code>sudo service docker start\n</code></pre> <p>Then try:</p> <pre><code>docker run hello-world\n</code></pre> <p>If you see</p> <pre><code>docker: permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Head \"http://%2Fvar%2Frun%2Fdocker.sock/_ping\": dial unix /var/run/docker.sock: connect: permission denied.\nSee 'docker run --help'.\n</code></pre> <p>Set up your user:</p> <pre><code># Create the docker group if it does not exist\nsudo groupadd docker\n\n# Add your user to the docker group.\nsudo usermod -aG docker $USER\n\n# Log in to the new docker group (to avoid having to log out / log in again; \n# but if not enough, try to reboot)\nnewgrp docker\n\n# Check docker again\ndocker run hello-world\n</code></pre> <p>If you still get an error, reboot:</p> <pre><code>reboot\n</code></pre>"},{"location":"ai-ml-datascience/containers/setupdocker/#create-the-dockerfile","title":"Create the dockerfile","text":"<p>Start <code>code .</code> and open Visual Studio Code.</p> <p>In the project directory, create a file named <code>dockerfile</code> with the following content:</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.8\n# Set the working directory to /app\nWORKDIR /app\n# Install Jupyter Notebook\nRUN pip install jupyter\n# Make port 8888 available to the world outside this container\nEXPOSE 8888\n# Define environment variable\nENV NAME World\n# Run Jupyter Notebook when the container launches\nCMD [\"jupyter\", \"notebook\", \"--ip=0.0.0.0\", \"--port=8888\", \"--no-browser\", \"--allow-root\"]\n</code></pre>"},{"location":"ai-ml-datascience/containers/setupdocker/#create-a-requirementstxt-file-as-needed","title":"Create a requirements.txt file (as needed)","text":"<p>Use Visual Studio Code to create a file named <code>requirements.txt</code> listing them.</p> <p>For example:</p> <pre><code>numpy==1.25.2\npandas==1.5.3\nmatplotlib==3.7.1\nseaborn==0.13.1\nscikit-learn==1.2.2\nsklearn-pandas==2.2.0\nmatplotlib\n</code></pre> <p>Then you can add the following code to your <code>Dockerfile</code> to run the requirements.</p> <pre><code># Set the working directory to /app\nWORKDIR /app\n# Copy the current directory contents into the container at /app\nCOPY . /app\n# Install any needed packages specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n</code></pre>"},{"location":"ai-ml-datascience/containers/setupdocker/#build-docker-image","title":"Build Docker image","text":"<p>Open a terminal, navigate to the project directory, and run the following command to build the Docker image:</p> <pre><code>docker build -t my-jupyter .\n</code></pre>"},{"location":"ai-ml-datascience/containers/setupdocker/#run-the-docker-container","title":"Run the Docker container","text":"<p>Once the image is built, run the command to run the container.</p> <pre><code>docker run -p 8888:8888 my-jupyter\n</code></pre> <p>Click on the https://127.0.0.1:8888 link. It has the code to log into Jupyter notebook</p> <p></p>"},{"location":"ai-ml-datascience/containers/setupdocker/#create-your-notebook-on-your-local-computer","title":"Create your notebook on your local computer","text":"<p>You are logged into the notebook from the startup directory on your development computer.</p> <p></p>"},{"location":"ai-ml-datascience/containers/setupdocker/#run-the-container-with-environment-variables","title":"Run the container with environment variables","text":"<p>In many cases, you may want to pass in a token or API key when running the notebook. First set up the variables and export. Then you can start the container:</p> <pre><code>docker run -p 8888:8888 -e MUSICPREFERENCE='rockandroll'  my-jupyter\n</code></pre> <p>Inside the notebook, to see the environment variable, run:</p> <pre><code>!TODO\n</code></pre>"},{"location":"ai-ml-datascience/containers/setupdocker/#tips","title":"Tips","text":"<ul> <li>If your project has specific Python packages, list them in the <code>requirements.txt</code> file.</li> <li>Customize the Dockerfile based on your project requirements.</li> <li>Ensure Docker is running and the Docker daemon is accessible.</li> <li>If port 8888 is unavailable, choose a different port in the <code>docker run</code> command (e.g., <code>-p 8889:8888</code>).</li> </ul>"},{"location":"ai-ml-datascience/containers/setupdocker/#summary-of-docker-commands","title":"Summary of Docker commands","text":"<p>Summary of Docker commands:</p> <ul> <li><code>docker build -t &lt;my-image-name&gt;</code> to build the Docker Image</li> <li><code>docker run -d --name &lt;my-container-name&gt; &lt;my-image-name&gt;</code> to build the Docker Container</li> <li><code>docker images</code> to display the list of created images</li> <li><code>docker ps -a</code> to show the list of containers</li> <li><code>docker rmi &lt;my-image-id&gt;</code> to remove an image</li> <li><code>docker stop &lt;my-container-id&gt;</code> to stop a running container</li> <li><code>docker rm &lt;my-container-id&gt;</code> to remove a stopped container</li> </ul>"},{"location":"ai-ml-datascience/containers/setupdocker/#reference","title":"Reference","text":"<p>See:</p> <ul> <li>Setting Up and Running Jupyter Notebook in a Docker Container</li> <li>How to fix docker: Got permission denied issue</li> </ul>"},{"location":"ai-ml-datascience/containers/setupdockersecrets/","title":"Set up your secrets for Docker","text":"<p>You can provide sensitive data to your Docker container, such as passwords, API keys, and certificates.</p> <p>There are several ways to pass in the secret.</p> <ul> <li>Use Docker Secrets and Docker Swarm. If you want to do this, see Gettins started with swarm mode and then How to Handle Secrets in Docker. You'll need Windows Server to do this.</li> <li>Use Docker Compose. Docker Compose provides an effective solution for managing secrets for organizations handling sensitive data such as passwords or API keys. You can read your secrets from an external file (like a TXT file).</li> </ul>"},{"location":"ai-ml-datascience/containers/setupdockersecrets/#set-up-docker-and-docker-compose","title":"Set up Docker and Docker Compose","text":"<ol> <li>See Docker and Docker Compose on Windows Subsystem for Linux (WSL)</li> </ol>"},{"location":"ai-ml-datascience/containers/setupdockersecrets/#references","title":"References","text":"<ul> <li>Docker Secrets: An Introductory Guide with Examples</li> </ul>"},{"location":"ai-ml-datascience/containers/setupsecretsforkubernetes/","title":"Set up your secrets for Kubernetes","text":""},{"location":"ai-ml-datascience/containers/setupsecretsforkubernetes/#references","title":"References","text":"<ul> <li>How to Securely Create, Edit, and Update Your Kubernetes Secrets</li> </ul>"},{"location":"ai-ml-datascience/containers/setuptokensforcontainers/","title":"Set up your tokens, passwords, secrets for Podman","text":"<p>How do you pass in the password to a database or a token to access an online service? You could simply a file when running the container and store the credentials there?</p> <p>If the container is exported into the image, the credentials would be exported as well. Anyone who has control over the image would be able to access the database.</p> <p>Or you can pass the credentials using the CLI. But then you need to enter the data everytime you run the container.</p> <p>In this article, you learn how to set up your token and pass it into a running container. First, Podman, then Docker.</p>"},{"location":"ai-ml-datascience/containers/setuptokensforcontainers/#set-up-your-tokens-for-podman","title":"Set up your tokens for Podman","text":"<p>You can use <code>podman secret</code> alone with their sub-commands <code>create, rm, ls</code>, and <code>inspect</code>. And you can use the <code>--secret</code> flag to inject the secret into the container.</p> <p>Podman secrets are available in Podman 3.1.0. Podman 4.3.0 added support for Kubernetes secrets on top of Podman secrets using the <code>podman kube play</code> command.</p>"},{"location":"ai-ml-datascience/containers/setuptokensforcontainers/#set-up-inspect-remove-the-secret","title":"Set up, inspect, remove the secret","text":"<p>Let's begin by setting up your secret data into a file named <code>.secretfile</code>. And to name of the secret is <code>secretname</code></p> <pre><code>echo \"supersecretdata\" &gt; .secretfile\npodman secret create secretname .secretfile\n</code></pre> <p>Next use the <code>podman secret inspect</code> command to see the metadata on the secret in Podman.</p> <pre><code>podman secret inspect secretname\n</code></pre> <p>Returns</p> <pre><code>[\n    {\n        \"ID\": \"94852d928258719b29d24323e\",\n        \"CreatedAt\": \"2024-08-17T08:16:36.234868975-07:00\",\n        \"UpdatedAt\": \"2024-08-17T08:16:36.234868975-07:00\",\n        \"Spec\": {\n            \"Name\": \"secretname\",\n            \"Driver\": {\n                \"Name\": \"file\",\n                \"Options\": {\n                    \"path\": \"/home/bruce/.local/share/containers/storage/secrets/filedriver\"\n                }\n            }\n        }\n    }\n]\n</code></pre> <p>You can list the secrets:</p> <p><pre><code>podman secret ls\n</code></pre> And receive</p> <pre><code>ID                         NAME        DRIVER      CREATED         UPDATED\n94852d928258719b29d24323e  secretname  file        44 seconds ago  44 seconds ago\n</code></pre> <p>and remove them</p> <pre><code>podman secret rm secretname\n</code></pre>"},{"location":"ai-ml-datascience/containers/setuptokensforcontainers/#pass-the-secret-into-the-container","title":"Pass the secret into the container","text":"<p>You pass in the secret name using the <code>--secret</code> flag. Then inside the container, the secret is in a file named <code>/run/secrets/secretname</code>. You can use the <code>--secret</code> flag multiple times to add numerous secrets to the container.</p> <p>In the following command, you run podman in a container named <code>acontainer</code>. The container uses the <code>alpine</code> image. And then runs the <code>cat</code> command to see the value in the <code>/run/secrets/secretname</code> file. The <code>-it</code> terminates the container after it runs.</p> <pre><code>podman run -it --secret secretname --name acontainer alpine cat /run/secrets/secretname\n</code></pre> <p>Secrets are not committed to an image</p> <p>Secrets will not be committed to an image or exported with a <code>podman commit</code> or a <code>podman export</code> command. This prevents sensitive information from accidentally being pushed to a public registry or given to the wrong person.</p>"},{"location":"ai-ml-datascience/containers/setuptokensforcontainers/#podman-cheatsheet","title":"Podman Cheatsheet","text":"<p>Download Podman Cheat Sheet</p>"},{"location":"ai-ml-datascience/containers/setuptokensforcontainers/#references","title":"References","text":"<ul> <li>Exploring the new Podman secret command</li> </ul>"},{"location":"ai-ml-datascience/gpu/installpytorch/","title":"Installing PyTorch","text":"<p>See Get Started Locally.</p>"},{"location":"ai-ml-datascience/gpu/installpytorch/#prerequisites","title":"Prerequisites","text":"<p>You will need to know the CUDA version you installed.</p>"},{"location":"ai-ml-datascience/gpu/installpytorch/#getting-started","title":"Getting started","text":"<p>See Tensors</p>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/","title":"How to set up GPU development for containers with Podman","text":"<p>In this article, learn how to set up your GPU development environment to run inside Podman. Most users can simply alias Docker to Podman (<code>alias docker=podman</code>) without any problems.</p> <p>Podman supports the Container Device Interface.</p> <p>The following is a compilation from sources, bug reports, GitHub repos that worked when I tried it on my laptop. Special thanks to henrymai who's README was significant help.</p>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#definitions","title":"Definitions","text":"<ul> <li>Podman is a utility so you can create and maintain containers. Podman is a daemonless, open source, Linux native tool designed to make it easy to find, run, build, share and deploy applications using Open Containers Initiative (OCI) Containers and Container Images. See What is Podman,</li> <li>CDI is an open specification for container runtimes that abstracts what access to a device, such as an NVIDIA GPU, means, and standardizes access across container runtimes. Popular container runtimes can read and process the specification to ensure that a device is available in a container. CDI simplifies adding support for devices such as NVIDIA GPUs because the specification is applicable to all container runtimes that support CDI.</li> </ul>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#prerequisites","title":"Prerequisites","text":"<p>You will need to have:</p> <ul> <li>Laptop or desktop with GPU</li> <li>Device drivers installed</li> <li>WSL installed. (I installed a fresh distribution of Ubuntu.)</li> <li>Visual Studio Code (or other Linux text editor)</li> <li>The username</li> </ul> <p>To check the version you have installed use:</p> <pre><code>podman --version\n</code></pre> <p>See Install NVIDIA GPU display driver for details.</p>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#upgrade-ubuntu","title":"Upgrade Ubuntu","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade\n\n# get release version\nubuntu_release=lsb_release -r\n$ubuntu_release\n</code></pre>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#install-podman","title":"Install podman","text":"<p>You will also need Podman installed. Start WSL, then run</p> <pre><code>sudo apt remove docker docker-engine docker.io containerd runc\nsudo apt install --no-install-recommends apt-transport-https ca-certificates curl gnupg2\nsudo apt -y install podman\npodman version\n</code></pre> <p>You should see something like this:</p> <pre><code>WARN[0000] \"/\" is not a shared mount, this could cause issues or missing mounts with rootless containers\nClient:       Podman Engine\nVersion:      4.9.3\nAPI Version:  4.9.3\nGo Version:   go1.22.2\nBuilt:        Wed Dec 31 16:00:00 1969\nOS/Arch:      linux/amd64\n</code></pre> <p>Important</p> <p>The version of Podman should return a version greater than 4.9. Updating the Windows host does not update the version of Podman running in your distribution. In my case, I created a new distribution to get the latest version (as of the time of this writing).</p>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#start-podman","title":"Start Podman","text":"<p>To start Podman the first time, run:</p> <pre><code>podman machine init\npodman machine start\n</code></pre>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#add-dockerio-registry","title":"Add docker.io registry","text":"<pre><code>echo 'unqualified-search-registries = [\"docker.io\"]' | sudo tee /etc/containers/registries.conf\n</code></pre>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#run-a-container-from-podman","title":"Run a container from Podman","text":"<p>Run a sample container in Podman.</p> <pre><code>podman run -it --rm busybox\n</code></pre> <p>You should see:</p> <pre><code>Resolved \"busybox\" as an alias (/etc/containers/registries.conf.d/shortnames.conf)\nTrying to pull docker.io/library/busybox:latest...\nGetting image source signatures\nCopying blob ec562eabd705 done   |\nCopying config 65ad0d468e done   |\nWriting manifest to image destination\n/ #\n</code></pre> <p>Type <code>exit</code>.</p> <pre><code>podman run ubi8-micro date\n</code></pre> <p>Or</p> <pre><code>podman run ubi8-micro date\n</code></pre> <p>You should see today's date.</p>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#install-toolkit-or-toolkit-base","title":"Install Toolkit or Toolkit Base","text":"<p>You will need to install either the NVIDIA Container Toolkit or you installed the <code>nvidia-container-toolkit-base</code> package.</p> <p>Get the Ubuntu version</p> <pre><code>. /etc/os-release\necho \"$VERSION_ID\"\n</code></pre> <p>To install the NVIDIA container toolkit, run:</p> <ol> <li>Configure the production repository:</li> </ol> <p><pre><code>distribution=ubuntu + echo \"$VERSION_ID\"\ncurl -s -L https://nvidia.github.io/libnvidia-container/gpgkey | sudo apt-key add - &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n</code></pre> 2. Update the packages list from the repository:</p> <p><pre><code>sudo apt-get update\n</code></pre> 3. Install the NVIDIA Container Toolkit packages:</p> <pre><code>sudo apt-get install nvidia-container-toolkit\n</code></pre> <p>You will see the Container Toolkit install beginning with something like this:</p> <pre><code>Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  libnvidia-container-tools libnvidia-container1 nvidia-container-toolkit-base\nThe following NEW packages will be installed:\n  libnvidia-container-tools libnvidia-container1 nvidia-container-toolkit nvidia-container-toolkit-base\n0 upgraded, 4 newly installed, 0 to remove and 3 not upgraded.\n</code></pre> <p>For more information, see Installing the NVIDIA Container Toolkit.</p>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#set-the-rootless-configuration-for-container-device-interface","title":"Set the rootless configuration for Container Device Interface","text":"<p>Set the NVIDIA container runtime to run as rootless. This sets you up for Podman's ability to manage containers without root access.</p> <ol> <li>Generate the CDI specification file:</li> </ol> <pre><code>sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\n</code></pre> <p>You will see some output similar to:</p> <pre><code>INFO[0000] Using /usr/lib/wsl/lib/libnvidia-ml.so.1\nINFO[0000] Auto-detected mode as 'wsl'\nINFO[0000] Selecting /dev/dxg as /dev/dxg\nINFO[0000] Using WSL driver store paths: [/usr/lib/wsl/drivers/nvlti.inf_amd64_70bff6400ff3c791 /usr/lib/wsl/drivers/u0390955.inf_amd64_53cfc5cd131b06d4]\n...\nINFO[0000] Selecting /usr/lib/wsl/drivers/nvlti.inf_amd64_70bff6400ff3c791/nvidia-smi as /usr/lib/wsl/drivers/nvlti.inf_amd64_70bff6400ff3c791/nvidia-smi\nINFO[0000] Generated CDI spec with version 0.8.0\n</code></pre> <ol> <li>Check the names of the generated devices:</li> </ol> <pre><code>nvidia-ctk cdi list\n</code></pre> <p>You should see something similar to:</p> <pre><code>INFO[0000] Found 1 CDI devices\nnvidia.com/gpu=all\n</code></pre> <ol> <li>Run a workload with CDI</li> </ol> <pre><code>podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable ubuntu nvidia-smi -L\n</code></pre> <p>You should see something similar to:\"</p> <pre><code>GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU (UUID: GPU-fbeb177f-f196-93e0-b215-12b7c899dc82)\n</code></pre>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#running-gpu-workloads","title":"Running GPU workloads","text":"<p>When it comes time to run you workloads, you will use something simlar to:</p> <p><pre><code>podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable ubuntu nvidia-smi\n</code></pre> The important parameters:</p> <ul> <li><code>--device nvidia.com/gpu=all</code> Add a host device to the container. </li> <li><code>--security-opt=label=disable</code> Turn off label separation for the container.</li> <li><code>-p</code> port forwarding.</li> </ul> <p>If you have more than one GPU available, you can specify which one using:</p> <pre><code>podman run --rm \\\n    --device nvidia.com/gpu=0 \\\n    --device nvidia.com/gpu=1:0 \\\n    --security-opt=label=disable \\\n    ubuntu nvidia-smi -L\n</code></pre>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#increase-memlock-and-stack-ulimits-optional","title":"Increase memlock and stack ulimits [optional]","text":"<p>This is necessary otherwise any reasonable sized training run will hit these limits immediately.</p> <p>Use:</p> <pre><code>code /etc/security/limits.conf\n</code></pre> <p>Assuming your username is <code>someuser</code>:</p> <pre><code>someuser soft memlock unlimited\nsomeuser hard memlock unlimited\nsomeuser soft stack 65536\nsomeuser hard stack 65536\n</code></pre>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#to-build-your-container","title":"To build your container","text":"<p>Look up the latest version of <code>nvidia/cuda</code> on docker.io. See nvidia/cuda on DockerHub. In my case, I found <code>cuda:12.6.0-cudnn-devel-ubuntu20.04</code></p> <pre><code>podman pull nvidia/cuda:12.6.0-cudnn-devel-ubuntu20.04\npodman run --rm --device nvidia.com/gpu=all --security-opt=label=disable nvidia/cuda:12.6.0-cudnn-devel-ubuntu20.04 nvidia-smi\n</code></pre> <p>The command requests the full GPU with index 0 and the first MIG device on GPU 1. The output should show only the UUIDs of the requested devices.</p> <p></p>"},{"location":"ai-ml-datascience/gpu/setupmlonwindows/#resources","title":"Resources","text":"<p>See:</p> <ul> <li>Get started with GPU acceleration for ML in WSL</li> <li>Podman for Windows</li> <li>Installing the NVIDIA Container Toolkit</li> <li>Support for Container Device Interface</li> <li>Setup podman for WSL2 with cuda support (rootless)</li> </ul>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/","title":"Install NVIDIA GPU display driver","text":"<p>In this article, learn how to set up your Windows laptop or desktop, using Windows Subsystem for Linux (WSL). You will also set up your graphics card to develop your machine learning application on your NVIDIA GPU.</p>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#definition","title":"Definition","text":"<ul> <li>GPU is an abbreviation for graphics processing unit</li> <li>CUDA is a proprietary parallel computing platform and application programming interface (API) for software to use certain types of graphics processing units (GPUs) for accelerated general-purpose processing. CUDA is designed to work with programming languages such as C, C++, Fortran and Python. </li> </ul>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#prerequisites","title":"Prerequisites","text":"<p>You will need:</p> <ul> <li>Laptop or desktop development computer with an NVIDIA GPU. See how to check the GPU hardware in the following step.</li> <li>Ensure you are running Windows 11 or Windows 10, version 21H2 or higher.</li> </ul>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#find-out-your-gpu","title":"Find out your GPU","text":"<p>Your computer\u2019s GPU, or ,  helps your PC or laptop handle visuals like graphics and videos.</p> <p>Windows Task Manager, System Information, PowerShell, and DxDiag are built-in tools to check your GPU on Windows.</p> <p>Start Task Manager from your start menu. Click Performance tab. You can see the GPUs installed on your computer.</p> <p></p> <p>You may have more than one display. </p> <p>For gamers: if you have multiple GPUs in your system \u2014 for example, as in a laptop with a low-power Intel GPU for use on battery power and a high-power NVIDIA GPU for use while plugged in and gaming \u2014 you can control which GPU a game uses from Windows 10's Settings app. These controls are also built into the NVIDIA Control Panel.</p> <p>You can also check by running PowerShell command:</p> <pre><code>Get-CimInstance win32_VideoController\n</code></pre> <p>You will need to know the GPU information to select the right driver for your computer.</p>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#hardware-configuration-in-edge","title":"Hardware configuration in Edge","text":"<p>You can see the current hardware configuration and driver support in Edge. Start your browser and type in this URL:</p> <pre><code>edge://gpu\n</code></pre>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#install-the-gpu-driver","title":"Install the GPU Driver","text":"<p>Download and install the NVIDIA CUDA enabled driver for WSL to use with your existing CUDA ML workflows. </p> <p></p> <p>For more info about which driver to install, see:</p> <ul> <li>Getting Started with CUDA on WSL 2</li> <li>CUDA on Windows Subsystem for Linux (WSL)</li> </ul>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#install-or-update-wsl-2","title":"Install or Update WSL 2","text":"<p>Launch your preferred Windows Terminal / Command Prompt / Powershell and install WSL:</p> <pre><code>wsl.exe --install\n</code></pre> <p>Ensure you have the latest WSL kernel:</p> <pre><code>wsl.exe --update\n</code></pre>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#start-wsl","title":"Start WSL","text":"<p>From a Windows terminal, enter WSL:</p> <pre><code>C:\\&gt; wsl.exe\n</code></pre>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#test-the-driver-installation","title":"Test the driver installation","text":"<p>To test the driver installation:</p> <p><pre><code>nvidia-smi -L\n</code></pre> Returns something similar to:</p> <pre><code>GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU (UUID: GPU-fbeb177f-f196-93e0-b215-12b7c899dc82)\n</code></pre> <p>OR for more fun</p> <pre><code>nvidia-smi\n</code></pre> <p>to see:</p> <p></p>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#test-your-docker-container","title":"Test your Docker container","text":"<p>You will need to have set up Docker. Run this command to start your GPU with NVIDIA NGC TensorFlow container.</p> <pre><code>docker run --gpus all -it --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 nvcr.io/nvidia/tensorflow:20.03-tf2-py3\n</code></pre>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#try-a-pre-trained-model","title":"Try a pre-trained model","text":"<p>Pull the Tensorflow Docker image:</p> <p><pre><code>docker pull tensorflow/tensorflow:latest-gpu-jupyter\ndocker run -it --rm -p 8888:8888 docker pull tensorflow/tensorflow:latest-gpu-jupyter\n</code></pre> Start jupyter somehow.</p>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#run-gpu-enabled-image","title":"Run GPU-enabled image","text":"<pre><code>lspci | grep -i nvidia\n</code></pre> <pre><code>docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu \\\n   python -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n</code></pre>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#install-cudnn-library","title":"Install cuDNN Library","text":"<p>cuDNN (CUDA Deep Neural Network) is a library developed by NVIDIA that provides optimized primitives for deep neural networks. It can significantly speed up the training and inference of deep learning models on GPUs. To use cuDNN with Jupyter Notebook, you need to download and install the cuDNN library from NVIDIA\u2019s website: https://developer.nvidia.com/cudnn.</p> <p></p>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#next-steps","title":"Next steps","text":"<p>See: - NVIDIA CUDA on WSL User Guide - Start using your exisiting Linux workflows through NVIDIA Docker, or by installing PyTorch or TensorFlow inside WSL. - Set up Jupyter Notebook - Set up for Docker - Set up for Podman</p>"},{"location":"ai-ml-datascience/gpu/setupnvidiadriver/#references","title":"References","text":"<p>See:</p> <ul> <li>Getting Started with CUDA on WSL 2</li> <li>How to Check What Graphics Card (GPU) Is in Your PC</li> <li>Microsoft documentation. Enable NVIDIA CUDA on WSL</li> <li>Red Hat documentation: Installing Podman and the NVIDIA Container Toolkit</li> <li>Get started with GPU acceleration for ML in WSL</li> </ul> <p>Video:</p> <p>CUDA Tutorials I Installing CUDA Toolkit on Windows and WSL</p>"},{"location":"ai-ml-datascience/llm/largelanguage/","title":"Understanding Transformers: Large Language Models","text":"<p>Large Language Models (LLMs) like those belonging to the GPT, and BERT family have demonstrated a significant advancement over earlier neural network architectures such as Recurrent Neural Networks (RNNs) in the field of Natural Language Processing (NLP). </p> <p>Large language models can be defined as AI/machine learning models that try to solve NLP tasks related to:</p> <ul> <li>text generation</li> <li>summarization</li> <li>translation</li> <li>question &amp; answering (Q&amp;A)</li> </ul> <p>The following diagram from  Humza Naveed, et.al. paper A Comprehensive Overview of Large Language Model shows the wide variety of tasks that can be solved using large language models.</p> <p></p> <p>Large language models are 'large' because they are pre-trained with a large number of parameters (100M+) on large corpora of text to process/understand and generate natural language text for a wide variety of NLP tasks. </p> <p>The LLM family includes:</p> <ul> <li>BERT (NLU \u2013 Natural language understanding). BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained Natural Language Process model developed by Google. It is based on the Transformer architecture, which uses self-attention mechanisms to capture relationships between words in a model. It\u2019s bidirectional because it considers both the right and left context of each word when encoding its representation. Also, BERT has been pre-trained on a large text corpus and can be fine-tuned on specific NLP tasks like classification and question answering. </li> <li>GPT (NLG \u2013 natural language generation). GPTs can produce new, coherent text based on the patterns learned from massive datasets. They are \u201cpre-trained\u201d because they undergo an initial training phase on vast amounts of text data. This allows them to acquire a broad knowledge base before being fine-tuned for specific tasks.</li> <li>T5 (Summarization)</li> </ul> <p>The specific LLM models such as OpenAI\u2019s models (GPT3.5, GPT-4 \u2013 Billions of parameters), PaLM2, Llama 2, etc demonstrate exceptional performance in various NLP / text processing tasks mentioned before. Some of these LLMs are open-sourced (Llama 2) while other ain\u2019t (such as ChatGPT models).</p>"},{"location":"ai-ml-datascience/llm/largelanguage/#architecture-of-attention-mechanism","title":"Architecture of attention mechanism","text":"<p>LLMs are built on neural network architectures, particularly the transformer architecture. The article, Attention is all you need from Google, provides the how the architecture uses using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.</p> <p></p> <p>The transformer architecture consists of two main components: the encoder network and the decoder network.</p>"},{"location":"ai-ml-datascience/llm/largelanguage/#encoder-network","title":"Encoder network","text":"<p>Encoder network (the left side of the architecture shown in the previous section) takes an input sequence and produces a sequence of hidden states. For example, the encoder network takes a sequence of words in the source language, such as English.</p> <ul> <li>Input sequence example. For example, consider the sentence \"The cat sat on the mat\".</li> <li>Input Processing. The encoder processes this sequence word by word. Each word is first converted into a numerical form (like a vector) that represents its meaning. This is typically done using word embeddings.</li> <li>Sequence of Hidden States. As it processes each word, the encoder uses self-attention mechanisms to understand the context of each word about every other word in the sentence. This results in a sequence of hidden states, each state being a vector that encodes the contextual information of a word in the sentence. For instance, the hidden state for \"cat\" would capture not just the meaning of \"cat\" but also its relationship to \"sat\", \"on\", \"the\", \"mat\" in the sentence.</li> </ul>"},{"location":"ai-ml-datascience/llm/largelanguage/#decoder-network","title":"Decoder network","text":"<p>Decoder network (the right side of the architecture shown in the previous section) takes a target sequence and uses the encoder's output to generate a sequence of predictions.</p> <ul> <li>Translation task example. For example, let's say, the decoder network aims to generate the translation in the target language, say French. It starts with a starting token (like <code>&lt;start&gt;</code>) and generates one word at a time.</li> <li>Using Encoder's Output. The decoder uses the sequence of hidden states produced by the encoder to understand the context of the source sentence.</li> <li>Sequence of Predictions. For each word it generates, the decoder uses cross-attention mechanisms to focus on different parts of the encoder\u2019s output. This helps the decoder figure out which words in the English sentence are most relevant to the current word it is trying to predict in French. For example, when translating \"The cat sat on the mat\" to French, the decoder might focus on the hidden state for \"cat\" when trying to predict the French word for 'cat'.</li> <li>Iterative Process. This process is iterative. With each word generated in French, the decoder updates its state and makes the next prediction, until the entire sentence is translated.</li> </ul>"},{"location":"ai-ml-datascience/llm/largelanguage/#use-cases-summary","title":"Use cases summary","text":"<p>The following table summarizes the architecture types and some use cases for each of the network types.</p> Architecture Types Examples Use cases Encoder-only transformers BERT (Google) Sentence classification, named entity recognition, extractive question answering Encoder-decoder transformers T5 (Google) Summarization, translation, question answering Decoder-only transformers GPT Series (OpenAI) Text generation"},{"location":"ai-ml-datascience/llm/largelanguage/#llm-types","title":"LLM types","text":"<p>There are three different LLM types based on the above transformer architecture using encoder, decoder, or both networks.</p>"},{"location":"ai-ml-datascience/llm/largelanguage/#autoregressive-language-models-eg-gpt","title":"Autoregressive Language Models (e.g., GPT)","text":"<p>Autoregressive models primarily use the decoder part of the Transformer architecture, making them well-suited for natural language generation (NLG) tasks like text summarization, generation, etc. These models generate text by predicting the next word in a sequence given the previous words. They are trained to maximize the likelihood of each word in the training dataset, given its context. </p> <p>The most well-known example of an autoregressive language model is OpenAI\u2019s GPT (Generative Pre-trained Transformer) series, with GPT-4o being the latest and most powerful iteration. Autoregressive models based on decoder networks primarily leverage layers related to self-attention, cross-attention mechanisms, and feed-forward networks as part of their neural network architecture. </p>"},{"location":"ai-ml-datascience/llm/largelanguage/#example","title":"Example","text":"<ul> <li>Input: \"Introducing new smartphone, the UltraPhone 3000, which is designed to\"</li> <li>The generated text can be: \"redefine your mobile experience with its cutting-edge technology and unparalleled performance.\"</li> </ul>"},{"location":"ai-ml-datascience/llm/largelanguage/#autoencoding-language-models-eg-bert","title":"Autoencoding Language Models (e.g., BERT)","text":"<p>Autoencoding models, on the other hand, mainly use the encoder part of the Transformer. It\u2019s designed for tasks like classification, question answering, etc. These models learn to generate a fixed-size vector representation (also called embeddings) of input text by reconstructing the original input from a masked or corrupted version of it. They are trained to predict missing or masked words in the input text by leveraging the surrounding context. </p> <p>BERT (Bidirectional Encoder Representations from Transformers), developed by Google, is one of the most famous autoencoding language models. It can be fine-tuned for a variety of NLP tasks, such as sentiment analysis, named entity recognition and question answering. Autoencoding models based on encoder networks primarily leverage layers related to self-attention mechanisms and feed-forward networks as part of their neural network architecture. </p>"},{"location":"ai-ml-datascience/llm/largelanguage/#example_1","title":"Example","text":"<p>Fill in the missing or masked words in a sentence, producing a semantically meaningful and complete sentence.</p> <p>Let\u2019s say the input to the autoencoding model is the following:</p> <p>\"The latest superhero movie had an _______ storyline, but the visual effects were _______.\"</p> <p>The completed text will look like the following:</p> <p>\"The latest superhero movie had a decent storyline, but the visual effects were mind-blowing.\"</p>"},{"location":"ai-ml-datascience/llm/largelanguage/#combination-of-autoencoding-and-autoregressive","title":"Combination of autoencoding and autoregressive","text":"<p>such as the T5 (Text-to-Text Transfer Transformer) model. Developed by Google in 2020, T5 LLM can perform natural language understanding (NLU) and natural language generation (NLG). T5 LLM can be understood as a pure transformer using both encoder and decoder networks.</p>"},{"location":"ai-ml-datascience/llm/largelanguage/#dive-deeper","title":"Dive deeper","text":"<p>See Attention is All You Need: Demystifying the Transformer Revolution in NLP for a deep dive into transformers.</p>"},{"location":"ai-ml-datascience/llm/largelanguage/#references","title":"References","text":"<ul> <li>Understanding Transformers: A Deep Dive into NLP\u2019s Core Technology</li> <li>A Comprehensive Overview of Large Language Model</li> <li>Large Language Models (LLMs): Types, Examples</li> <li>Attention is All You Need: Demystifying the Transformer Revolution in NLP</li> </ul>"},{"location":"ai-ml-datascience/llm/largelanguage/#books","title":"Books","text":"<ul> <li>Natural Language Processing with Transformers</li> <li>Generative Deep Learning: Teaching Machines to Paint, Write, Compose, and Play</li> <li>Attention is all you need</li> </ul>"},{"location":"ai-ml-datascience/llm/llm-tooling/","title":"LLM tooling","text":"<ul> <li>Hugging Face RAG Transformer: Provides a comprehensive collection of pre-trained models, including RAG.</li> <li>Vellum.ai: Vellum is a development platform for building LLM apps with tools for prompt engineering, semantic search, version control, testing, and monitoring.</li> <li>Elasticsearch: A A powerful search engine, ideal for the retrieval phase in RAG.</li> <li>FAISS (Facebook AI Similarity Search): Efficient for similarity search in large datasets, useful for retrieval.</li> <li>Dense Passage Retrieval (DPR): Optimized for retrieving relevant passages from extensive text corpora.</li> <li>Haystack: An NLP framework that simplifies the building of search systems, integrating well with Elasticsearch and DPR.</li> <li>PyTorch and TensorFlow: Foundational deep learning frameworks for developing and training RAG models.</li> <li>ColBERT: A BERT-based ranking model for high-precision retrieval.</li> <li>Apache Solr: An open-source search platform, an alternative to Elasticsearch for retrieval.</li> <li>Pinecone: A scalable vector database optimized for machine learning applications.Ideal for vector-based similarity search, playing a crucial role in the retrieval phase of RAG.</li> <li>Langchain: A toolkit designed to integrate language models with external knowledge sources. Bridges the gap between language models and external data, useful for both the retrieval and augmentation stages in RAG.</li> <li>LlamaIndex: Specializes in indexing and retrieving information, aiding the retrieval stage of RAG. Facilitates efficient indexing, making it suitable for applications requiring rapid and relevant data retrieval.</li> </ul>"},{"location":"ai-ml-datascience/llm/llm-tooling/#langchain-vs-llamaindex","title":"LangChain vs LlamaIndex","text":"<p>Another big choice is with Langchain and LlamaIndex. Datacamp article LangChain vs LlamaIndex: A Detailed Comparison provides a good overview of which works best based on your use case. They write:</p> Feature Langchain LlamaIndex Data indexing LangChain provides a modular and customizable approach to data indexing with complex chains of operations, integrating multiple tools and LLM calls. LlamaIndex transforms various types of data, such as unstructured text documents and structured database records, into numerical embeddings that capture their semantic meaning. Retrieval algorithms LangChain integrates retrieval algorithms with LLMs to produce context-aware outputs. LangChain can dynamically retrieve and process relevant information based on the context of the user\u2019s input, which is useful for interactive applications like chatbots. LlamaIndex is optimized for retrieval, using algorithms to rank documents based on their semantic similarity to perform a query. Customization LangChain, however, provides extensive customization options. It supports the creation of complex workflows for highly tailored applications with specific requirements. LlamaIndex offers limited customization focused on indexing and retrieval tasks. Its design is optimized for these specific functions, providing high accuracy. Context retention LangChain excels in context retention, which is crucial for applications where retaining information from previous interactions and coherent and contextually relevant responses over long conversations are crucial. LlamaIndex provides basic context retention capabilities suitable for simple search and retrieval tasks. It can manage the context of queries to some extent but is not designed to maintain long interactions. Use cases LangChain is better suited for applications requiring complex interaction and content generation, such as customer support, code documentation, and various NLP tasks. LlamaIndex is ideal for internal search systems, knowledge management, and enterprise solutions where accurate information retrieval is critical. Performance LangChain is efficient in handling complex data structures that can operate inside its modular architecture for sophisticated workflows. LlamaIndex is optimized for speed and accuracy; the fast retrieval of relevant information. Optimization is crucial for handling large volumes of data and quick responses. Lifecycle management LangChain offers evaluation suite, LangSmith, tools for testing, debugging, and optimizing LLM applications, ensuring that applications perform well under real-world conditions. LlamaIndex integrates with debugging and monitoring tools to facilitate lifecycle management. Integration helps tracking the performance and reliability of applications by providing insights and tools for troubleshooting."},{"location":"ai-ml-datascience/llm/llm-tooling/#references","title":"References","text":"<ul> <li>RAG 101: What is RAG and why does it matter?</li> </ul>"},{"location":"ai-ml-datascience/llm/prompt-engineering/","title":"Prompt engineering","text":"<p>Generative artificial intelligence (AI) systems are designed to generate specific outputs based on the quality of provided prompts. Prompt engineering helps generative AI models better comprehend and respond to a wide range of queries, from the simple to the highly technical.</p> <p>The basic rule is that good prompts equal good results. Generative AI relies on the iterative refinement of different prompt engineering techniques to effectively learn from diverse input data and adapt to minimize biases, confusion and produce more accurate responses.</p> <p>Prompt engineering</p> <p>Prompt engineering is the art of communicating with a generative large language model. --ChatGPT</p> <p>In this article, you will learn more about how researchers have described what you can do in prompt engineering. You will find tips, tricks, and suggested ways to building your prompts.</p>"},{"location":"ai-ml-datascience/llm/prompt-engineering/#guidelines","title":"Guidelines","text":"<p>A paper, Principled Instructions Are All You Need for Questioning LLaMA-\u00bd, GPT-3.5/4 introduces 26 guiding principles designed to streamline the process of querying and prompting large language models.</p> <p>The following illustration from the article shows:</p> <p></p>"},{"location":"ai-ml-datascience/llm/prompt-engineering/#prompt-principles","title":"Prompt principles","text":"<p>The study tabulated 26 ordered prompt principles, which can further be categorized into five distinct categories:</p> <ul> <li>Prompt Structure and Clarity: Integrate the intended audience in the prompt.</li> <li>Specificity and Information: Implement example-driven prompting (Use few-shot prompting)</li> <li>User Interaction and Engagement: Allow the model to ask precise details and requirements until it has enough information to provide the needed response</li> <li>Content and Language Style: Instruct the tone and style of response</li> <li>Complex Tasks and Coding Prompts: Break down complex tasks into a sequence of simpler steps as prompts.</li> </ul> <p></p>"},{"location":"ai-ml-datascience/llm/prompt-engineering/#techniques","title":"Techniques","text":"<p>In A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications offer a systematic survey of prompt engineering techniques, and offer a concise \u201coverview of the evolution of prompting techniques, spanning from zero-shot prompting to the latest advancements.\u201d as shown in the following illustation.</p> <p></p> <p>Prompt engineers can employ the following advanced techniques to improve the model\u2019s understanding and output quality.</p> <ul> <li>Zero-shot prompting provides the machine learning model with a task it hasn\u2019t explicitly been trained on. Zero-shot prompting tests the model\u2019s ability to produce relevant outputs without relying on prior examples.</li> <li>Few-shot prompting or in-context learning gives the model a few sample outputs (shots) to help it learn what the requestor wants it to do. The learning model can better understand the desired output if it has context to draw on.</li> <li>Chain-of-thought prompting (CoT) is an advanced technique that provides step-by-step reasoning for the model to follow. Breaking down a complex task into intermediate steps, or \u201cchains of reasoning,\u201d helps the model achieve better language understanding and create more accurate outputs.</li> <li>Prompt chaining is useful to accomplish complex tasks which an LLM might struggle to address if prompted with a very detailed prompt. In prompt chaining, chain prompts perform transformations or additional processes on the generated responses before reaching a final desired state.</li> <li>Tree of thought is a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts that serve as intermediate steps for general problem solving with language models.</li> </ul>"},{"location":"ai-ml-datascience/llm/prompt-engineering/#six-strategies-from-openai","title":"Six strategies from OpenAI","text":"<p>As suggested by OpenAI documentation:</p> <ul> <li>Write clear instructions. These models can\u2019t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you\u2019d like to see. The less the model has to guess at what you want, the more likely you\u2019ll get it.</li> <li>Provide reference text. Language models can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to these models can help in answering with fewer fabrications.</li> <li>Split complex tasks into simpler subtasks. Just as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks submitted to a language model. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.</li> <li>Give the model time to 'think'. If asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, models make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a \"chain of thought\" before an answer can help the model reason its way toward correct answers more reliably.</li> <li>Use external tools. Compensate for the weaknesses of the model by feeding it the outputs of other tools. For example, a text retrieval system (sometimes called RAG or retrieval augmented generation) can tell the model about relevant documents. </li> <li>Test changes systematically. Improving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an \"eval\").</li> </ul>"},{"location":"ai-ml-datascience/llm/prompt-engineering/#prompt-structure","title":"Prompt structure","text":"<p>In her prompt engineering blog, How I Won Singapore\u2019s GPT-4 Prompt Engineering Competition, Sheila Teo offers a practical strategy and worthy insights into how to obtain the best results from LLM by using the CO-STAR framework.</p>"},{"location":"ai-ml-datascience/llm/prompt-engineering/#co-star-framework","title":"CO-STAR framework","text":"<p>The CO-STAR framework considers all the key aspects that influence the effectiveness and relevance of an LLM\u2019s response, leading to more optimal responses.</p> <p></p> <p>CO-STAR</p> <ul> <li>C: Context: Provide background and information on the task</li> <li>O: Objective: Define the task that you want the LLM to perform</li> <li>S: Style: Specify the writing style you want the LLM to use</li> <li>T: Tone: Set the attitude and tone of the response</li> <li>A: Audience: Identify who the response is for</li> <li>R: Response: Provide the response format and style</li> </ul> <p>You can view extensive examples in these two Colab notebooks:</p> <ul> <li>Basic Tasks</li> <li>NLP Tasks</li> </ul> <p>The following illustration from the Basic Tasks notebook shows the prompt:</p> <p></p> <p>And the response:</p> <p></p>"},{"location":"ai-ml-datascience/llm/prompt-engineering/#example-prompts","title":"Example prompts","text":"<p>Prompts are linked to type of tasks, meaning the kind of task you wish the LLM perform equates to a type of prompt\u2013and how you will craft it.</p> <p>The following list shows various task-related prompts and advises on crafting effective prompts to accomplish these tasks:</p> <ul> <li>Text Summarization</li> <li>Information Extraction</li> <li>Question Answering</li> <li>Text Classification</li> <li>Conversation</li> <li>Code Generation</li> <li>Reasoning</li> </ul>"},{"location":"ai-ml-datascience/llm/prompt-engineering/#tutorials","title":"Tutorials","text":"<p>The following tutorials, examples provide hands-on practice building prompts.</p>"},{"location":"ai-ml-datascience/llm/prompt-engineering/#openai-prompts","title":"OpenAI prompts","text":"<p>OpenAI provides a set of prompts for you to begin your prompt engineering.  See Prompt examples.</p> <p></p>"},{"location":"ai-ml-datascience/llm/prompt-engineering/#prompt-lab","title":"Prompt Lab","text":"<p>You can learn to use watsonx.ai Prompt Lab, a GUI-based no-code tool to quickly test different models and prompts. Using the prompt lab, you can quickly see the difference in outputs between prompts formatted with correct system and instruction prompts, and those without them.</p> <p></p> <p>Guiding Llama 2 with prompt engineering by developing system and instruction prompts</p>"},{"location":"ai-ml-datascience/llm/prompt-engineering/#microsoft","title":"Microsoft","text":"<p>Microsoft provides excellent introduction to GenAI and prompt engineering. The course consists of 21 lessons. </p> <p></p> <p>See:</p> <ul> <li>Understanding Prompt Engineering Fundamentals with the video</li> <li>Creating Advanced Prompts with the video</li> </ul>"},{"location":"ai-ml-datascience/llm/prompt-engineering/#references","title":"References","text":"<ul> <li>Best Prompt Techniques for Best LLM Responses</li> <li>What is prompt engineering?</li> <li>OpenAI documentation Prompt Engineering</li> <li>How I Won Singapore\u2019s GPT-4 Prompt Engineering Competition</li> <li>Prompt Engineering Guide</li> <li>Microsoft Prompt engineering techniques</li> </ul>"},{"location":"ai-ml-datascience/llm/rag-overview/","title":"Implementing RAG","text":"<p>RAG (Retrieval-Augmented Generation) expands the knowledge of large language models (LLMs) from their initial training data to external datasets you provide. </p> <p>You provide the data. RAGs give your users immediate access to accurate, real-time, and relevant answers. So when one of your employees or customers asks your LLM a question, they get answers trained on your secure business data.</p>"},{"location":"ai-ml-datascience/llm/rag-overview/#business-value","title":"Business value","text":"<p>Once you build a RAG using a pipeline:</p> <ul> <li>LLMs can answer complex questions: RAG allows LLMs to tap into external knowledge bases and specific bodies of information to answer challenging questions with precision and detail.</li> <li>LLMs that generate up-to-date content: By grounding outputs in real-world data, RAG-powered LLMs can create more factual and accurate documents, reports, and other content.</li> <li>Increase LLM response accuracy: RAG augments answer generation with real-time data that\u2019s relevant to your industry, customers, and business \u2013 so your chatbot is less likely to hallucinate to fill in missing information. </li> </ul>"},{"location":"ai-ml-datascience/llm/rag-overview/#how-it-works","title":"How it works","text":"<p>RAG works in three stages:</p> <ol> <li> <p>Retrieval: Someone queries the LLM and the system looks for relevant information that informs the final response. </p> <p>It searches through an external dataset or document collection to find relevant pieces of information. This dataset could be a curated knowledge base, a set of web pages, or any extensive collection of text, images, videos, and audio.</p> </li> <li> <p>Augmentation: The input query is enhanced with the information retrieved in the previous step.</p> <p>The relevant information found during the retrieval step is combined with the original query. This augmented input is then prepared for the next stage, ensuring that it is in a format suitable for the generation model.</p> </li> <li> <p>Generation: The final augmented response or output is generated. Your LLM uses the additional context provided by the augmented input to produce an answer that is not only relevant to the original query but enriched with information from external sources.</p> </li> </ol> <p>The following diagram from the OpenAI Cookbook provides an overview:</p> <p></p> <p>So far so good. So how do you managed this whole process? You use a pipeline.</p>"},{"location":"ai-ml-datascience/llm/rag-overview/#turns-out-rag-pipelines-can-be-complex","title":"Turns out, RAG Pipelines can be complex","text":"<p>In general, the RAG Pipelines break into five steps:</p> <ol> <li>Loading: This involves importing your data into the RAG pipeline \u2013 e.g. text files, PDFs, websites, databases, APIs, etc. LlamaHub offers an extensive array of connectors for this purpose.</li> <li>Indexing: At this stage, you're developing a data structure conducive to data querying. For Large Language Models (LLMs), this typically involves generating vector embeddings, which are numerical representations of your data's meaning.</li> <li>Storing: After indexing your data, the next step is to store the index along with any additional metadata. This is crucial to eliminate the necessity for re-indexing in the future.</li> <li>Querying: Depending on your chosen indexing strategy, there are numerous methods to utilize LLMs and LlamaIndex data structures for querying. These methods range from sub-queries and multi-step queries to hybrid approaches.</li> <li>Evaluation: An indispensable part of any RAG pipeline is to assess its effectiveness. This could be in comparison to other strategies or following any modifications. Evaluation offers objective metrics to gauge the accuracy, reliability, and speed of your responses to queries.</li> </ol> <p>For more information, see RAG Pipeline explained</p>"},{"location":"ai-ml-datascience/llm/rag-overview/#deep-dive-into-rag","title":"Deep dive into RAG","text":"<p>For a deep dive into the complexity, see Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.</p>"},{"location":"ai-ml-datascience/llm/rag-overview/#select-technologies","title":"Select technologies","text":"<p>The technologies bleed across the steps in the pipeline. And the solutions vary drive a particular LLM soltuion. </p>"},{"location":"ai-ml-datascience/llm/rag-overview/#decoding-vs-encoding-vs-both","title":"Decoding vs Encoding vs both","text":"<p>In LLM models article in this site, you can see a table summarizes the architecture types and some use cases for each of the network types.</p> Architecture Types Examples Use cases Encoder-only transformers BERT (Google) Sentence classification, named entity recognition, extractive question answering Encoder-decoder transformers T5 (Google) Summarization, translation, question answering Decoder-only transformers GPT Series (OpenAI) Text generation"},{"location":"ai-ml-datascience/llm/rag-overview/#references","title":"References","text":"<ul> <li>RAG 101: What is RAG and why does it matter?</li> <li>Understanding RAG Part I: Why It\u2019s Needed</li> <li>Understanding RAG Part II: How Classic RAG Works</li> <li>Understanding RAG III: Fusion Retrieval and Reranking</li> <li>LangChain vs LlamaIndex: A Detailed Comparison</li> </ul>"},{"location":"ai-ml-datascience/llm/rag-pipeline/","title":"RAG Pipeline - Indexing","text":"<p>RAG (Retrieval-Augmented Generation) expands the knowledge of large language models (LLMs) from their initial training data to external datasets you provide. </p> <p>You provide the data. RAGs give your users immediate access to accurate, real-time, and relevant answers. So when one of your employees or customers asks your LLM a question, they get answers trained on your secure business data.</p> <p>The pipeline is built around two steps:</p> <ol> <li>Indexing. Getting your data, preparing it for the Large Langugage Model (LLM), and storing it.</li> <li>Retrieval and generation. That's where the user interacts and your model generates the response.</li> </ol>"},{"location":"ai-ml-datascience/notebook/setupsecrets/","title":"Set up tokens and use as environment variables","text":"<p>When you are working on your local development computer and if the secret doesn't change between executions, you can set the secrets in a special file.</p> <p>Secrets can be about databases, passwords, API gateways, operating modes that determine whether our app is running in developer or production mode. Basically, all the code you need to actually access the different functions of your app.</p> <p>Those tokens, secrets, passwords might not necessarily be things which you want your user to be able to see.</p> <p>In this tutorial you will see how to use a <code>.env</code> file to make it easier to share with teammates while allowing them to set their own environmental variables, but not necessarily check in and let the whole world see.</p> <p>Tokens are in the clear</p> <p>This demonstrates how to store the tokens in the clear. </p>"},{"location":"ai-ml-datascience/notebook/setupsecrets/#set-gitignore","title":"Set .gitignore","text":"<p>When you build out a project in Git, you will have a file named <code>.gitignore</code>. Open it to be sure that the list of files Git ignores on check in include. You should see, </p> <pre><code>.env\n</code></pre>"},{"location":"ai-ml-datascience/notebook/setupsecrets/#create-the-env-file","title":"Create the .env file","text":"<p>To configure the development environment, add a .env in the root directory of your project:</p> <pre><code>.\n\u251c\u2500\u2500 .env\n\u2514\u2500\u2500 foo.py\n</code></pre> <p>For example, use:</p> <pre><code>cd ~\ncat &gt; ~/project/.env &lt;&lt;EOF\n#!/bin/bash  \n#filename: .env\nMUSICSECRET=polkalover  \nEOF\n</code></pre> <p>To test, run:</p> <pre><code># see the file\ncat ~/project/.env\n</code></pre> <p>To append an new environment variable:</p> <pre><code>echo \"MUSICPREFEENCE=rockandroll\"  &gt;&gt; ~/project/.env\n</code></pre>"},{"location":"ai-ml-datascience/notebook/setupsecrets/#make-the-token-file-available-only-to-you","title":"Make the token file available only to you","text":"<p>Secure the token file so that only the user can see it.</p> <pre><code>chmod 600 ~/project/.env\n</code></pre> <p><code>chmod 600</code> is a file permission setting in Linux that grants read and write permissions to the owner, while denying all permissions to the group and other users.</p>"},{"location":"ai-ml-datascience/notebook/setupsecrets/#use-the-token-in-a-command-line","title":"Use the token in a command line","text":"<p>You can load the file and set the token to an environment variable.</p> <pre><code>. ~/project/.musictoken\n</code></pre> <p>To see the environment variables:</p> <pre><code># check the tokens\necho $MUSICSECRET $MUSICPREFEENCE\n</code></pre> <p>[!NOTE] If you want to encrypt your variables as passwords, use encpass.sh</p>"},{"location":"ai-ml-datascience/notebook/setupsecrets/#use-the-keys-in-your-jupyter-notebook","title":"Use the keys in your Jupyter Notebook","text":"<p>To use the keys in a notebook in three steps:</p> <ol> <li>Install <code>dotenv</code> into your Python environment.</li> <li>Load the environment in your .env file and assign variables in your application</li> </ol> <p>Let's start with installing dotevn</p>"},{"location":"ai-ml-datascience/notebook/setupsecrets/#install-dotenv-in-your-python-environment","title":"Install <code>dotenv</code> in your Python environment","text":"<p>You can install using pip or conda.</p> <pre><code>conda install conda-forge::python-dotenv\n## or\n## pip install python-dotenv\n</code></pre>"},{"location":"ai-ml-datascience/notebook/setupsecrets/#load-the-environment-in-your-env-file-and-assign-variables-in-your-application","title":"Load the environment in your .env file and assign variables in your application","text":"<p>At the top of your notebook:</p> <pre><code>from dotenv, import load_dotenv\n\ndef configure() -&gt; None:\n    \"\"\"Load environment variables and configure OpenAI API key.\"\"\"\n    load_dotenv()\n    music_secret = os.getenv(\"MUSICSECRET\")\n    music_preference = os.getenv(\"MUSICPREFEENCE\")\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n</code></pre> <p>With <code>load_dotenv(override=True)</code> or <code>dotenv_values()</code>, the value of a variable is the first of the values defined in the following list:</p> <ol> <li>Value of that variable in the <code>.env</code> file.</li> <li>Value of that variable in the environment.</li> <li>Default value, if provided. 4/ Empty string.</li> </ol> <p>With <code>load_dotenv(override=False)</code>, the value of a variable is the first of the values defined in the following list:</p> <ol> <li>Value of that variable in the environment.</li> <li>Value of that variable in the .env file.</li> <li>Default value, if provided.</li> <li>Empty string.</li> </ol> <p>To test:</p> <pre><code>configure()\nprint(\"MUSICSECRET = \", music_secret)\nprint(\"MUSICPREFEENCE = \", music_preference)\nprint(\"OPENAI_API_KEY = \", openai.api_key)\n</code></pre>"},{"location":"ai-ml-datascience/notebook/setupsecrets/#another-technique","title":"Another technique","text":"<p>You can also use magic commands to load your <code>.env</code> data.</p> <pre><code>%load_ext dotenv\n%dotenv\n</code></pre>"},{"location":"ai-ml-datascience/notebook/setupsecrets/#for-more-information-and-techniques","title":"For more information and techniques","text":"<p>See python-dotenv</p>"},{"location":"ai-ml-datascience/notebook/setupsecrets/#references","title":"References","text":"<ul> <li>Hiding secret from command line parameter on Unix</li> <li>Exploring the new Podman secret command</li> <li>Using .env files: what they are, and when to use them</li> </ul>"},{"location":"ai-ml-datascience/notebook/startjupyterinenv/","title":"Start Jupyter notebook in a Conda environment","text":"<p>You must run different projects on separate environments. The environments include the <code>conda</code> and <code>pip</code> packages and their dependencies.</p> <p>For why, see Why you should use a virtual environment for EVERY python project</p>"},{"location":"ai-ml-datascience/notebook/startjupyterinenv/#prerequisites","title":"Prerequisites","text":"<ul> <li>Install Anaconda or miniconda</li> <li>Terminal (cmd/PowerShell/bash) with the directory set to your code</li> </ul>"},{"location":"ai-ml-datascience/notebook/startjupyterinenv/#check-the-list-of-environments","title":"Check the list of environments","text":"<pre><code>conda env list\n</code></pre> <p>Switching between environments works as simply as typing <code>conda activate [NAME]</code> and if done with it deactivating it (and going back to the base environment) with <code>conda deactivate</code>.</p>"},{"location":"ai-ml-datascience/notebook/startjupyterinenv/#create-environment","title":"Create environment","text":"<p>The following command creates an environment named <code>myenv</code> followed by a particular version of Python and the list of packages:</p> <pre><code>conda create -n myenv python=3.9 scipy=0.17.3 astroid babel\n</code></pre> <p>Installation</p> <p>Install all the programs that you want in this environment at the same time. Installing one program at a time can lead to dependency conflicts.</p>"},{"location":"ai-ml-datascience/notebook/startjupyterinenv/#create-an-environment-from-a-environmentyml-file","title":"Create an environment from a environment.yml file","text":"<p>Create the environment from a environment.yml file:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>The name of the enironment can be in the <code>environment.yml</code> file. </p> <p>To verify that the environment was successfully created, use:</p> <pre><code>conda info --envs\n</code></pre>"},{"location":"ai-ml-datascience/notebook/startjupyterinenv/#but-what-if-it-fails","title":"But what if it fails","text":"<p>You might face ResolvePackageNotFound: failure while creating your environment.</p> <p>You can add those dependencies to the <code>dependencies</code> | <code>pip</code> section in your environments.yml file.</p> <p>For more details, see Setting Up a Conda Environment in Less Than 5 Minutes.</p>"},{"location":"ai-ml-datascience/notebook/startjupyterinenv/#activate-environment","title":"Activate environment","text":"<pre><code>conda activate myenv\njupyter notebook\n</code></pre>"},{"location":"ai-ml-datascience/notebook/startjupyterinenv/#best-practices-with-conda","title":"Best practices with Conda","text":"<p>We recommend that you:</p> <ul> <li>Use pip only after conda</li> <li>Install as many requirements as possible with conda then use pip.</li> <li>Pip should be run with <code>--upgrade-strategy only-if-needed</code> (the default).</li> <li>Do not use pip with the <code>--user argument</code>, avoid all users installs.</li> </ul> <p>Use conda environments for isolation</p> <ul> <li>Create a conda environment to isolate any changes pip makes.</li> <li>Environments take up little space thanks to hard links.</li> </ul> <p>Care should be taken to avoid running pip in the root environment.</p> <ul> <li>Recreate the environment if changes are needed</li> <li>Once pip has been used, conda will be unaware of the changes.</li> </ul> <p>To install additional conda packages, it is best to recreate the environment.</p> <ul> <li>Store conda and pip requirements in text files</li> <li>Package requirements can be passed to conda via the <code>--file</code> argument.</li> </ul> <p>Pip accepts a list of Python packages with <code>-r</code> or <code>--requirements</code>.</p> <p>Conda env will export or create environments based on a file with conda and pip requirements.</p> <p>Tip</p> <p>You can put the pip requirements into your environment. Add <code>- pip:</code> to the dependencies followed by the list of pip packages you will need. For example:</p> <pre><code>name: myenv\nchannels:\n    - defaults\ndependencies:\n    - pandas\n    - numpy\n    - pip:\n        - hdfs==1.90\n</code></pre>"},{"location":"ai-ml-datascience/notebook/startjupyterinenv/#set-up-your-dockerfile","title":"Set up your dockerfile","text":"<p>You can use your <code>environments.yml</code> file to build out your Docker (or Podman) container. Here is a stub for an environment:</p> <pre><code>FROM continuumio/miniconda3\nADD environment.yml /tmp/environment.yml\nRUN conda env create -f /tmp/environment.yml\n# Pull the environment name out of the environment.yml\nRUN echo \"source activate $(head -1 /tmp/environment.yml | cut -d' ' -f2)\" &gt; ~/.bashrc\nENV PATH /opt/conda/envs/$(head -1 /tmp/environment.yml | cut -d' ' -f2)/bin:$PATH\n</code></pre> <p>For more information, see Conda Environments with Docker.</p>"},{"location":"ai-ml-datascience/notebook/startjupyterinenv/#references","title":"References","text":"<ul> <li>Managing environments</li> <li>Setting Up a Conda Environment in Less Than 5 Minutes</li> <li>Conda Environments with Docker</li> </ul>"},{"location":"azure/CreateCP4DonAzure/","title":"Create Cloud Pak for Data on Azure","text":"<p>Cloud Pak for Data uses Azure services and features, including VNets, Availability Zones, Availability Sets, security groups, Managed Disks, and Azure Load Balancers to build a reliable and scalable cloud platform.</p> <p>The link in the References section provides a template that deploys an Openshift cluster on Azure with all the required resources, infrastructure and then deploys IBM Cloud Pak for Data along with the add-ons that user chooses.</p> <p>The deployment guide on GitHub provides step-by-step instructions for deploying IBM Cloud Pak for Data on a Red Hat OpenShift Container Platform 4.8 cluster on Azure. With this template, you can automatically deploy a multi-master, production instance of Cloud Pak for Data.</p>"},{"location":"azure/CreateCP4DonAzure/#prerequisites","title":"Prerequisites","text":"<p>You will need:</p> <ul> <li>Access to Azure with Contributor and User Access Administrator permissions to run the template to create a Service Principal on the resource group that you will also need to create.</li> </ul>"},{"location":"azure/CreateCP4DonAzure/#notes","title":"Notes","text":"<p>You may add parameters to <code>azuredeployp.parameters.json</code> to specify additional parameters. The following are defaults in <code>azuredeploy.json</code>are:</p> Default value <code>masterInstanceCount</code> 3 <code>workerInstanceCount</code> 3 <code>masterVmSize</code> <code>Standard_D8s_v3</code> <code>workerVmSize</code> <code>Standard_D16s_v3</code> <code>dataDiskSize</code> <code>1024</code> <p>If you choose Portworx as your storage class, see Portworx documentation for generating <code>portworx spec url</code>.</p>"},{"location":"azure/CreateCP4DonAzure/#reference","title":"Reference","text":"<p>See Cloud Pak for Data on Azure.</p>"},{"location":"azure/DevInstallOpenShiftOnAzure/","title":"Dev install OpenShift 4.8 on Azure with customizations","text":"<p>In this document learn to install OpenShift on Azure.</p> <p>In particular, this will document the config file needed by the installer for a custom install that meets the following requirements:</p>"},{"location":"azure/DevInstallOpenShiftOnAzure/#architectural-decisions-for-this-installation","title":"Architectural decisions for this installation","text":"<ul> <li>Installer provisioned infrastructure</li> <li>OpenShift administrator-level credential secret in the kube-system namespace. (As an alternative, you can put Cloud Credential Operator (CCO) into manual mode)</li> </ul>"},{"location":"azure/DevInstallOpenShiftOnAzure/#prerequisites","title":"Prerequisites","text":"<p>You will need:</p> <ul> <li>Azure environment set up according to the OpsInstallOpenShiftOnAzure documentation. In particular, review details about account configuration, account limits, public DNS zone configuration, required roles, creating service principals, and supported Azure regions.</li> <li>Firewall configured to allow access to sites.</li> <li>Azure CLI. May already be installed on Bastion. See How to install the Azure CLI for your operating system.</li> <li>Access to OpenShift Container Platform. </li> <li><code>ssh-keygen</code> installed.</li> <li>Linux Bastion with 500 MB of local disk space and <code>ssh</code>, <code>tar</code>.</li> <li>A Red Hat account.</li> </ul> <p>Log on using <code>az login</code>. Then use <code>az account show</code> to see the subscription id, the tenant id, such as:</p> <pre><code>[\n  {\n    \"cloudName\": \"AzureCloud\",\n    \"homeTenantId\": \"4e7730a0-17bb-4dfa-8dad-7c54d3e761b7\",\n    \"id\": \"f9700497-11d3-4005-8b1d-3bf45a667424\",\n    \"isDefault\": true,\n    \"managedByTenants\": [],\n    \"name\": \"Microsoft Azure Sponsorship\",\n    \"state\": \"Enabled\",\n    \"tenantId\": \"4e7730a0-17bb-4dfa-8dad-7c54d3e761b7\",\n    \"user\": {\n      \"name\": \"Bruce.Kyle@ibm.com\",\n      \"type\": \"user\"\n    }\n  }\n]\n</code></pre> <p>NOTE: if you have multiple subscriptions, use <code>az account set --subscription mysubscription</code> where  <code>mysubscription</code> is the GUID or the name of the account you want to make default.</p> <p>You will need the <code>id</code> is the subscriptiond id and <code>tenantId</code> is the tenant id you will need later.</p> <p>You should also have several items from the service principal from the administrator.</p> <ul> <li>baseDomain: such as <code>ibmtechgarage.com</code></li> <li>appId: it will be a GUID </li> <li>password: it will be the password, probably a GUID generated when the service principal was created</li> </ul> <p>IMPORTANT: The <code>appId</code> and the <code>password</code> are sensitive and should not be checked into code. </p>"},{"location":"azure/DevInstallOpenShiftOnAzure/#generate-a-key-pair-for-cluster-node-ssh-access","title":"Generate a key pair for cluster node SSH access","text":"<p>You must provide the SSH public key during the installation process.</p> <pre><code>mkdir ~/.azkeys\ncd ~/.azkeys\nssh-keygen -t rsa -b 2048 -N '' -f ./azkeys\n</code></pre> <p>NOTE: Generating <code>rsa</code> key so that OpenShift Container Platform cluster can use FIPS Validated / Modules in Process cryptographic libraries on the x86_64 architecture.</p> <p>View the public key.</p> <pre><code>cat ~/.azkeys/azkeys.pub\n</code></pre> <p>Add the SSH private key identity to the SSH agent for your local user.</p> <pre><code>eval \"$(ssh-agent -s)\"\n# responds with something like Agent pid 814\n</code></pre> <p>Add SSH private key to the <code>ssh-agent</code>.</p> <pre><code>ssh-add ~/.azkeys/azkeys\n# responds with something like Identity added: /home/bruce/.azkeys/azkeys (bruce@LAPTOP-SLIV8CB4)\n</code></pre>"},{"location":"azure/DevInstallOpenShiftOnAzure/#get-the-installation-program","title":"Get the installation program","text":"<p>Create a location to download your files, such as.</p> <pre><code>mkdir ~/openshiftinstallerfiles \n</code></pre> <p>To download the installation file:</p> <ol> <li>Download the Infrastructure Provider page on the OpenShift Cluster Manager site.</li> </ol> <p>IMPORTANT: Do not select the top Azure managed services. This is ARO that is a managed service running on Azure. We are selecting intaller-provisioned infrastructure to give us the most flexibility in version, license, sizing, networking.</p> <p>Select:</p> <p></p> <p>Click Installer-provisioned infrastrcture box.</p> <p>Download the installation program for your operating system, and place the file in the directory where you will store the installation configuration files.</p> <p></p> <ol> <li>Copy the file to a known location</li> </ol> <p>Move the file to your <code>~/openshiftinstallerfiles</code> directory, such as <code>mv /mnt/c/Users/6J1943897/Downloads/openshift-install-linux.tar.gz ~/openshiftinstallerfiles</code></p> <p>Extract the installation program. </p> <pre><code>cd ~/openshiftinstallerfiles\ntar xvf openshift-install-linux.tar.gz\n\n# check to see the files using ls. you should see README.md and openshift-install\n</code></pre> <ol> <li>Download your installation pull secret from the Red Hat OpenShift Cluster Manager. </li> </ol> <p>Return to the OpenShift Cluster Manager page. Click Download pull secret button.</p>"},{"location":"azure/DevInstallOpenShiftOnAzure/#create-the-config-file","title":"Create the config file","text":"<p>Create a folder for your config file.</p> <pre><code>mkdir ~/azureconfig\ncd ~/openshiftinstallerfiles\n</code></pre> <p>Create the config file.</p> <pre><code>./openshift-install create install-config --dir ~/azureconfig\n</code></pre> <p>At the prompts, use the arrow keys to select <code>azure</code> then type enter.</p>"},{"location":"azure/DevInstallOpenShiftOnAzure/#customize-the-installation-config-file","title":"Customize the installation config file","text":"<p>You can customize the </p> <p>When the installer is complete you will see the console URL and credentials for accessing your new cluster. A kubeconfig file will also be generated for you to use with the oc CLI tools you downloaded.</p>"},{"location":"azure/DevInstallOpenShiftOnAzure/#configure-storageclass-for-rwx","title":"Configure StorageClass for RWX","text":"<p>TFor use with Cloud Paks and customer applications, you will want to dynamically provision ReadWriteMany (RWX) storage, which provides that your storage volume can be mounted as read-write by many nodes. </p> <p>You can use either OpenShift Container Platform storage (OCS) or OpenShift Data Foundation (ODF) Operators or set Azure Files StorageClass. </p> <p>OpenShift Container Storage (OCS) has been updated to OpenShift Data Foundation (ODF) starting with version OCP 4.9. For more information, see either:</p> <ul> <li>OpenShift Container Platform storage overview </li> <li>Deploying OpenShift Data Foundation on Azure Red Hat OpenShift.</li> </ul> <p>OR if you prefer, you can set up Azure Files as your StorageClass. See Create an Azure Files StorageClass on Azure Red Hat OpenShift 4.</p>"},{"location":"azure/DevInstallOpenShiftOnAzure/#reference","title":"Reference","text":"<ul> <li>Installing a cluster on Azure with customizations</li> <li></li> <li>Install Instana</li> <li>Install Slack</li> </ul>"},{"location":"azure/DevInstallOpenShiftOnAzure/#contributors","title":"Contributors","text":"<p>March 22, 2022</p>"},{"location":"azure/Landingzone/","title":"Azure Landing Zone","text":"<p>An Azure landing zone architecture is scalable and modular to meet various deployment needs. The repeatable infrastructure allows you to apply configurations and controls to every subscription consistently. Modules make it easy to deploy and modify specific Azure landing zone architecture components as your requirements evolve.</p> <p>The Azure landing zone conceptual architecture shown in the following diagram represents an opinionated target architecture for your Azure landing zone. You should use this conceptual architecture as a starting point and tailor the architecture to meet your needs.</p> <p></p>"},{"location":"azure/Landingzone/#landing-zone","title":"Landing zone","text":"<p>A landing zone helps you plan for and design an Azure deployment, by conceptualizing a designated area for placement and integration of resources. There are two types of landing zones:</p> <ul> <li>platform landing zone: provides centralized enterprise-scale foundational services for workloads and applications.</li> <li>application landing zone: provides services specific to an application or workload.</li> </ul> <p>Concretely, a landing zone can be viewed through two lenses:</p> <ul> <li>reference architecture: a specific design that illustrates resource deployment to one or more Azure subscriptions, which meet the requirements of the landing zone.</li> <li>reference implementation: artifacts that deploy Azure resources into the landing zone subscription(s), according to the reference architecture. Many landing zones offer multiple deployment options, but the most common is a ready-made Infrastructure as Code (IaC) template referred to as a landing zone accelerator. Accelerators automate and accelerate the deployment of a reference implementation, using IaC technology such as ARM, Bicep, Terraform, and others.</li> </ul>"},{"location":"azure/Landingzone/#landing-zone-journey","title":"Landing zone journey","text":""},{"location":"azure/Landingzone/#reference","title":"Reference","text":"<ul> <li>Prepare for cloud adoption</li> <li>What is an Azure landing zone?</li> </ul>"},{"location":"azure/OpsInstallOpenShiftOnAzure/","title":"Install instructions for operations team for OpenShift on Azure in restricted client environment for an MVP","text":"<p>This documents the steps to stand up Red Hat OpenShift 4.8 in Azure in a restrictive environment, including</p> <ul> <li>Prerequisites</li> <li>Preparation for the environment by the Ops team</li> </ul> <p>The document follows these general steps:</p> <ul> <li>Access to OpenShift Container Platform. </li> <li>Set Account Limits, specifically set the number of vCPU set to 60 plus the number already needed in the subscription.</li> <li>Configure a public DNS zone</li> <li>A service principal with both Contributor and User Access Administrator. (This step requires Owner permission to create the service principal). NOTE: We will need to securely copy the service principal.</li> <li>User access for anyone working on the installation for Contributor.</li> </ul>"},{"location":"azure/OpsInstallOpenShiftOnAzure/#prerequisites","title":"Prerequisites","text":"<p>You will need:</p> <ul> <li>Azure subscription</li> <li>Azure CLI installed</li> <li>Approval of CoreOS for the operating system of the Control Plane nodes that is part of OpenShift 4.8</li> <li>Access to the mirrored/approved Red Hat OpenShift 4.8 images for the worker nodes</li> </ul> <p>The admin Azure account subscription must have the following roles to create the service principal:</p> <ul> <li>User Access Administrator</li> <li>Owner</li> </ul> <p>Know how to log into Azure using:</p> <pre><code>az login\naz account show\n# validate you are in the right subscription\n</code></pre> <p>| Save the values of the <code>tenantId</code> and <code>id</code> from the previous output. You need these values during OpenShift Container Platform installation.</p>"},{"location":"azure/OpsInstallOpenShiftOnAzure/#configure-your-firewall","title":"Configure your firewall","text":"<p>The following assumes a cluster name of <code>watsonaiopsmvp</code> and a base domain name of <code>costcowaiopsmvp.com</code>.</p>"},{"location":"azure/OpsInstallOpenShiftOnAzure/#allowlist-the-following-registry-urls","title":"Allowlist the following registry URLs","text":"URL Port Function <code>registry.redhat.io</code> 443, 80 Provides core container images <code>quay.io</code> 443, 80 Provides core container images <code>*.quay.io</code> 443, 80 Provides core container images <code>sso.redhat.com</code> 443, 80 The https://console.redhat.com/openshift site uses authentication from sso.redhat.com <code>*.openshiftapps.com</code> 443, 80 Provides Red Hat Enterprise Linux CoreOS (RHCOS) images <p>When you add a site, such as <code>quay.io</code>, to your allowlist, do not add a wildcard entry, such as *.quay.io, to your denylist. In most cases, image registries use a content delivery network (CDN) to serve images. If a firewall blocks access, then image downloads are denied when the initial download request is redirected to a hostname such as cdn01.quay.io.</p> <p>CDN hostnames, such as cdn01.quay.io, are covered when you add a wildcard entry, such as *.quay.io, in your allowlist.</p>"},{"location":"azure/OpsInstallOpenShiftOnAzure/#allow-access-for-watson-ai-ops","title":"Allow access for Watson AI Ops","text":"<p>Inbound ports that WAIOPs exposes (for ingress communication):</p> <ul> <li>Generally, WAIOPs pods use only standard ports (80/443) defined by OCP routes for HTTP/HTTPs inbound connections coming from external clients such as the browser (for accessing the WAIOps console), an external Kafka client that needs to place data on a Kafka topic, Slack inbound data, etc.</li> </ul> <p>In our case, we will need standard ports (80/443) to the Instana instance.</p> <p>Outbound ports that WAIOPs needs access to (for egress communication):</p> <ul> <li>Generally, these types of connectors (e.g. ELK, Splunk, Humio, LogDNA, SNOW topology observers, etc.), WAIOps uses a PULL model. Therefore, the endpoint exposed by those external products specifies the port and connection parameters that WAIOPs should used for pulling the data (event, log, etc.) from those external systems. The port number used by that external system is totally dependent on how that external tool was configured. WAIOps does not dictate what that value should be.</li> </ul> <p>Not to be implemented in this MVP.</p> <p>Internal communication within the OCP cluster where WAIOPs is installed</p> <ul> <li>WAIOps internally uses other ports as well (e.g. 8080; 8383; 8686, etc.) for communication between its own pods (ClusterIP type) running on the same OCP cluster. Hence, these are not exposed as OCP routes.</li> </ul>"},{"location":"azure/OpsInstallOpenShiftOnAzure/#allow-access-for-red-hat-insights","title":"Allow access for Red Hat Insights","text":"URL Port Function <code>cert-api.access.redhat.com</code> 443, 80 Required for Telemetry <code>api.access.redhat.com</code> 443, 80 Required for Telemetry <code>infogw.api.openshift.com</code> 443, 80 Required for Telemetry <code>console.redhat.com/api/ingress</code>, <code>cloud.redhat.com/api/ingress</code> 443, 80 Required for Telemetry and for insights-operator"},{"location":"azure/OpsInstallOpenShiftOnAzure/#allow-access-to-azure-resources","title":"Allow access to Azure resources","text":"URL Port Function <code>management.azure.com</code> 443, 80 Required to access Azure services and resources. Review the Azure REST API reference in the Azure documentation to determine the endpoints to allow for your APIs. <code>*.blob.core.windows.net</code> 443, 80 Required to download Ignition files. <code>login.microsoftonline.com</code> 443, 80 Required to access Azure services and resources. Review the Azure REST API reference in the Azure documentation to determine the endpoints to allow for your APIs."},{"location":"azure/OpsInstallOpenShiftOnAzure/#allow-list-for-the-following-urls","title":"Allow list for the following URLs","text":"URL Port Function <code>mirror.openshift.com</code> 443, 80 Required to access mirrored installation content and images. This site is also a source of release image signatures, although the Cluster Version Operator needs only a single functioning source. <code>storage.googleapis.com/openshift-release</code> 443, 80 A source of release image signatures, although the Cluster Version Operator needs only a single functioning source. <code>*.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;</code> in our case use <code>*.apps.watsonaiopsmvp.costcowaiopsmvp.com</code> 443, 80 Required to access the default cluster routes unless you set an ingress wildcard during installation. <code>quayio-production-s3.s3.amazonaws.com</code> 443, 80 Required to access Quay image content in AWS. <code>api.openshift.com</code> 443, 80 Required both for your cluster token and to check if updates are available for the cluster. <code>art-rhcos-ci.s3.amazonaws.com</code> 443, 80 Required to download Red Hat Enterprise Linux CoreOS (RHCOS) images. <code>console.redhat.com/openshift</code> 443, 80 Required for your cluster token. <code>registry.access.redhat.com</code> 443, 80 Required for odo CLI."},{"location":"azure/OpsInstallOpenShiftOnAzure/#time-controller","title":"Time controller","text":"<p>If you use a default Red Hat Network Time Protocol (NTP) server allow the following URLs:</p> <ol> <li><code>rhel.pool.ntp.org</code></li> <li><code>rhel.pool.ntp.org</code></li> <li><code>rhel.pool.ntp.org</code></li> </ol> <p>If you do not use a default Red Hat NTP server, verify the NTP server for your platform and allow it in your firewall.</p> <p>For more information, see Configuring your firewall</p>"},{"location":"azure/OpsInstallOpenShiftOnAzure/#configure-subscription-limit","title":"Configure subscription limit","text":"<p>For IBM Cloud Pak for Watson AIOps AI Manager Small without high availability.</p> <p>The default Azure subscription will not have service limited needed. The system administration will need to update the subscription:</p> Component Number of components required Description vCPU for AIOps (Small) 60 + the number already allocated for other services on the subscription Hardware requirement totals for AI Manager and OpenShift control plane <p>To set the required number of vCPUs:</p> <ol> <li> <p>From the Azure portal, click Help + support in the lower left corner.</p> </li> <li> <p>Click New support request and then select the required values:</p> <p>a. From the Issue type list, select Service and subscription limits (quotas).</p> <p>b. From the Subscription list, select the subscription to modify.</p> <p>c. From the Quota type list, select the quota to increase. For example, select Compute-VM (cores-vCPUs) subscription limit increases to increase the number of vCPUs, which is required to install a cluster.</p> <p>d. Click Next: Solutions.</p> </li> <li> <p>On the Problem Details page, provide the required information for your quota increase:</p> <p>a Click Provide details and provide the required details in the Quota details window.</p> <p>b. In the SUPPORT METHOD and CONTACT INFO sections, provide the issue severity and your contact details.</p> </li> <li> <p>Click Next: Review + create and then click Create.</p> </li> </ol> <p>The complete list of requirements is here</p>"},{"location":"azure/OpsInstallOpenShiftOnAzure/#configure-new-dns-zone","title":"Configure new DNS zone","text":"<p>To install OpenShift Container Platform, the Microsoft Azure account you use must have a dedicated public hosted DNS zone in your account. This zone must be authoritative for the domain. </p> <ol> <li>Identify your domain, such as <code>costcowaiopsmvp.com</code></li> <li>Create resource group <code>rg-wus2-waiopsmvp-01</code> using:</li> </ol> <pre><code>az group create --name rg-wus2-waiopsmvp-01 --location \"westus2\"\n</code></pre> <ol> <li>Create a DNS Zone</li> </ol> <pre><code>az network dns zone create -g rg-wus2-waiopsmvp-01 -n costcowaiopsmvp.com\n</code></pre> <ol> <li>Create a DNS Record</li> </ol> <p>The following creates a record with the relative name \"demo\" in the DNS Zone \"costcowaiopsmvp.com\" in the resource group \"MyResourceGroup\". The fully-qualified name of the record set is \"costcowaiopsmvp.com\". The record type is \"A\", with IP address \"10.10.10.10\", and a default TTL of 3600 seconds (1 hour).</p> <pre><code>az network dns record-set a add-record -g rg-wus2-waiopsmvp-01 -z costcowaiopsmvp.com -n demo -a 10.10.10.10\n</code></pre> <ol> <li>View the records</li> </ol> <pre><code>az network dns record-set list -g rg-wus2-waiopsmvp-01 -z costcowaiopsmvp.com\n</code></pre> <p>For more information, see Quickstart: Create an Azure DNS zone and record using Azure CLI</p>"},{"location":"azure/OpsInstallOpenShiftOnAzure/#create-a-service-principal","title":"Create a service principal","text":"<p>Because OpenShift Container Platform and its installation program must create Microsoft Azure resources through Azure Resource Manager, you must create a service principal to represent it.</p> <pre><code>az ad sp create-for-rbac --role Contributor --name costcowaiopsmvp-01\n</code></pre> <p>Record the values of the <code>appId</code> and <code>password</code> from the output. You need these values during OpenShift Container Platform installation. Set the <code>appId</code> to an environment variable where <code>&lt;appid&gt;</code> is the value from the output of the previous command.</p> <pre><code>APP_ID=&lt;appid&gt;\n</code></pre> <p>IMPORTANT: You must always add the Contributor and User Access Administrator roles to the app registration service principal so the cluster can assign credentials for its components.</p> <p>Next, assign the User Access Administrator role and the Azure Active Directory Graph permission:</p> <pre><code>az role assignment create --role \"User Access Administrator\" --assignee-object-id $(az ad sp list --filter \"appId eq '$APP_ID'\" | jq '.[0].objectId' -r)\naz ad app permission add --id $APP_ID --api 00000002-0000-0000-c000-000000000000 --api-permissions 824c81eb-e3f8-4ee6-8f6d-de7f50d565b7=Role\n</code></pre> <p>Approve the permissions request. If your account does not have the Azure Active Directory tenant administrator role, follow the guidelines for your organization to request that the tenant administrator approve your permissions request.</p> <pre><code>az ad app permission grant --id $APP_ID --api 00000002-0000-0000-c000-000000000000\n</code></pre> <p>For more information, see Creating a service principal</p>"},{"location":"azure/OpsInstallOpenShiftOnAzure/#port-access","title":"Port access","text":"<p>Inbound ports that WAIOPs exposes (for ingress communication):</p> <ul> <li>WAIOPs pods use only standard ports (80/443) defined by OCP routes for HTTP/HTTPs inbound connections coming from external clients such as the browser (for accessing the WAIOps console), an external Kafka client that needs to place data on a Kafka topic, Slack inbound data, etc.</li> </ul> <p>Outbound ports that WAIOPs needs access to (for egress communication):</p> <ul> <li>For these types of connectors (e.g. ELK, Splunk, Humio, LogDNA, SNOW topology observers, etc.), WAIOps uses a PULL model. Therefore, the endpoint exposed by those external products specifies the port and connection parameters that WAIOPs should used for pulling the data (event, log, etc.) from those external systems. The port number used by that external system is totally dependent on how that external tool was configured. WAIOps does not dictate what that value should be.</li> </ul> <p>Internal communication within the OCP cluster where WAIOPs is installed</p> <ul> <li>WAIOps internally uses other ports as well (e.g. 8080; 8383; 8686, etc.) for communication between its own pods (ClusterIP type) running on the same OCP cluster. Hence, these are not exposed as OCP routes.</li> </ul>"},{"location":"azure/OpsInstallOpenShiftOnAzure/#next-steps","title":"Next steps","text":"<ol> <li>Define the config for Installing a cluster on Azure with customizations</li> <li>Install ODF for 601 Gi of persistent storage</li> <li>Install Watson AI Ops or other Cloud Pak.</li> </ol>"},{"location":"azure/OpsInstallOpenShiftOnAzure/#references","title":"References","text":"<p>Red Hat documentation: Configuring an Azure account Hardware requirements for IBM Cloud Pak for Watson AIOps AI Manager Azure Account Set Up. This document is a guide for preparing a new Azure account for use with OpenShift. It will help prepare an account to create a single cluster and provide insight for adjustments which may be needed for additional clusters. Watson AI Ops 3.3 pre-prod doc for hardware, OCP version and necessary setup on that page to ensure the configured OCP should be maximally compatible. </p>"},{"location":"azure/OpsInstallOpenShiftOnAzure/#contributors","title":"Contributors","text":"<ul> <li>Bruce Kyle</li> <li>Volodymyr Rozdolsky</li> <li>Hamza Elgindy</li> </ul> <p>March 22, 2022</p>"},{"location":"azure/SampleARMwithScripts/","title":"Sample Azure Resource Manager (ARM) best practice templates","text":"<p>The ARM templates provided as defaults through the Azure portal for various resources are not particularly extensible. And in some few cases, the defaults may not pass security checks and comply with basic policies.</p> <p>The following links take you to my GitHub site for:</p> <ul> <li> <p>ARM templates</p> <ul> <li>Deployment template</li> <li>Parameter template</li> </ul> </li> <li> <p>PowerShell script to test the ARM template deployment</p> <ul> <li>Including <code>.EXAMPLE</code> code that you can copy and paste in PowerShell to test the template deployment</li> </ul> </li> <li> <p>Azure DevOps pipeline script to deploy a site</p> </li> </ul>"},{"location":"azure/SampleARMwithScripts/#features","title":"Features","text":"<p>ARM Templates</p> <ul> <li>Extensible</li> <li>Incorporate best practices, such as Role Based Access Control, Private Endpoints, Hub-Spoke network architecture, managed identities, security permissions</li> <li>Both template and parameters</li> </ul> <p>PowerShell</p> <ul> <li>Test individual templates and combination of templates</li> </ul> <p>Azure DevOps Pipeline</p> <ul> <li>End to end DevOps Pipeline that ties it all together</li> </ul>"},{"location":"azure/SampleARMwithScripts/#deploy-arm-templates-using-azure-devops-pipeline","title":"Deploy ARM templates using Azure DevOps Pipeline","text":"<p>You can integrate Azure Resource Manager templates (ARM templates) with Azure Pipelines for continuous integration and continuous deployment (CI/CD). In this article, you learn two more advanced ways to deploy templates with Azure Pipelines.</p> <p>The example pipelines shown in these templates demonstrate to to add task that runs an Azure PowerShell script. This option has the advantage of providing consistency throughout the development life cycle because you can use the same script that you used when running local tests. Your script deploys the template but can also perform other operations such as getting values to use as parameters.</p> <p>A few examples show how to add tasks to copy and deploy tasks. This option offers a convenient alternative to the project script.</p>"},{"location":"azure/SampleARMwithScripts/#supported-kubernetes-architecture","title":"Supported Kubernetes architecture","text":"<p>The following diagram illustrates a hybrid deployment where Kubernetes is a central deployment for many services. </p> <p></p>"},{"location":"azure/SampleARMwithScripts/#sample-templates-and-scripts","title":"Sample templates and scripts","text":"<p>From this GitHub:</p> <ul> <li>Naming convention</li> <li>Create Resource Group with locks and access control</li> <li>Azure Storage account</li> <li>Azure Kubernetes Service (AKS) using CNI and Azure Container Registry (ACR)</li> <li>Azure Functions</li> <li>CosmosDB</li> <li>Key Vault, Key Vault for Cosmos, Key Vault Permissions, Key Vault Secret, Key Vault for Storage</li> <li>ARM template to Retrieve Object ID</li> <li>Role Assignment </li> <li>SignalR</li> <li>Virtual Network</li> <li>Public IP</li> <li>Redis</li> </ul>"},{"location":"azure/SampleARMwithScripts/#more-examples","title":"More examples","text":"<p>See:</p> <ul> <li>GitHub quickstarts</li> </ul>"},{"location":"azure/SampleARMwithScripts/#references","title":"References","text":"<ul> <li>Tutorial: Deploy a local ARM template</li> <li>Integrate ARM templates with Azure Pipelines</li> </ul>"},{"location":"azure/SetupBastionEnvironment/","title":"Set up Azure Bastion installation environment","text":"<p>In this section, set up the Bastion to the create and connect to the OpenShift clusters. You will:</p> <ol> <li>Set up a resource group</li> <li>Create a virtual network for the Bastion and the Virtual Machine you will use with the Bastion</li> <li>Create a virtual machine</li> <li>Create the Bastion </li> <li>Connect the Bastion to the Virtual Machine</li> <li>Use the Virtual Machine</li> </ol> <p>The following diagram shows the architecture of the Bastion you will set up:</p> <p></p>"},{"location":"azure/SetupBastionEnvironment/#prerequisites","title":"Prerequisites","text":"<p>You will need:</p> <ul> <li>Azure subscription</li> <li>Contributor access to create resource group, virtual networks, Bastion</li> <li>Azure CLI installed on your development computer, log in using <code>az login</code> and set your subscription using <code>az account set --subscription &lt;subscriptionid&gt;</code></li> </ul> <p>A virtual network design:</p> <ul> <li>Base virtual network for Bastion: 10.0.0.0/24</li> <li>Bastion subnet: 10.0.0.64/26</li> <li> <p>Bastion VM subnet: 10.0.0.128/26</p> </li> <li> <p>Control plane subnet: 10.0.0.0/26</p> </li> <li>Worker subnet: 10.0.0.64/26</li> </ul> <p>OR</p> <ul> <li>Base virtual network for Bastion: 10.0.0.0/24</li> <li>Bastion subnet: 10.0.0.64/26</li> <li> <p>Bastion VM subnet: 10.0.0.128/26</p> </li> <li> <p>Base virtual network for OpenShift: 10.0.2.0/24</p> </li> <li>Control plane subnet: 10.0.2.0/26</li> <li>Worker subnet: 10.0.2.128/26</li> </ul> <p>The following diagram shows how you can calculate the subnets in your environment:</p> <p></p> <p>NOTE: Azure requires the first five or so IPs in the subnet to be allocated to Azure.</p>"},{"location":"azure/SetupBastionEnvironment/#create-the-environment-variables-you-will-usse","title":"Create the environment variables you will usse","text":"<p>Open the shell or your development computer and start Bash. Set the environment variables to use:</p> <pre><code>LOCATION=eastus\nLOCATION_ABBR=eus\nBASTION_RG=rg-costco-issue80-1\nBASTION_VNET=vnet-$LOCATION_ABBR-bastion-01\nBASTION_VM_SUBNET=sub-$LOCATION_ABBR-bastion-01\nBASTION_SUBNET=AzureBastionSubnet\nBASTION=b-$LOCATION_ABBR-bastion-01\nBASTION_IP=ip-$LOCATION_ABBR-bastion-01\nIP=ip-$LOCATION_ABBR-bastion-01\n\n\nBASTION_VM_NAME=vm-bastion-01\n\n## Base virtual network for Bastion: 10.0.0.0/24\n## Bastion subnet: 10.0.0.64/26\n## Bastion VM subnet: 10.0.0.128/26\nBASE_VNET_IP_ADDRESS=\"10.0.0.0/24\"\nBASTION_SUBNET_IP_ADDRESS=\"10.0.0.64/26\"\nBASTION_VM_SUBNET_IP_ADDRESS=\"10.0.0.128/26\"\n</code></pre>"},{"location":"azure/SetupBastionEnvironment/#set-up-a-linux-vm-in-the-same-virtual-network","title":"Set up a Linux VM in the same virtual network","text":"<p>Create a virtual machine using the CLI.</p> <pre><code># az group create --name $BASTION_RG --location $LOCATION\n\naz network vnet create --resource-group $BASTION_RG --name  $BASTION_VNET \\\n  --address-prefix $BASE_VNET_IP_ADDRESS \\\n  --subnet-name $BASTION_VM_SUBNET --subnet-prefix $BASTION_VM_SUBNET_IP_ADDRESS\n</code></pre> <p>The <code>--generate-ssh-keys</code> parameter is used to automatically generate an SSH key, and put it in the default key location (~/.ssh). </p> <p>For more infomration, see Quickstart: Create a Linux virtual machine with the Azure CLI</p>"},{"location":"azure/SetupBastionEnvironment/#set-up-bastion","title":"Set up Bastion","text":"<p>Set up the Bastion.</p> <pre><code>az network vnet subnet create --resource-group $BASTION_RG --vnet-name $BASTION_VNET \\\n  --name $BASTION_SUBNET --address-prefix $BASTION_SUBNET_IP_ADDRESS\naz network public-ip create --resource-group $BASTION_RG --name $BASTION_IP \\\n  --sku Standard --location $LOCATION\naz network bastion create --name $BASTION --public-ip-address $BASTION_IP --resource-group $BASTION_RG \\\n  --vnet-name $BASTION_VNET  --location $LOCATION\n</code></pre> <p>For more information, see Deploy Bastion using Azure CLI</p>"},{"location":"azure/SetupBastionEnvironment/#how-to-copy-and-paste","title":"How to copy and paste","text":"<p>See Bastion VM Copy Paste.</p>"},{"location":"azure/SetupBastionEnvironment/#install-openshift-virtual-network","title":"Install OpenShift virtual network","text":"<ul> <li>Base virtual network for OpenShift: 10.0.2.0/24</li> <li>Control plane subnet: 10.0.2.0/26</li> <li>Worker subnet: 10.0.2.128/26</li> </ul> <pre><code>OPENSHIFT_RG=$BASTION_RG\nINSTANCE=06\nLOCATION_ABBR=eus\nOPENSHIFT_VNET_NAME=vnet-$LOCATION_ABBR-openshift-$INSTANCE\nOPENSHIFT_BASE_VNET_ADDR=\"10.0.2.0/24\"\nOPENSHIFT_CONTROLPLANESUBNET_NAME=sub-$LOCATION_ABBR-openshift-controlplane-$INSTANCE\nOPENSHIFT_CONTROLPLANE_SUBNET_ADDR=\"10.0.2.0/26\"\nOPENSHIFT_WORKER_SUBNET_NAME=sub-$LOCATION_ABBR-openshift-worker-$INSTANCE\nOPENSHIFT_WORKER_SUBNET_ADDR=\"10.0.2.128/26\"\n\n# Base virtual network for OpenShift: 10.0.2.0/24\n# Control plane subnet: 10.0.2.0/26\n# Worker subnet: 10.0.2.128/26\naz network vnet create --resource-group $OPENSHIFT_RG --name  $OPENSHIFT_VNET_NAME \\\n  --address-prefix $OPENSHIFT_BASE_VNET_ADDR \\\n  --subnet-name $OPENSHIFT_CONTROLPLANESUBNET_NAME --subnet-prefix $OPENSHIFT_CONTROLPLANE_SUBNET_ADDR\n\naz network vnet subnet create --resource-group $OPENSHIFT_RG --vnet-name $OPENSHIFT_VNET_NAME \\\n  --name $OPENSHIFT_WORKER_SUBNET_NAME --address-prefix $OPENSHIFT_WORKER_SUBNET_ADDR\n</code></pre>"},{"location":"azure/SetupBastionEnvironment/#peer-networks","title":"Peer networks","text":"<p>Connect virtual networks to each other with virtual network peering. Once virtual networks are peered, resources in both virtual networks are able to communicate with each other, with the same latency and bandwidth as if the resources were in the same virtual network.</p> <pre><code># Get the id for myVirtualNetwork1.\nvNet1Id=$(az network vnet show \\\n  --resource-group $OPENSHIFT_RG \\\n  --name $OPENSHIFT_VNET_NAME \\\n  --query id --out tsv)\n\n# Get the id for myVirtualNetwork2.\nvNet2Id=$(az network vnet show \\\n  --resource-group $BASTION_RG \\\n  --name $BASTION_VNET \\\n  --query id \\\n  --out tsv)\n\naz network vnet peering create \\\n  --name $OPENSHIFT_VNET_NAME-$BASTION_VNET \\\n  --resource-group $BASTION_RG  \\\n  --vnet-name $BASTION_VNET \\\n  --remote-vnet $OPENSHIFT_VNET \\\n  --allow-vnet-access\n</code></pre> <p>For more information, see Connect virtual networks with virtual network peering using the Azure CLI</p>"},{"location":"azure/SetupBastionEnvironment/#install-openshift","title":"Install OpenShift","text":"<p>To install OpenShift from the Basion:</p> <ol> <li>Generate your ssh key required for OpenShift.</li> <li>Get the OpenShift entitlement key</li> </ol> <p>Do Install OCP on Azure for Ops and Install OpenShift on Azure.</p>"},{"location":"azure/SetupBastionEnvironment/#configure-vnet-peering","title":"Configure VNet peering","text":"<p>To configure VNet peering to connect your OpenShift worker nodes to computers in other virtual networks:</p> <ol> <li>Verify that you have configured VNets, and virtual machines within the VNets.</li> <li>Configure VNet peering.</li> <li>Configure Bastion in one of the VNets.</li> <li>Verify permissions.</li> <li>Connect to a VM via Azure Bastion. In order to connect via Azure Bastion, you must have the correct permissions for the subscription you are signed into.</li> </ol> <p>For more information, see VNet peering and Azure Bastion</p>"},{"location":"azure/SetupBastionEnvironment/#contributors","title":"Contributors","text":"<ul> <li>Bruce Kyle</li> <li>Volodymyr Rozdolsky</li> <li>Hamza Elgindy</li> </ul> <p>March 22, 2022</p>"},{"location":"azure/aks-terraform/","title":"Walkthrough: Create Azure Kubernetes Service (AKS) using Terraform","text":"<p>When you are building your cloud infrastructure, you can think of it as code. Infrastructure as code means that the virtual machines, networking, and storage can all be thought of as code. On Azure, you can build your infrastructure using Azure Resource Manager (ARM) templates and deploy using PowerShell. You could also use PowerShell or Azure CLI to express your infrastructure. Many enterprises use Terraform, an open source infrastructure as code provider by HashiCorp, to build, change, version cloud infrastructure.</p> <p>You can use Terraform across multiple platforms, including Amazon Web Services, IBM Cloud (formerly Bluemix), Google Cloud Platform, DigitalOcean, Linode, Microsoft Azure, Oracle Cloud Infrastructure, OVH, Scaleway VMware vSphere or Open Telekom Cloud, OpenNebula and OpenStack. In this article, we\u2019ll explore Azure. At a high level, you write the configuration of your infrastructure in Terraform files that can describe the infrastructure of a single application or of your entire data center, and then apply it to the target cloud (in this case Azure).</p> <p>In this article, you install Terraform and configure it, create the Terraform configuration plans for two resource groups an AKS cluster and Azure Log Analytics workspace, and apply the plans into Azure.</p> <p>When you deploy the infrastructure, Terraform figures out the what changes changes to reach the desired state and then executes your configuration to build your cloud in its desired state.</p> <p>In this article you will use a new JSON-like syntax of HCL to describe your infrastructure, then use the Terraform command line tool to build an execution plan, create a resource graph of your resources. You apply the plan for Terraform tp figure out what to change and in what order, and then will execute those changes.</p>"},{"location":"azure/aks-terraform/#prerequisites","title":"Prerequisites","text":"<p>You will need:</p> <ul> <li>An Azure subscription and general understanding of Azure resource groups and resources.</li> <li>Contributor role access on the subscription.</li> <li>This demo uses Ubuntu 18.04 LTS on Linux on WSL on Windows or an similar Bash shell.</li> <li>The Azure CLI.</li> <li>Azure CLI for AKS so you can use kubectl. To install, use: <code>az aks install-cli</code>.</li> <li>Visual Studio Code or another editor.</li> <li>The lightweight and flexible command-line JSON processor, jq.</li> <li>To use the graphics, install GraphViz.</li> </ul> <p>For many of the requirements, you could use Azure Cloud Shell. Azure CLI, Terraform, and jq are already installed in Cloud Shell. If you are using Visual Studio Code, optionally install the Azure Terraform extension from Microsoft, <code>ms-azuretools.vscode-azureterraform</code>. The extension lets you run Terraform commands from inside VS Code, such as : <code>init</code>, <code>plan</code>, <code>apply</code>, <code>validate</code>, <code>refresh</code> and <code>destroy</code>. It also includes a visualizer.</p>"},{"location":"azure/aks-terraform/#core-terraform-workflow","title":"Core Terraform workflow","text":"<p>The core Terraform workflow has three steps:</p> <ol> <li>Write. Author infrastructure as code.</li> <li>Plan. Preview changes before applying.</li> <li>Apply. Provision reproducible infrastructure.</li> </ol> <p>In this article you will repeat the pattern to install the Resource group, the Log Analytics workspace, and AKS. But first, let\u2019s set up Terraform and configure Terraform for Azure.</p>"},{"location":"azure/aks-terraform/#install-terraform","title":"Install Terraform","text":"<p>Use these steps to install Terraform:</p> <ol> <li>Download Terraform for your platform.</li> <li>Unzip the file</li> <li>Either moving it to a directory included in your system\u2019s <code>PATH</code> or add the location where you unzipped Terraform to your <code>PATH</code>.</li> <li>Verify Terraform is running using <code>terraform --version</code>.</li> </ol> <pre><code>terraform \u2013version\n</code></pre>"},{"location":"azure/aks-terraform/#configure-terraform-for-azure","title":"Configure Terraform for Azure","text":"<p>To create resources in Azure, Terraform will need permissions. In this Terraform walkthrough, use a service principle. The security principal defines the access policy and permissions for the user or application in the Azure AD tenant.</p> <p>To configure Terraform you will need to:</p> <ol> <li>Create a service principal for Terraform to access Azure.</li> <li>Set the service principal to environment variables that Terraform uses.</li> </ol> <p>The following sample creates your service principal using <code>az ad sp create-for-rbac</code>.</p> <pre><code>## Requires jq .. to install see: https://stedolan.github.io/jq/download/\n\n# set some environment variables to use to create the service principal\nexport SUBSCRIPTION_ID=3464892e-e827-4752-bad5-b4f93c00dbbe\nexport PROJECT_NAME=\"wus2-azure-aks-terraform-demo\"\n\naz account set --subscription=\"${SUBSCRIPTION_ID}\"\n\n# create the service principal to the subscription scope and save it to an auth file\nTF_SERVICE_PRINCIPAL=$(az ad sp create-for-rbac --skip-assignment --role 'Contributor' --name rbac-tf-$PROJECT_NAME --output json --scopes=\"/subscriptions/${SUBSCRIPTION_ID}\")\n\nexport ARM_SUBSCRIPTION_ID=$SUBSCRIPTION_ID\nexport ARM_CLIENT_ID=$(echo $TF_SERVICE_PRINCIPAL | jq '.appId')\nexport ARM_CLIENT_SECRET=$(echo $TF_SERVICE_PRINCIPAL | jq '.password')\nexport ARM_TENANT_ID=$(echo $TF_SERVICE_PRINCIPAL | jq '.tenant')\n\n# Not needed for public, required for usgovernment, german, china\nexport ARM_ENVIRONMENT=public\n</code></pre> <p>It uses a name for you to identify it within Azure Active Directory. The service principal must have contributor permission in the subscription in order to create resource groups. <code>--skip-assignment</code> allows the service principal to access resources under the current subscription.</p> <p>Next, the sample sets the environment variables that Terraform uses. The Azure Terraform modules uses the following environment variables.</p> <ul> <li><code>ARM_SUBSCRIPTION_ID</code></li> <li><code>ARM_CLIENT_ID</code></li> <li><code>ARM_CLIENT_SECRET</code></li> <li><code>ARM_TENANT_ID</code></li> <li><code>ARM_ENVIRONMENT</code></li> </ul> <p>Use <code>ARM_ENVIROMENT</code> to specify the cloud, such as China, Germany, or GovCloud.</p> <p>Next although not required for the resource group, you will soon need to create an SSH key. You will use when you create your Kubernetes cluster.</p> <pre><code>ssh-keygen -t rsa -b 4096 -m PEM\n\n# display the public key\ncat $HOME/.ssh/id_rsa.pub\n</code></pre> <p>The public key is put into your home directory <code>~/.ssh/id_rsa.pub</code>.</p>"},{"location":"azure/aks-terraform/#create-a-resource-group-using-hcl","title":"Create a resource group using HCL","text":"<p>In this step, you will use HashiCorp Configuration Language (HCL) to define a resource group and then use Terraform to deploy the resource group to Azure. The syntax of HCL is similar to JSON, but adds the idea of providing names to the object. The steps to create a resource group are:</p> <ol> <li>Define the Azure provider in a file named <code>main.tf</code></li> <li>Create the resource group in a file named <code>resourcegroup.tf</code></li> <li>Define the variables used by the resource group definition in a file named <code>variables.tf</code></li> <li>Create a Terraform plan</li> <li>Apply the Terraform plan into Azure</li> </ol>"},{"location":"azure/aks-terraform/#create-maintf","title":"Create main.tf","text":"<p>Let\u2019s first create a directory and use your editor to create a file named <code>main.tf</code> Start by creating the directory and opening your editor. https://gist.github.com/brucedkyle/121d11c763b5182cdaaa82b6b1b08ec8#file-create-main-bash In main.tf, define the Terraform version and that you are targeting Azure.</p> <pre><code>provider \"azurerm\" {\n  # The \"feature\" block is required for AzureRM provider 2.x. \n  # If you are using Terraform version 1.x, the \"features\" block is not allowed.\n  version = \"~&gt;2.0\"\n  features {}\n}\n</code></pre> <p>Save the file and exit the editor.</p>"},{"location":"azure/aks-terraform/#create-resourcegrouptf","title":"Create resourcegroup.tf","text":"<p>Next, create <code>resourcegroup.tf</code> in the same directory.</p> <pre><code>code resourcegroup.tf\n</code></pre> <p>Use the following HCL in resouregroup.tf:</p> <pre><code># Defines the main resource group\n\nresource \"azurerm_resource_group\" \"rg\" {\n  name = \"${var.resource_group_prefix}${var.project_name}-${var.environment}\"\n  location = var.project_location\n  tags = {\n        \"Cost Center\" = var.project_name\n        Environment = var.environment\n        Team = \"infrastructure\"\n        Project = var.project_name\n    }\n}\n</code></pre> <p>Save the file. Notice that Terraform uses its own format where <code>resource \"azurerm_resource_group\"</code> provides Terraform with kind of resource to create and <code>\"rg\"</code> is a name you can use later to reference this particular resource group.</p>"},{"location":"azure/aks-terraform/#create-variablestf","title":"Create variables.tf","text":"<p>Next, create a new file for the variables named <code>variables.tf</code>.</p> <pre><code>code variables.tf\n</code></pre> <p>Use the following content to set the default values of the variables. There are more variables that you need to create a resource group.. There are more variables that you need to create a resource group.</p> <pre><code>variable resource_group_prefix {\n  default = \"rg-\"\n}\n\nvariable project_name {\n  default = \"wus2-aksdemo\"\n}\n\nvariable project_location {\n  default = \"West US 2\"\n}\n\nvariable environment {\n  default = \"devtest\"\n}\n\nvariable company_name {\n  default = \"azuredays\"\n}\n</code></pre> <p>Use a project name based on your naming conventions. In this case, the name includes the region, project name, and environment. Use your own company name without spaces.</p> <p>Save the file.</p>"},{"location":"azure/aks-terraform/#create-a-terraform-plan","title":"Create a Terraform plan","text":"<p>Next, we will take several steps to first initialize\u00a0Terraform, then have Terraform figure out the dependencies and the differences between what exists in Azure. create a plan, then apply the plan into Azure. Here are the steps:</p> <ol> <li>Initiate Terraform plan using <code>terraform init</code></li> <li>Use Terraform to create a plan for creating the resource groups by using <code>terraform plan</code></li> <li>Apply the plan to Azure by using <code>terraform apply</code></li> <li>Query Azure to see that the resource group was created</li> </ol> <p>To initialize Terraform to be sure you have all of the requirements in place.</p> <pre><code>terraform init\n</code></pre> <p>You will see this response.</p> <p></p> <p>When you are building larger infrastructure, you may want to check for typos and be sure you have all your variables. You can validate using:</p> <pre><code>terraform validate\n</code></pre>"},{"location":"azure/aks-terraform/#create-a-terraform-plan_1","title":"Create a Terraform plan","text":"<p>Terraform compares the requested resources to the state information saved by Terraform and then outputs the planned execution. You have not yet created your Azure resources.</p> <pre><code>terraform plan -var project_name=$PROJECT_NAME -out out.plan\n</code></pre> <p>Note that in this command, you change the <code>project_name</code> variable from the command line. And you send the output to an <code>out.plan</code> file.</p> <p>Once you run the command, you should see the result of the plan, shown in the following illustration.</p> <p></p>"},{"location":"azure/aks-terraform/#apply-the-terraform-plan","title":"Apply the Terraform plan","text":"<p>Once the plan is created, you can then apply the plan.</p> <pre><code>terraform apply \"out.plan\"\n</code></pre> <p>Terraform will ask to confirm that you want to make the changes.</p> <p>You have now deployed your resource groups. you can test that the resource group was created. Use <code>terraform show</code> to see the state of your Terraform deployment.</p> <pre><code>terraform show\n</code></pre> <p>Terraform will show the resource it has created.</p> <p></p> <p>And you can check it in Azure. The following code returns a list of resource groups with the <code>Project</code> tag set to the project name.</p> <pre><code>az group list --subscription $SUBSCRIPTION_ID --tag \"Project=$PROJECT_NAME\"\n</code></pre> <p>The result displays the resource group properties from the Azure CLI.</p> <p></p> <p>In this step, you defined the resources, created a plan, and applied the plan into Azure. Next, let\u2019s add resources to create the Log Analytics workspace to monitor the Azure Kubernetes Service (AKS) cluster.</p>"},{"location":"azure/aks-terraform/#create-log-analytics-workspace-using-hcl","title":"Create Log Analytics workspace using HCL","text":"<p>It a best practice to provide log analytics for your container. To give you visibility into the performance of your cluster, Azure Monitor for containers collects memory and processor metrics from controllers, nodes, and containers that are available in Kubernetes. Container logs are also collected.</p> <p>This following chart from the Azure documentation shows the various ways metrics and logs are gathered.</p> <p></p> <p>To create log analytics resources, use your text editor to add a new HCL file named <code>loganalytics.tf</code> in the same directory.</p> <pre><code>### For Log Analytics Workspace\nvariable log_analytics_workspace_prefix {\n    default = \"workspace-\"\n}\n\n# refer https://azure.microsoft.com/global-infrastructure/services/?products=monitor for log analytics available regions\nvariable log_analytics_workspace_location {\n    default = \"Central US\"\n}\n\n# refer https://azure.microsoft.com/pricing/details/monitor/ for log analytics pricing \nvariable log_analytics_workspace_sku {\n    default = \"PerGB2018\"\n}\n</code></pre> <p>Save the file. Note that in this configuration file, you create a shared resource group and create log analytics workspace and logs for multiple applications to use.</p> <p>As the final step in this section, validate, create a plan into an file named <code>out.plan</code>, and then apply that plan.</p> <pre><code># init\nterraform init\n\n# verify\nterraform validate\n\n# plan and send the plan to an out.plan file\nterraform plan -var project_name=$PROJECT_NAME -out out.plan\n\n# apply the plan from the out.plan file\nterraform apply out.plan\n\nterraform show\n</code></pre> <p>The result is that two resources are created in the shared resource group. You can find yours in the Azure portal.</p> <p></p> <p>In this step, you created and HCL file configuring the Log Analytics resources, created an out plan, and then applied the plans.</p> <p>Next, let\u2019s create the Kubernetes cluster.</p>"},{"location":"azure/aks-terraform/#create-kubernetes-cluster-using-hcl","title":"Create Kubernetes cluster using HCL","text":"<p>You are now ready to create the cluster. This will be take a few steps:</p> <ol> <li>Create a service principal for Azure to use to create the cluster</li> <li>Add a HCL file to configure the AKS resources</li> <li>Add a HCL file to define the output from Terraform</li> <li>Add a HCL variables file for AKS parameters</li> <li>Validate, plan, and apply the resources</li> </ol> <p>Let\u2019s complete each step.</p> <p>Step 1. Create the service principal and assign to environment variables. You will send those environment variables into the Terraform plan.</p> <pre><code>#create service principal for AKS to use\naz ad sp create-for-rbac --name \"rbac-$PROJECT_NAME\" --skip-assignment\n\n# retrieve the appId and the password \nAKS_SERVICE_PRINCIPAL=$(az ad sp list --display-name \"rbac-$PROJECT_NAME\" --query \"[].{id:appId, id.password}\" --output json)\nexport TF_VAR_client_id=$(echo $AKS_SERVICE_PRINCIPAL | jq '.appId')\nexport TF_VAR_client_secret=$(echo $AKS_SERVICE_PRINCIPAL | jq '.password')\n\nexport TF_VAR_client_id=$(echo $TF_SERVICEPRINCIPAL | jq '.appId')\nexport TF_VAR_client_secret=$(echo $TF_SERVICEPRINCIPAL | jq '.password')\n\n# get the public key file location\nexport TF_VAR_ssh_public_key=$HOME/.ssh/id_rsa.pub\n</code></pre> <p>This is a different service principal from the one used in a previous section. The service principal in the previous section gave Terraform permissions to interact with Azure Resource Manager. This one gives the AKS service permission to create resources.</p> <p>Step 2. To create the cluster add a file named <code>aks.tf</code> in the same folder that contains the Terraform configuration for the cluster. The file provides the configuration for the cluster. Use the following code.</p> <pre><code>resource \"azurerm_kubernetes_cluster\" \"k8s\" {\n    name                = var.cluster_name\n    location            = azurerm_resource_group.rg_aks.location\n    resource_group_name = azurerm_resource_group.rg_aks.name\n    dns_prefix          = var.dns_prefix\n\n    linux_profile {\n        admin_username = var.admin_name\n\n        ssh_key {\n            key_data = file(var.ssh_public_key)\n        }\n    }\n\n    default_node_pool {\n        name            = \"agentpool\"\n        node_count      = var.agent_count\n        vm_size         = \"Standard_F1\"\n    }\n\n    service_principal {\n        client_id     = var.client_id\n        client_secret = var.client_secret\n    }\n\n    addon_profile {\n        oms_agent {\n        enabled                    = true\n        log_analytics_workspace_id = azurerm_log_analytics_workspace.test.id\n        }\n    }\n\n    tags = {\n        Environment = var.environment\n    }\n}\n</code></pre> <p>Step 3. Next, provide a file that describes the output. The file, <code>output.tf</code> tells Terraform the values to return. It will provide the information you will need to log into the cluster.</p> <pre><code>output \"client_key\" {\n    value = azurerm_kubernetes_cluster.k8s.kube_config.0.client_key\n}\n\noutput \"client_certificate\" {\n    value = azurerm_kubernetes_cluster.k8s.kube_config.0.client_certificate\n}\n\noutput \"cluster_ca_certificate\" {\n    value = azurerm_kubernetes_cluster.k8s.kube_config.0.cluster_ca_certificate\n}\n\noutput \"cluster_username\" {\n    value = azurerm_kubernetes_cluster.k8s.kube_config.0.username\n}\n\noutput \"cluster_password\" {\n    value = azurerm_kubernetes_cluster.k8s.kube_config.0.password\n}\n\noutput \"kube_config\" {\n    value = azurerm_kubernetes_cluster.k8s.kube_config_raw\n}\n\noutput \"host\" {\n    value = azurerm_kubernetes_cluster.k8s.kube_config.0.host\n}\n</code></pre> <p>The output keyword in HCL specifies the output attributes. The values can be traced from the resource <code>azurerm_kubernetes_cluster.k8s</code> that you created in your resource. The attributes are exported by Terraform. In this case, the values export attributes of the Kubernetes cluster resource. In particular, the values are found in the the <code>kube_config</code> and the <code>kube_config_raw</code> attributes.</p> <p>Step 4. Next you will want a file that provides the variables. Create a file named <code>var-aks.tf</code> with the following content.</p> <pre><code>variable \"client_id\" {}\nvariable \"client_secret\" {}\n\nvariable \"agent_count\" {\n    default = 3\n}\n\nvariable \"ssh_public_key\" {\n    default = \"~/.ssh/id_rsa.pub\"\n}\n\nvariable \"dns_prefix\" {\n    default = \"aksdemo\"\n}\n\nvariable cluster_name {\n    default = \"aksdemo\"\n}\n</code></pre> <p>Step 5. Validate, plan, and apply the resources. You can either provide the variables for which there is no defined default value, or provide them on the command line when you construction the plan. One of the important variables is the SSH key file location. Let\u2019s set those variables using the environment variables that you created in the first steps. In this case, read in the public ssh key locally.</p> <pre><code>terraform plan -var project_name=$PROJECT_NAME -var 'client_id=$TF_VAR_client_id' -var 'client_secret=$TF_VAR_client_secret' -var 'ssh_public_key=$TF_VAR_ssh_public_key' -out out.plan\n</code></pre>"},{"location":"azure/aks-terraform/#log-into-the-cluster","title":"Log into the cluster","text":"<p>Now that we\u2019ve stood up the cluster, let\u2019s log in. The values you need to log in was generated by Step 5 in the previous section where you applied the Terraform configuration to Azure. You need to extract the values that Terraform generated as output and use those data to log in to the cluster.</p> <p>The following code shows how to get the output using <code>terraform output kube_config</code>, which extracts the value of an output variable from the state file. Next, set the values inside <code>kube_config</code> into a file named <code>azurek8s</code>. Then you can then set an environment variable, <code>KUBECONFIG</code> that <code>kubectl</code> recognizes as containing the secrets needed to log into the cluster.</p> <pre><code># extracts the value of an output variable kube_config from the state file\necho \"$(terraform output kube_config)\" &gt; ./azurek8s\n\n# set the KUBECONFIG to that file\nexport KUBECONFIG=./azurek8s\n\n# Log into the cluster using the KUBECONFIG data\nkubectl get nodes\n</code></pre> <p>For more information about the <code>KUBECONFIG</code> environment variable, see Organizing Cluster Access Using kubeconfig Files.</p> <p>When all goes well, you log into your cluster and see the nodes. At this point you can use <code>kubedtl</code> commands to install and operation your Kubernetes Deployments and Services.</p>"},{"location":"azure/aks-terraform/#visualize-the-dependencies-terraform","title":"Visualize the dependencies Terraform","text":"<p>Create an SVG file of the Terraform dependencies. You can view the SVG file in your favorite browser.</p> <pre><code># install GraphViz\nsudo apt install graphviz\n\n# creates a dot file that can be translated by GraphViz\nterraform graph | dot -Tsvg &gt; graph.svg\n\n# open graph.svg in your favorite browser\n</code></pre> <p>In this tutorial, you used Terraform created a resource group, log analytics workspace, and Kubernetes cluster, and logged into the cluster to see the nodes.</p> <p>And you learned that Terraform creates a plan that notifies you of destructive changes before you apply the plan.</p>"},{"location":"azure/aks-terraform/#next-steps","title":"Next steps","text":"<ul> <li>This sample shows how you can deploy a basic AKS. To use RBAC, you will want to provide additional information. See Create a RBAC Azure Kubernetes Services (AKS) cluster with Azure Active Directory using Terraform on GitHub.</li> <li>If you are working as a team, you will probably want to use Terraform backend to store the state of your Azure deployments. For more information, see azurerm backend.</li> <li>Learn how to Deploying Terraform Infrastructure using Azure DevOps Pipelines Step by Step (Advanced)</li> <li>Explore the existing Terraform configuration files available on GitHub. See terraform-provider-azurerm</li> </ul>"},{"location":"azure/aks-terraform/#references","title":"References","text":"<ul> <li>The Core Terraform Workflow</li> <li>Terraform on Azure documentation</li> <li>Tutorial: Create a Kubernetes cluster with Azure Kubernetes Service using Terraform</li> <li>Azure Provider in the Terraform documentation.</li> </ul>"},{"location":"azure/aks/","title":"Walkthrough: Create Azure Kubernetes Service (AKS) using ARM template","text":"<p>Azure Kubernetes Service (AKS) provides a hosted Kubernetes service where Azure handles critical tasks like health monitoring and maintenance for you. AKS reduces the complexity and operational overhead of managing Kubernetes by offloading much of that responsibility to Azure. When you create AKS, Azure provides the Kubernetes control plane. You need manage only the agent nodes within your clusters.</p> <p>There are several ways to deploy to Azure, including using the portal, Azure CLI, Azure PowerShell, and Terraform.</p> <p>In this walkthrough, you will create an AKS cluster using an ARM template and then use Azure CLI to deploy a simple application to the cluster. You will review the design decisions made for the walkthrough, see how the template supports Kubenet for Kubernetes networking, role-based-access-control (RBAC) and how it supports managed identities to communicate with other Azure resources. Finally, you will use a Kubernetes manifest file to define the desired state of the cluster, and test the application.</p> <p>An ARM template sets you up to being able to deploy AKS clusters and your applications into Pods in your Azure DevOps pipeline or using GitHub actions. You can set the parameters to define the characteristics of the cluster at deployment time.</p>"},{"location":"azure/aks/#prerequisites","title":"Prerequisites","text":"<p>To run this walkthrough, you will need:</p> <ul> <li>Azure Account.</li> <li>Have basic understanding of Azure resource groups and ARM templates</li> <li>Understand how to navigate the Azure portal.</li> <li>To have completed Quickstart: Deploy an Azure Kubernetes Service (AKS) cluster using an ARM template. The quickstart provides a quick introduction to the concepts in this article.</li> <li>To run locally, you will need:<ul> <li>Azure CLI, version 2.8.0 or later.</li> <li>Bash. For Windows, Windows Services for Liniux (WSL) or GitBash.</li> </ul> </li> <li>To run in Azure Portal, start Azure Cloud Shell and run in the CLI</li> <li>A high level understanding of Azure Role Based Access Control (RBAC), the concept of a built-in role, what an objectId is.</li> </ul>"},{"location":"azure/aks/#design-considerations","title":"Design considerations","text":"<p>This walkthrough implements the following design considerations:</p> <ul> <li>Kubenet. In the example, use Kubenet networking where Azure manages the virtual network resources as the cluster is being deployed. The alternative is Azure Container Networking Interface (CNI) plugin. With CNI pods receive individual IP addresses that can route to other network services. For more information, see Choose the appropriate network model.</li> <li>Managed identities. AKS requires additional resources like load balancers and managed disks in Azure. To create these resources, Azure uses either a service principal or a managed identity. If you use managed identity, you do no need to manage a service principal. For more information, see Use managed identities in Azure Kubernetes Service.</li> <li>Kubernetes Role-based-access-control (RBAC). Kubernetes provides its own role-based-access control when enabled. It is the way you can dynamically configure policies that regulating access to computer or network resources based on the roles of individual users within your organization. For more information, see Using RBAC Authorization in the Kubernetes documentation.</li> <li>Linux nodes. You can run Windows nodes in your AKS cluster that uses CNI, but in this walkthrough we will use Linux.</li> <li>Virtual machine scale set. Because we are deploying a single node, you may need to adjust the number of nodes that run your workloads. The cluster autoscaler component can watch for Pods in your cluster that can\u2019t be scheduled because of resource constraints. For more information, see Automatically scale a cluster to meet application demands on Azure Kubernetes Service (AKS).</li> </ul> <p>In a future blog post, you will learn now to assign an admin group to manage Kubernetes using Azure Active Directory.</p> <p>Let\u2019s first look at the steps in the walkthrough.</p>"},{"location":"azure/aks/#steps-in-the-walkthrough","title":"Steps in the walkthrough","text":"<p>You will take the following steps to deploy:</p> <ol> <li>Start your command prompt</li> <li>Set environment variables, log in to Azure, and create a resource group</li> <li>Review the ARM template</li> <li>Deploy the ARM template</li> <li>Review what was deployed</li> <li>Log in to the cluster</li> <li>Connect to the cluster</li> <li>Run an application</li> </ol>"},{"location":"azure/aks/#start-your-command-prompt","title":"Start your command prompt","text":"<p>Open your command prompt on your local machine or start Azure Cloud Shell. On Windows you can use Windows Subsystem for Linux (WSL) or Bash.</p> <p>Let\u2019s begin by creating an SSH key pair.</p>"},{"location":"azure/aks/#set-environment-variables-log-in-create-resource-group","title":"Set environment variables, log in, create resource group","text":"<p>Next, use the following code to log in, set environment variables, and create the resource group:</p> <pre><code>SUBSCRIPTION_ID=&lt;your subscription id&gt;\nRESOURCE_GROUP_NAME=\"aks-eus2-aksdays-demo-01\"\nAKS_NAME=\"aks-eus2-aksdays-demo-01\"\nLOCATION=\"east us 2\"\nNODE_SIZE=\"Standard_B2s\"\nDEPLOYMENT_NAME=$AKS_NAME-$RANDOM\n\naz account set --subscription $SUBSCRIPTION_ID\naz group create --name $RESOURCE_GROUP_NAME --location \"$LOCATION\"\n</code></pre>"},{"location":"azure/aks/#review-the-arm-template","title":"Review the ARM Template","text":"<p>Next copy the ARM template into your directory. In Cloud Shell, you can upload the file.</p> <pre><code>{\n    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n    \"contentVersion\": \"1.0.0.0\",\n    \"parameters\": {\n        \"aksClusterName\": {\n            \"type\": \"string\",\n            \"defaultValue\": \"aks101cluster-vmss\",\n            \"metadata\": {\n                \"description\": \"The name of the Managed Cluster resource.\"\n            }\n        },\n        \"location\": {\n            \"defaultValue\": \"[resourceGroup().location]\",\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"The location of AKS resource.\"\n            }\n        },\n        \"dnsPrefix\": {\n            \"type\": \"string\",\n            \"metadata\": {\n                \"description\": \"Optional DNS prefix to use with hosted Kubernetes API server FQDN.\"\n            }\n        },\n        \"osDiskSizeGB\": {\n            \"type\": \"int\",\n            \"defaultValue\": 0,\n            \"metadata\": {\n                \"description\": \"Disk size (in GiB) to provision for each of the agent pool nodes. This value ranges from 0 to 1023. Specifying 0 will apply the default disk size for that agentVMSize.\"\n            },\n            \"minValue\": 0,\n            \"maxValue\": 1023\n        },\n        \"agentCount\": {\n            \"type\": \"int\",\n            \"defaultValue\": 3,\n            \"metadata\": {\n                \"description\": \"The number of nodes for the cluster. 1 Node is enough for Dev/Test and minimum 3 nodes, is recommended for Production\"\n            },\n            \"minValue\": 1,\n            \"maxValue\": 100\n        },\n        \"agentVMSize\": {\n            \"type\": \"string\",\n            \"defaultValue\": \"Standard_DS2_v2\",\n            \"metadata\": {\n                \"description\": \"The size of the Virtual Machine.\"\n            }\n        },\n        \"osType\": {\n            \"type\": \"string\",\n            \"defaultValue\": \"Linux\",\n            \"allowedValues\": [\n                \"Linux\",\n                \"Windows\"\n            ],\n            \"metadata\": {\n                \"description\": \"The type of operating system.\"\n            }\n        }\n    },\n    \"resources\": [\n        {\n            \"apiVersion\": \"2020-09-01\",\n            \"type\": \"Microsoft.ContainerService/managedClusters\",\n            \"location\": \"[parameters('location')]\",\n            \"name\": \"[parameters('aksClusterName')]\",\n            \"tags\": {\n                \"displayname\": \"AKS Cluster\"\n            },\n            \"identity\": {\n                \"type\": \"SystemAssigned\"\n            },\n            \"properties\": {\n                \"kubernetesVersion\": \"1.18.8\",\n                \"enableRBAC\": true,\n                \"dnsPrefix\": \"[parameters('dnsPrefix')]\",\n                \"agentPoolProfiles\": [\n                    {\n                        \"name\": \"agentpool\",\n                        \"osDiskSizeGB\": \"[parameters('osDiskSizeGB')]\",\n                        \"count\": \"[parameters('agentCount')]\",\n                        \"vmSize\": \"[parameters('agentVMSize')]\",\n                        \"osType\": \"[parameters('osType')]\",\n                        \"storageProfile\": \"ManagedDisks\",\n                        \"type\": \"VirtualMachineScaleSets\",\n                        \"mode\": \"System\"\n                    }\n                ]\n            }\n        }\n    ],\n    \"outputs\": {\n        \"controlPlaneFQDN\": {\n            \"type\": \"string\",\n            \"value\": \"[reference(resourceId('Microsoft.ContainerService/managedClusters/', parameters('aksClusterName'))).fqdn]\"\n        }\n    }\n}\n</code></pre> <p>Let\u2019s review some key lines in the ARM template. You will need to pass in:</p> <ul> <li><code>aksClusterName</code>. The name of the managed cluster resource. The name is used to create the resource group containing the nodes.</li> <li><code>dnsPrefix</code>. DNS prefix specified when creating the managed cluster.</li> </ul> <p>In the template itself, the following properties are set in the <code>Microsoft.ContainerService/managedClusters</code> resource:</p> <ul> <li><code>identity</code>. The identity is system assigned managed identity. Credential rotation for managed identity happens automatically every 46 days according to Azure Active Directory default. For more information how managed identities are used in AKS including the permissions granted to the managed identity, see Summary of managed identities for Kubernetes.</li> <li><code>kubernetesVersion</code>. The version number needs to include <code>[major].[minor].[patch]</code>. To check for supported versions use <code>az aks get-versions --location $LOCATION --output table</code></li> <li><code>enableRBAC</code> Whether to enable Kubernetes internal Role-Based Access Control to grant users, groups, and service accounts access to only the resources they need. Note that this parameter does not provide for Azure Active Directory RBAC.</li> <li><code>agentPoolProfiles.type</code>. This is set to virtual machine scale sets.</li> </ul> <p>You can review the various properties available for the latest ARM Template reference.</p>"},{"location":"azure/aks/#deploy-the-cluster","title":"Deploy the cluster","text":"<p>Run the following code to deploy the cluster.</p> <pre><code>az deployment group create --name $DEPLOYMENT_NAME \\\n    --resource-group $RESOURCE_GROUP_NAME --template-file \"./azuredeploy.json\" \\\n    --parameters aksClusterName=$AKS_NAME dnsPrefix=aks-eus2-aksdays-demo-01 \\\n    agentCount=1 agentVMSize=$NODE_SIZE\n</code></pre> <p>The script show how to use <code>az deployment group create</code> to deploy the ARM template and uses <code>--parameters</code> to provide the deployment parameter values, such as <code>aksClusterName</code>, <code>dnsPrefix</code>, and a small <code>agentVMSize</code>. Note: The The system node pool must use VM sku with more than 2 cores and 4GB memory.</p> <p>The script then retrieves the output and displays the <code>controlPlaneFQDN</code>.</p>"},{"location":"azure/aks/#review-what-is-deployed","title":"Review what is deployed","text":"<p>The AKS template deploys the following resources:</p> <ul> <li>A resource group that contains:<ul> <li>The nodes</li> <li>A load balancer</li> <li>Public IP</li> <li>Network Security Group (NSG)</li> <li>Managed identity</li> <li>Virtual network</li> <li>Route table</li> </ul> </li> <li>A resource group that contains:<ul> <li>The Kubernetes service</li> <li>The node pools</li> <li>Kubernet Networking</li> </ul> </li> </ul> <p>You can view the deployment in the portal. First, go to the resource group and view the resources deployed as shown in the following illustration.</p> <p></p> <p>Go to the resource group that was created for your cluster. It will have a name similar to <code>MC_aks-eus2-aksdays-demo-01_aks-eus2-aksdays-demo-01_eastus2</code>. Here you will see the various resources deployed to support Kubernetes service.</p> <p></p> <p>In this example, Azure resources are created using managed identity, which allows Azure to create additional resources such as a load balancer and managed disks.</p> <p>Let\u2019s log into the cluster.</p>"},{"location":"azure/aks/#connect-to-the-cluster","title":"Connect to the cluster","text":"<p>Next, install the Kubernetes tools that you will need, and then use AKS to get your credentials</p> <pre><code>az aks install-cli\naz aks get-credentials --resource-group $RESOURCE_GROUP_NAME --name $AKS_NAME\n</code></pre> <p>The code installs the tools you will need to interact with Kubernetes, including kubectl. And then it downloads credentials and configures the Kubernetes CLI to use them.</p> <p>To verify the connection to your cluster, use the <code>kubectl get</code> to return a list of the cluster nodes.</p> <p></p> <pre><code>kubectl get nodes\n</code></pre>"},{"location":"azure/aks/#run-an-application","title":"Run an application","text":"<p>Next, let\u2019s use a Kubernetes manifest file defines a desired state for the cluster. In this case, let\u2019s use the example in the Azure quickstart documentation.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: azure-vote-back\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: azure-vote-back\n  template:\n    metadata:\n      labels:\n        app: azure-vote-back\n    spec:\n      nodeSelector:\n        \"beta.kubernetes.io/os\": linux\n      containers:\n      - name: azure-vote-back\n        image: mcr.microsoft.com/oss/bitnami/redis:6.0.8\n        env:\n        - name: ALLOW_EMPTY_PASSWORD\n          value: \"yes\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 250m\n            memory: 256Mi\n        ports:\n        - containerPort: 6379\n          name: redis\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: azure-vote-back\nspec:\n  ports:\n  - port: 6379\n  selector:\n    app: azure-vote-back\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: azure-vote-front\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: azure-vote-front\n  template:\n    metadata:\n      labels:\n        app: azure-vote-front\n    spec:\n      nodeSelector:\n        \"beta.kubernetes.io/os\": linux\n      containers:\n      - name: azure-vote-front\n        image: mcr.microsoft.com/azuredocs/azure-vote-front:v1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 250m\n            memory: 256Mi\n        ports:\n        - containerPort: 80\n        env:\n        - name: REDIS\n          value: \"azure-vote-back\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: azure-vote-front\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n  selector:\n    app: azure-vote-front\n</code></pre> <p>Save this file locally as <code>azure-vote.yaml</code>.</p> <p>Let\u2019s do a quick code review before you apply the manifest into Kubernetes. The manifest has four parts separated by <code>--</code>. The manifest could have been deployed as four separate files. But for simplicity in the walkthrough it is a single file here. The file includes two deployments and two services.</p> <ul> <li>A deployment is a declarative way to update a Pod and ReplicaSet. The Pod has one or more containers. You describe a desired state in a Deployment, and the Kubernetes Deployment Controller changes the actual state to the desired state at a controlled rate.</li> <li>A service is an abstraction which defines a logical set of Pods and a policy by which to access them (sometimes this pattern is called a micro-service).</li> </ul> <p>The deployment of <code>azure-vote-back</code> has a single container that pulls the <code>mcr.microsoft.com/oss/bitnami/redis:6.0.8</code> image and sets its resource sizes. It also defines a label <code>app: azure-vote-back</code>.</p> <p>The service of <code>azure-vote-back</code> specification creates a new Service object named \u201cazure-vote-back\u201d, which targets TCP port 6379 on any Pod with the <code>app=azure-vote-back</code> label.</p> <p>The labels connect Services to Deployments. Deployments define the Pods.</p> <p>Next, let\u2019s apply the manifest using <code>kubectl apply</code> as in the code sample:</p> <pre><code>kubectl apply -f azure-vote.yaml\n</code></pre> <p>The Pod does not come up right away. To monitor the progress, use:</p> <pre><code>kubectl get service azure-vote-front \u2013watch\n</code></pre> <p>Kubernetes will report back the status of the service. Watch the EXTERNAL-IP column.\u00a0 Type <code>ctrl+c</code> to exit watch.</p> <p></p> <p>When it changes to an public IP address, you can use the external IP address in your browser to go to the application.</p> <p></p>"},{"location":"azure/aks/#summary","title":"Summary","text":"<p>In this walkthrough, you reviewed the design decisions and deployed a Kubernetes cluster. Then you used a manifest to deploy an image into Kubernetes.</p>"},{"location":"azure/aks/#references","title":"References","text":"<ul> <li>Quickstart: Deploy an Azure Kubernetes Service (AKS) cluster using an ARM template</li> <li>Use managed identities in Azure Kubernetes Service</li> </ul>"},{"location":"azure/architectures/AzureOpenAI-LandingZone/","title":"Azure OpenAI Landing Zone","text":"<p>Azure Landing Zones provide a solid foundation for your cloud environment. When deploying complex AI services such as Azure OpenAI, using a Landing Zone approach helps you manage your resources in a structured, consistent manner, ensuring governance, compliance, and security are properly maintained.</p>"},{"location":"azure/compliance/azure-policy/","title":"Understanding Azure Policy for regulatory compliance","text":"<p>Use Azure Policy to manage and enforce your standards for governance and compliance and to assess that compliance at scale. The idea is to set standards and to be able to demonstrated your organization is meeting your regularoty compliance goals.</p> <p>In previous blog posts, you learned about setting up Management Groups and Security Center. For management groups, you learned that policies can be applied\u00a0 across multiple subscriptions. You noticed that Security Center provides a set of policies (an an policy initiative) for your subscription.</p> <p>In this post, learn the basics of Azure Policy for you to manage resource consistency, regulatory compliance, security, and cost. And how Policies can be grouped together as initiatives, and how you can assign initiatives to specific regulatory compliance goals.</p>"},{"location":"azure/compliance/azure-policy/#what-is-an-azure-policy","title":"What is an Azure Policy?","text":"<p>You can think of an Azure Policy as a business rule that you want to apply to a resource. Policy definitions are defined in JSON. And rules that are grouped together is called a policy set.</p> <p>Once you have defined your Azure Policy, you apply it to a scope, such as such as\u00a0management groups, subscriptions,\u00a0resource groups, or individual resources.</p> <p>Importantly, when you signed up for Azure Security Center standard, you added many out of the box policies to your subscription.</p> <p>But first, let\u2019s check the policies deployed as part of Security Center.</p>"},{"location":"azure/compliance/azure-policy/#explore-policies-deployed-by-security-center","title":"Explore Policies deployed by Security Center","text":"<p>When you installed Security Center, you installed the Security Center dashboard, andyou also added a set of Azure Policies. To view the policies, open your browser and log into Azure. Next, type Policy in the search bar.</p> <p></p> <p>The Policy dashboard appears. To view the list of policy initiatives that have been added to your subscription, click ASC Default (subscription: ).</p> <p></p> <p>Once you click on the ASC Default, view the list of policies that have been added for the ASC (Azure Security Center) initiative. Click View definition, as shown in the illustration.</p> <p></p> <p>You can view each of the policies defined in the ASC initiative, which consists of a the list of Policies. Click on one that you are interested in and you can see the policy as defined in JSON. The following illustration shows the policy titled S_torage accounts should restrict network access._</p> <p></p> <p>Click Assign to see how you might assign the polilcy to a scope (subscription, resource group, or resource.\u00a0 Unter the tabs on the Assign policy page, you can also set whether the policy is to be enforced, which parameters to apply, and how to run a rediation step. The following illustration shows how to set Policy enforcement on the Basics tab.</p> <p></p> <p>In this section, you learned about policies set up by Security Center, how to view the policy, and where to find the definition of the policy in JSON. In the next section, you will learn the effects of policies.</p>"},{"location":"azure/compliance/azure-policy/#policy-as-rules","title":"Policy as rules","text":"<p>Business rules for handling non-compliant resources vary widely between organizations. Examples of how an organization wants the platform to respond to a non-complaint resource include:</p> <ul> <li>Deny the resource change</li> <li>Log the change to the resource</li> <li>Alter the resource before the change</li> <li>Alter the resource after the change</li> <li>Deploy related compliant resources</li> </ul>"},{"location":"azure/compliance/azure-policy/#kinds-of-policy-definitions","title":"Kinds of policy definitions","text":"<p>As an administrator, you control and audit using policies that written using JSON, and are created from one of two kinds of definition:</p> <ul> <li>Built-in: A set of policies that are supplied automatically by Microsoft</li> <li>Custom: Policies that you can write and store in Azure</li> </ul> <p>There are some interesting examples in the selection of built-in policies:</p> <ul> <li>Enforce tag and its value: Enforce a tag that might be used for cross-charging consumption of a single Azure subscription.</li> <li>Allowed regions. Enforce deployment of resources to particular regions, such as those in Europe, India, or United States.</li> <li>Not allowed resource types: Restrict access to certain kinds of Azure resources.</li> <li>Allowed virtual machine SKUs: Limit the series and size of virtual machines that delegated administrators can deploy.</li> </ul> <p>The Azure Policy definition structure contains elements in JSON for:</p> <ul> <li>Display name</li> <li>Description</li> <li>Mode</li> <li>Metadata</li> <li>Parameters</li> <li>Policy rule<ul> <li>Logical evaluation</li> <li>Effect</li> </ul> </li> </ul>"},{"location":"azure/compliance/azure-policy/#apply-a-policy-effect","title":"Apply a Policy effect","text":"<p>When you apply the policy, you apply the Policy effect. The effect determines what happens when the policy rule matches the resource. So, when you review an existing resource or create a resource or view, the policy is reviewed in the following order:</p> <ol> <li>Disabled. Should the policy even be evaluated (as shown in the previous section)?</li> <li>Append or\u00a0Modify. Should the policy add something that you specify in or modify a resource? For example, specifying allowed IPs for a storage resource.</li> <li>Deny. Should the policy block the request?\u00a0 For example, a rule could be set to deny that storage resource that are not georedundant.</li> <li>Audit. Create a warning event in the activity log when evaluating a non-compliant resource</li> </ol> <p>When you successfully deploy a resource, you can specify:</p> <ul> <li>AuditIfNotExists enables auditing of resources\u00a0related to the resource. For example, it would evaluate a Virtual Machines to determine if the Antimalware extension exists then audits when missing.</li> <li>DeployIfNotExists evaluate to determine if additional compliance logging or action is required. For example, you might evaluates SQL Server databases to determine if transparentDataEncryption is enabled. If not, then a deployment to enable is executed.</li> </ul> <p>For more information, see Understand Azure Policy effects.</p>"},{"location":"azure/compliance/azure-policy/#sample-built-in-policy","title":"Sample built-in policy","text":"<p>The following is an example of a policy provided by Microsoft, \u201cSecure transfer to storage accounts should be enabled\u201d.</p> <pre><code>{\n  \"properties\": {\n    \"displayName\": \"Secure transfer to storage accounts should be enabled\",\n    \"policyType\": \"BuiltIn\",\n    \"mode\": \"Indexed\",\n    \"description\": \"Audit requirement of Secure transfer in your storage account. Secure transfer is an option that forces your storage account to accept requests only from secure connections (HTTPS). Use of HTTPS ensures authentication between the server and the service and protects data in transit from network layer attacks such as man-in-the-middle, eavesdropping, and session-hijacking\",\n    \"metadata\": {\n      \"version\": \"2.0.0\",\n      \"category\": \"Storage\"\n    },\n    \"parameters\": {\n      \"effect\": {\n        \"type\": \"String\",\n        \"metadata\": {\n          \"displayName\": \"Effect\",\n          \"description\": \"The effect determines what happens when the policy rule is evaluated to match\"\n        },\n        \"allowedValues\": [\n          \"Audit\",\n          \"Deny\",\n          \"Disabled\"\n        ],\n        \"defaultValue\": \"Audit\"\n      }\n    },\n    \"policyRule\": {\n      \"if\": {\n        \"allOf\": [\n          {\n            \"field\": \"type\",\n            \"equals\": \"Microsoft.Storage/storageAccounts\"\n          },\n          {\n            \"anyOf\": [\n              {\n                \"allOf\": [\n                  {\n                    \"value\": \"[requestContext().apiVersion]\",\n                    \"less\": \"2019-04-01\"\n                  },\n                  {\n                    \"field\": \"Microsoft.Storage/storageAccounts/supportsHttpsTrafficOnly\",\n                    \"exists\": \"false\"\n                  }\n                ]\n              },\n              {\n                \"field\": \"Microsoft.Storage/storageAccounts/supportsHttpsTrafficOnly\",\n                \"equals\": \"false\"\n              }\n            ]\n          }\n        ]\n      },\n      \"then\": {\n        \"effect\": \"[parameters('effect')]\"\n      }\n    }\n  },\n  \"id\": \"/providers/Microsoft.Authorization/policyDefinitions/404c3081-a854-4457-ae30-26a93ef643f9\",\n  \"type\": \"Microsoft.Authorization/policyDefinitions\",\n  \"name\": \"404c3081-a854-4457-ae30-26a93ef643f9\"\n}\n</code></pre> <p>The <code>policyRule</code> element says that if the field in the resource <code>Microsoft.Storage/storageAccounts/supportsHttpsTrafficOnly</code> is <code>false</code>, then apply the effect, which is provided as a parameter. The effect defaults to <code>Audit</code>.</p> <p>The policy <code>id</code> is <code>/providers/Microsoft.Authorization/policyDefinitions/404c3081-a854-4457-ae30-26a93ef643f9</code>, which you will use in the next section.</p> <p>When you apply the policy, you provide it a scope (management group, subscription, or resource) and the effect.</p> <p>For more information about this particular policy, see Require secure transfer to ensure secure connections.</p>"},{"location":"azure/compliance/azure-policy/#policy-initiatives","title":"Policy initiatives","text":"<p>The policy definitions can be grouped into an initiative. Security Center is an example of several policies tied togeather using an initiative.</p> <p>Using JSON, you can group your own policies together using the Azure Policy initiative definition structure, which contains elements for:</p> <ul> <li>display name</li> <li>description</li> <li>metadata</li> <li>parameters</li> <li>policy definitions</li> <li>policy groups (this property is part of Regulatory Compliance, explained in the next section.)</li> </ul> <p>You can assign several of your own policies togeather as an initiative. In the following example, selected portions of the NIST SP 800-53 R4 initiative on GitHub includes the storage account rule used in the previous section.</p> <pre><code>{\n  \"properties\": {\n      \"displayName\": \"NIST SP 800-53 R4\",\n      \"policyType\": \"BuiltIn\",\n      \"description\": \"This initiative includes audit and virtual machine extension deployment policies that address a subset of NIST SP 800-53 R4 controls. Additional policies will be added in upcoming releases. For more information, visit https://aka.ms/nist80053-blueprint.\",\n      \"metadata\": {\n        \"version\": \"2.0.1\",\n        \"category\": \"Regulatory Compliance\"\n      },  \n      \"policyDefinitions\": [\n        {\n          \"policyDefinitionReferenceId\": \"AuditSecureTransferToStorageAccounts\",\n          \"policyDefinitionId\": \"/providers/Microsoft.Authorization/policyDefinitions/404c3081-a854-4457-ae30-26a93ef643f9\",\n          \"parameters\": {},\n          \"groupNames\": [\n            \"NIST_SP_800-53_R4_SC-8(1)\"\n          ]\n        }\n      ],\n      \"comment\": \"policyDefinitionGroups go here\"\n  },\n  \"id\": \"/providers/Microsoft.Authorization/policySetDefinitions/cf25b9c1-bd23-4eb6-bd2c-f4f3ac644a5f\",\n  \"name\": \"cf25b9c1-bd23-4eb6-bd2c-f4f3ac644a5f\"\n}\n</code></pre> <p>Note that <code>policyDefinitionId</code> refers to the resource id <code>/providers/Microsoft.Authorization/policyDefinitions/404c3081-a854-4457-ae30-26a93ef643f9</code> of the storage account policy. The initiative has <code>displayName</code> of <code>NIST_SP_800-53_R4_SC-8(1)</code>.</p>"},{"location":"azure/compliance/azure-policy/#regulatory-compliance","title":"Regulatory Compliance","text":"<p>When you create initiatives, you are creating sets of policies that can be used to valiadate your regulatory compliance. But how can you track the policy initiative back to a specific regulation?</p> <p>Use the policy groups element in the policy initiative JSON to define your own regulatory compliance.\u00a0In the following example, the name refers to a specific rule in NIST, but is also associated with other policies.</p> <pre><code>   \"policyDefinitionGroups\": [\n      {\n        \"name\": \"NIST_SP_800-53_R4_SC-8(1)\",\n        \"additionalMetadataId\": \"/providers/Microsoft.PolicyInsights/policyMetadata/NIST_SP_800-53_R4_SC-8(1)\"\n      }\n    ]\n</code></pre> <p>The schema makes is possible for you to define many policies to a particular regulatory compliance, but also for the regulator compliance to apply to multiple policies.</p> <p>If using a built-in Regulatory Compliance initiative definition as a reference, it\u2019s recommended to monitor the source of the Regulatory Compliance definitions in the\u00a0Azure Policy GitHub repo.</p> <p>You can even link your custom Regulatory Compliance initiative to your Azure Security Center dashboard, see Azure Security Center \u2013 Using custom security policies.</p> <p>For more information, see Regulatory Compliance in Azure Policy.</p>"},{"location":"azure/compliance/azure-policy/#policy-as-code","title":"Policy as Code","text":"<p>In the beginning, you will be using the portal to implement your policies. But as time goes on, you will want to lock down the workflow. You will want to demonstrate how and when your policies and how your code demonstrates regulatory compliance.</p> <p>You will want to put your code into a DevOps workflow. When you make a change, check in the change into a code repository, then deploy it into your environments.</p> <p>As provided in the guidance, it is best practice:</p> <p>By making Azure Policy validation an early component of the build and deployment process the application and operations teams discover if their changes are non-compliant, long before it\u2019s too late and they\u2019re attempting to deploy in production.</p> <p>For more information, see Design Policy as Code workflows.</p>"},{"location":"azure/compliance/azure-policy/#policy-as-blueprints","title":"Policy as Blueprints","text":"<p>Azure Policy and policy intiatives can be automated for deployment. For example, when you create a new subscription, you will want a set of policies in place when the subscription is created.</p> <p>Blueprints are a declarative way to orchestrate the deployment of various resource templates and other artifacts such as:</p> <ul> <li>Role Assignments</li> <li>Policy Assignments</li> <li>Azure Resource Manager templates</li> <li>Resource Groups</li> </ul> <p>Blueprints will give you a way to stand up a subscription that meets the compliance for policies and standard resource deployments.</p> <p>The work you do in creating policies can be leveraged directly into Blueprints.</p>"},{"location":"azure/compliance/azure-policy/#next-steps","title":"Next steps","text":"<p>Learn more about the built in policy samples.</p> <p>Set up an organizational policy and implementation. See Policy enforcement decision guide in the Cloud Adoption Framework.</p> <p>And in next few posts, you will learn how to select and deploy Azure Blueprints. Use Azure Blueprints to deploy groups of policies (and resource templates) across your subscriptions, resource groups, or resources. The power of Azure Blueprints comes in at providing sets of policies that will demonstrate your regulatory compliance, such as</p> <ul> <li>Azure Security Benchmark</li> <li>CIS Microsoft Azure Foundations Benchmark v1.1.0</li> <li>NIST SP 800-53 R4</li> <li>NIST SP 800-171 R2</li> </ul> <p>And then, you will be ready to to deploy resources.</p>"},{"location":"azure/compliance/azure-policy/#references","title":"References","text":"<ul> <li>Azure Policy documentation</li> <li>Azure Policy Samples</li> <li>Aiden Finn\u2019s\u00a0Azure Policy for Governance Enforcement</li> </ul>"},{"location":"azure/compliance/governance/","title":"Governance","text":""},{"location":"azure/compliance/governance/#requirements-plan-for-your-enterprise-azure-subscription-for-production","title":"Requirements, plan for your enterprise Azure Subscription for production","text":"<p>Microsoft\u2019s Cloud Adoption Framework</p> <p>You can get started in Azure. But soon it becomes time to build your subscriptions for your enterprise. For example, giving unrestricted access to developers can make your devs very agile, but it can also lead to unintended cost consequences. In addition, you will want to have requirements to demonstrate compliance for security, monitoring, and resource access control.</p> <p>In this article we help organize some thoughts around the strategy and plan for building out your cloud, including a plan that you can put into Azure DevOps.</p> <p>The Cloud Adoption Framework provides guidance for in depth analysis and preparation for your cloud.\u00a0</p> <p>Developing your requirements you will want to recognize that your cloud deployments are ongoing, built around what your business goals. Your requirements should include managing, monitoring, and auditing the use of Azure resources.</p>"},{"location":"azure/compliance/governance/#governance-requirements","title":"Governance requirements","text":"<p>There are many requirements to meet in setting up your Azure subscriptions:</p> <ul> <li>Resource management. Define which resources and who may access those resources.</li> <li>Cost management. Track cloud usage and expenditures for your Azure resources and assign to cost centers or development teams.</li> <li>Identity management. Managing your identity lifecycles, including role-based-access-control across groups of users, admins, and managing the lifecycle for accessing resources.</li> <li>Security. Creating a secure infrastructure, but also being able to show auditor and others the steps you have taken for security.</li> <li>Deploy applications. You may migrate existing applications from on premises or other cloud providers. Or you may</li> <li>Monitor. Collect and analyze data to audit the performance, health, and availability of your resources.</li> <li>Protect. Keep your applications and data available, even with outages that are beyond your control.</li> </ul>"},{"location":"azure/compliance/governance/#azure-solutions","title":"Azure solutions","text":"<p>The Azure documentation describes management as:</p> <p>Management refers to the tasks and processes required to maintain your business applications and the resources that support them. Azure has many services and tools that work together to provide complete management.</p> <p>Overview of Management services in Azure</p> <p>Azure provides many resources to manage these requirements.</p> <ul> <li>Resource management. Azure organizes resources in resource groups and resources. Resource groups can be defined in templates, allow access using role-based-access-control (RBAC), and</li> <li>Management groups provide a way to manage access, policies, and compliance across multiple subscriptions.</li> <li>Azure Policy. Provides the guardrails for your developers, administrators, and users.</li> <li>Azure Monitor and Log Analytics. Azure resources for collecting and analyzing data pertaining to the performance, health, and availability of your resources.</li> <li>Azure cost management helps you monitor and control Azure spending and optimize resource use.</li> <li>Azure Security Center strengthens the security posture of your data centers, and provides advanced threat protection across your hybrid workloads in the cloud.</li> <li>And more.</li> </ul>"},{"location":"azure/compliance/governance/#define-your-requirements","title":"Define your requirements","text":"<p>Before deploying your applications at scale, you will want to define your requirements. Those requirements are not set in stone, but will be a touchpoint for your decision making. Here are some topics to be sure to include in your requirements.</p> <ul> <li>Cost. Cost is the first question I get asked, and it is often last one I can answer. Interestingly, the other requirements will determine cost. And the answer will always be an estimate. Cost is based on consumption. Many decision makers want a single cost for a period of time. Frankly, they are used to that cost structure from their past hardware purchases. But the cloud varies. And it can vary a lot based on your other requirements.</li> <li>Availability. How much do you want your application to be available? Are you sure? 99.995% may be attainable, but it will be expensive. It is one thing to backup and restore your cloud application with some downtime. It is another to provide high availability across multiple regions. How long can your enterprise go without access to the cloud? And who and how will you test availability?</li> <li>Security and compliance. Which are the compliance standards you want to audit and enforce? What kind of data are you keeping? What are the trade-offs that you are willing to take between availability and confidentiality? Which data will be exposed if a particular resource is breached?</li> <li>Management. Who is managing the cloud operations? Who is auditing the cloud security? Who is responsible for mitigating anything that goes wrong?</li> <li>Identity management. How are your identities and permissions handled? Who has access to the data? Identity is the new edge for accessing data.</li> <li>Expertise. What is the expertise of your teams (development, operations, security)? And how are they keeping up to date on the changes?</li> </ul> <p>Each of these topics has its own lifecycle, meaning there is a starting point, changes made, and retirement. How is your organization managing the lifecycles?</p> <p>Some places to look for deeper understanding:</p> <ul> <li>Cloud Adoption Framework. Provides a roadmap for your cloud setup, migration, development, governance, management. This is proven guidance for your enterprise that helps you create and implement the business and technology strategies.\u00a0</li> <li>ISO 27001 provides guidance in the form of controls that are designed for any size organization. Azure provides a blueprint sample that you can use to look for non-compliance.</li> <li>The National Institute of Standards and Technology (NIST), part of the US Department of Commerce,\u00a0provides Special Publication 800-144,\u00a0Guidelines on Security and Privacy in Public Cloud Computing. This 80-page report is comprehensive in both defining cloud computing and providing guidelines for using it in a secure and private manner.</li> <li>A more detailed set of controls is also available from NIST. Azure provides detailed guidance on your responsibilities in implementing SP 800-53 Rev. 4, Security and Privacy Controls for Federal Information Systems and Organizations. It provides a catalog of security and privacy controls for federal information systems and organizations and a process for selecting controls to protect organizational operations.</li> </ul> <p>In future postings in this blog, you learn how to use the Cloud Adoption Framework, the NIST SP 800-53 and ISO 27001 Blueprints to implement the controls in your Azure deployment. And you learn how to adapt each for your cloud.</p> <p>Important to remember \u2014 the cloud is a journey, not a destination. And your job will be to deploy a new data center. This time, it is in the cloud.</p>"},{"location":"azure/compliance/governance/#define-workloads","title":"Define workloads","text":"<p>You may want to run an digital estate assessment to identify workloads that may be a good fit for the cloud. Or you may have already identified cloud-first innovation and development opportunities.</p> <p>Either way, having a list of applications and a proposed infrastructure is helpful to proceed with your migration and design.</p>"},{"location":"azure/compliance/governance/#design","title":"Design","text":"<p>And I mean \u201cdesign\u201d as an imperative verb. Initially it is a general design to get an estimate of the ongoing feasibility.</p> <p>Cloud architecture is about tradeoffs and implementing best practices based on how you work and what you want to accomplish. The cloud comes in lots of sizes, lots of patterns.</p> <p></p>"},{"location":"azure/compliance/governance/#cloud-design-patterns","title":"Cloud design patterns","text":"<p>Azure documentation provides many patterns for your cloud design. But you will decide the tradeoffs between availability, data management, messaging, management and monitoring, performance and scale, resiliency, and security.</p> <p>For example, there are huge tradeoffs between virtual machines and VM sizing, Application Services and Azure Functions, and Kubernetes (AKS). Storage tradeoffs for blobs verses files verses disks attached for VMs, backup storage costs.</p>"},{"location":"azure/compliance/governance/#and-then-you-can-estimate-a-cost","title":"And then \u2026 you can estimate a cost","text":"<p>And once those questions are answered, and an architecture designed, then you can estimate a cost.</p> <p>And that cost varies by how you purchase Azure and your commitment to the cloud. For example, a three-year reserved instance or spot instances may save money.</p> <p>Once you have the green light, you can create a cloud adoption plan.</p>"},{"location":"azure/compliance/governance/#create-a-cloud-adoption-plan","title":"Create a Cloud Adoption Plan","text":"<p>You can create a cloud adoption plan in devops. It is a set of steps that you can deploy into Azure DevOps.\u00a0</p>"},{"location":"azure/compliance/governance/#deploy-the-plan-into-azure-devops","title":"Deploy the plan into Azure DevOps","text":"<p>To deploy the cloud adoption plan, open the Azure DevOps demo generator. This tool will deploy the template to your Azure DevOps tenant. You can then edit the specifics of your plan in Excel.</p>"},{"location":"azure/compliance/governance/#align-the-plan","title":"Align the plan","text":"<p>Use the Cloud Adoption Framework strategy and planning template to organize the your decisions and data points.\u00a0</p>"},{"location":"azure/compliance/governance/#next-steps","title":"Next steps","text":"<p>In our next post, get your setting up a your subscription.\u00a0</p>"},{"location":"azure/compliance/governance/#references","title":"References","text":"<ul> <li>Azure Architecture Center</li> <li>Governance in the Cloud Adoption Framework</li> <li>Overview of Management services in Azure</li> <li>Monitoring Azure resources</li> <li>Azure Policy</li> <li>Azure Security Center</li> <li>Azure Migrate</li> <li>Azure Identity Management and access control security best practices</li> <li>Defining Availability Requirements for Cloud Computing</li> <li>Training module for Microsoft Cloud Adoption Framework for Azure</li> </ul>"},{"location":"azure/compliance/log-analytics/","title":"Setting up Log Analytics workspace for production in enterprise","text":"<p> Operations and security are central in any cloud deployment. It should be top of mind in each of your cloud deployments.</p> <p>Enabling your operations team to find and fix errors, to build practices around scaling your data are essential to having a successful Azure data center.</p> <p>Log Analytics provides a unified way to show what is happening across your Azure data center.</p> <p>In this article learn how to set up Log Analytics to receive data from multiple Azure subscriptions, on premises virtual machines or other clouds. And learn to configure your Log Analytics workspace, set up role-based-access-control, and how to incorporate Log Analytics best practices. In addition, you will also learn how to get started with some important queries.</p> <p>Before you deploy resource groups and resources, you want to be able to measure and anlayze what is happening in our Azure data center. So before you start your deployments, it is a best practice to do these things:</p> <ol> <li>Deploy your Azure Log Analytics workspace</li> <li>Configure your Log Analytics workspace</li> <li>Deploy Security Center and have it send its logs to Azure Log Analytics. Learn about Security Center in the next article.</li> </ol> <p>And then you can deploy your workloads.</p> <p>In the next article, learn how to associate Azure Log Analytics with Security Center.</p>"},{"location":"azure/compliance/log-analytics/#prerequisites","title":"Prerequisites","text":"<p>You should already have:</p> <ul> <li>Azure CLI and PowerShell installed.</li> <li>Completed setting up your administrators as shown in our previous post.</li> <li>Some basic understanding of how to deploy resources using an ARM template.</li> </ul>"},{"location":"azure/compliance/log-analytics/#definitions","title":"Definitions","text":"<p>Let\u2019s start with some definitions about the changing terminology around Azure Monitor and Log Analytics.</p> <ul> <li>Azure Monitor log data is stored in a Log Analytics workspace.</li> <li>The term Log Analytics is changing to be Azure Monitor logs.</li> <li>Log analytics primarily applies to the page in the Azure portal used to write and run queries and analyze log data.</li> <li>Log Analytics and Application Insights have been consolidated into Azure Monitor.</li> <li>Log Analytics workspaces is where you create new workspaces and configure data sources.</li> <li>Management solutions have been renamed to monitoring solutions</li> </ul>"},{"location":"azure/compliance/log-analytics/#about-log-analytics-workspaces","title":"About Log Analytics workspaces","text":"<p>Azure Log Analytics is the primary tool in the Microsoft Azure portal for writing log queries and interactively analyzing their results. For those who have been around awhile, it was knows as OMS.</p> <p>Azure Monitor and many resources in Azure stores log data in a Log Analytics workspace. The workspace is a central repository for that you can use to collect information from monitors and many other sources.</p> <p>The following illustration shows how you collect data from multiple data sources and then use Log Analytics for alerts, analysis, and reports.</p> <p></p> <p>In this article you set up a shared Log Analytics workspace that is used across multiple applications, multiple subscriptions. You can think of it as a central workspace to monitor your the compute, network, and storage for your production environment.</p> <p>See Designing your Azure Monitor Logs deployment for more information about how to design workspace, where your data is collected and aggregated.</p>"},{"location":"azure/compliance/log-analytics/#set-up-log-analytics-workspace","title":"Set up Log Analytics workspace","text":"<p>Log Analytics can collect data from across multiple Azure Monitors, application, subscriptions, and even on premises or operations information across clouds.</p> <p>Select a pricing model based on the amount of data brought in, called per GB. This pricing model works best for containers and microservices where the definition of a node is less clear. \u201cPer GB\u201d data ingestion is the new basis for pricing across application, infrastructure, and networking monitoring.</p> <p>In order to strictly control the access to the log analytics data, you may want to create a subscription for your operations team that contains Log Analytics and perhaps other sensitive data, such as Key Vault. When you set up your Log Analytics workspace, you can configure the other data sources to send the data to it \u2014 regardless of region to aggregate across subscriptions. The name for the Log Analytics workspace is unique across all of Azure, so it can be used to accept data from all of your resources.</p> <p>Typically in an enterprise, you will have Azure Monitor data and data from Security Center and other resources providing data to a centralized Log Analytics workspace, as shown in the following illustration.</p> <p></p> <p>Architects will often set up a Log Analytics workspace for developers to monitor applications during development and then have a second one for production.</p> <p>You may want to consider putting your production Log Analytics workspace in its own subscription so you can strictly control who has access, who can view data, and who receives alarms. Adding a minimal number of people who have access to the subscription can help guarantee the data in Log Analytics.</p> <p>You can create the Log Analytics workspace using the portal, Azure CLI, or PowerShell. In this article, you will set up the Log Analytics workspace using PowerShell.</p>"},{"location":"azure/compliance/log-analytics/#set-up-log-analytics-workspace-using-powershell-and-an-arm-template","title":"Set up Log Analytics workspace using PowerShell and an ARM template","text":"<p>Log into Azure using PowerShell. This can be on your local computer or in Azure Cloud Shell.</p> <p>Then create a resource group to hold the Log Analytics workspace and its long term data. The following example shows how to create such a resource group, using your own SUBSCRIPTION_ID and the other parameters for your tags.</p> <pre><code>$ORGANIZATION_NAME = \"Az Days\"\n$LOCATION = \"Central US\"\n$LOCATION_ABBR= \"cenus\"\n$SUBSCRIPTION_ID = \"7005478c-99cb-4b5d-a56c-d60abc23d6af\"\n$ENVIRONMENT = \"Prod\"\n$COSTCENTER = \"Corporate\"\n$OWNER = \"bruce@azdays.com\"\n\n$createdData = Get-Date -Format \"yyyy-MM-dd\"\n$tags = @{\"Cost Center\"=$COSTCENTER; \"Location\"=$LOCATION; \"Environment\"=$ENVIRONMENT; \"Project\"=$ORGANIZATION_NAME; \"Owner\"=$OWNER; \"Created Date\"=$createdData; \"Tier\"=\"Management\" }\n\n$OrganizationName = $ORGANIZATION_NAME -replace '\\s', ''\n$OrganizationName = $OrganizationName.ToLower()\n\nif ($SubscriptionId -eq $null) {\n    $SubscriptionId = (Get-AzContext).Subscription.SubscriptionId\n}\nSet-AzContext -Name ($OrganizationName + \"Context\") -SubscriptionId $subscriptionID -Force\n\n#############\n# Create shared resource group for management organization\n#############\n\n$resourceGroupName = \"rg-$LOCATION_ABBR-$OrganizationName-$ENVIRONMENT-management\"\n\nNew-AzResourceGroup `\n      -Name $resourceGroupName `\n      -Location $Location_lc `\n      -Tag  $tags\n\nGet-AzResourceGroup -Name $resourceGroupName\nWrite-Host \"Created or updated: \" $ResourceGroupName\n</code></pre> <p>Next, you will use an ARM template to create a Log Analytics workspace, create a storage account to hold the Monitor log data, lock the storage account and the Log Analytics workspace resources from deletion. (You will learn more about ARM templates in later posts.)</p> <pre><code>{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"organization\": {\n      \"type\": \"string\",\n      \"metadata\": {\n        \"description\": \"Organization name. For example: AzDays\"\n      }\n    },\n    \"service-tier\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"PerNode\",\n      \"allowedValues\": [\n        \"Free\",\n        \"Standalone\",\n        \"PerNode\",\n        \"PerGB2018\"\n      ],\n      \"metadata\": {\n        \"description\": \"Service Tier: Free, Standalone, or PerNode\"\n      }\n    },\n    \"data-retention\": {\n      \"type\": \"int\",\n      \"defaultValue\": 365,\n      \"minValue\": 0,\n      \"maxValue\": 365,\n      \"metadata\": {\n        \"description\": \"Number of days data will be retained for.\"\n      }\n    },\n    \"location\": {\n      \"type\": \"string\",\n      \"defaultValue\": \"West US 2\",\n      \"allowedValues\": [\n        \"australiacentral\",\n        \"australiaeast\",\n        \"australiasoutheast\",\n        \"brazilsouth\",\n        \"canadacentral\",\n        \"centralindia\",\n        \"centralus\",\n        \"eastasia\",\n        \"eastus\",\n        \"eastus2\",\n        \"francecentral\",\n        \"japaneast\",\n        \"koreacentral\",\n        \"northcentralus\",\n        \"northeurope\",\n        \"southafricanorth\",\n        \"southcentralus\",\n        \"southeastasia\",\n        \"uksouth\",\n        \"ukwest\",\n        \"westcentralus\",\n        \"westeurope\",\n        \"westus\",\n        \"westus2\"\n      ],\n      \"metadata\": {\n        \"description\": \"Region used when establishing the workspace.\"\n      }\n    },\n    \"tags\": {\n      \"type\": \"object\",\n      \"defaultValue\": {\n        \"Cost Center\": \"[resourceGroup().tags['Cost Center']]\",\n        \"Location\": \"[resourceGroup().tags['Location']]\",\n        \"Environment\": \"[resourceGroup().tags['Environment']]\",\n        \"Owner\": \"[resourceGroup().tags['Owner']]\",\n        \"Organization\": \"[parameters('organization')]\",\n        \"Created Date\": \"[resourceGroup().tags['Created Date']]\",\n        \"Tier\": \"[resourceGroup().tags['Tier']]\"\n      }\n    },\n  },\n  \"variables\": {\n    \"deployment-prefix\": \"[concat('workload-', parameters('organization'))]\",\n    \"uniqueString\": \"[uniqueString(subscription().id, concat(variables('deployment-prefix'), '-log'))]\",\n    \"diagnostic-storageAccount-prefix\": \"[concat(, 'diag', replace(variables('deployment-prefix'), '-', ''))]\",\n    \"diagnostic-storageAccount-name\": \"[toLower(substring(replace(concat(variables('diagnostic-storageAccount-prefix'), variables('uniqueString'), variables('uniqueString')), '-', ''), 0, 23) )]\",\n    \"oms-workspace-name\": \"[concat('log-', variables('deployment-prefix'))]\"\n  },\n  \"resources\": [\n    {\n      \"comments\": \"----DIAGNOSTICS STORAGE ACCOUNT-----\",\n      \"type\": \"Microsoft.Storage/storageAccounts\",\n      \"name\": \"[variables('diagnostic-storageAccount-name')]\",\n      \"apiVersion\": \"2019-06-01\",\n      \"location\": \"[resourceGroup().location]\",\n      \"kind\": \"StorageV2\",\n      \"sku\": {\n        \"name\": \"Standard_LRS\",\n        \"tier\": \"Standard\"\n      },\n      \"tags\": \"[parameters('tags')]\",\n      \"properties\": {\n        \"supportsHttpsTrafficOnly\": true,\n        \"networkAcls\": {\n          \"bypass\": \"AzureServices\",\n          \"defaultAction\": \"Deny\"\n        }\n      }\n    },\n    {\n      \"type\": \"Microsoft.Storage/storageAccounts/providers/locks\",\n      \"apiVersion\": \"2016-09-01\",\n      \"name\": \"[concat(variables('diagnostic-storageAccount-name'), '/Microsoft.Authorization/storageDoNotDelete')]\",\n      \"dependsOn\": [\n        \"[concat('Microsoft.Storage/storageAccounts/', variables('diagnostic-storageAccount-name'))]\"\n      ],\n      \"comments\": \"Resource lock on diagnostic storage account\",\n      \"properties\": {\n        \"level\": \"CannotDelete\"\n      }\n    },\n    {\n      \"apiVersion\": \"2015-11-01-preview\",\n      \"location\": \"[parameters('location')]\",\n      \"name\": \"[variables('oms-workspace-name')]\",\n      \"properties\": {\n        \"sku\": {\n          \"Name\": \"[parameters('service-tier')]\"\n        },\n        \"retention\": \"[parameters('data-retention')]\"\n      },\n      \"tags\": {},\n      \"type\": \"Microsoft.OperationalInsights/workspaces\"\n    },\n    {\n      \"type\": \"Microsoft.OperationalInsights/workspaces/providers/locks\",\n      \"apiVersion\": \"2016-09-01\",\n      \"name\": \"[concat(variables('oms-workspace-name'), '/Microsoft.Authorization/logAnalyticsDoNotDelete')]\",\n      \"dependsOn\": [\n        \"[variables('oms-workspace-name')]\"\n      ],\n      \"comments\": \"Resource lock on Log Analytics\",\n      \"properties\": {\n        \"level\": \"CannotDelete\"\n      }\n    }\n  ],\n  \"outputs\": {\n    \"resourceID\": {\n      \"type\": \"string\",\n      \"value\": \"[resourceId('Microsoft.OperationalInsights/workspaces/', variables('oms-workspace-name'))]\"\n    },\n    \"workspaceName\":{\n        \"type\": \"string\",\n        \"value\": \"[variables('omsWorkspaceName')]\"\n    },\n    \"workspaceId\":{\n        \"type\": \"string\",\n        \"value\": \"[reference(resourceId('Microsoft.OperationalInsights/workspaces/', variables('omsWorkspaceName')), '2017-04-26-preview').customerId]\"\n    },\n    \"workspaceKey\":{\n        \"type\": \"string\",\n        \"value\": \"[listKeys(resourceId('Microsoft.OperationalInsights/workspaces/', variables('omsWorkspaceName')), '2017-04-26-preview').primarySharedKey]\"\n    }\n  }\n}\n</code></pre> <p>To deploy, you can either set up a parameters file, or deploy with a script similar to the following:</p> <pre><code>#Requires -Version 7.0\n#Requires -Modules PowerShellGet, Az, Az.Storage, , Az.Resources\n&lt;#\n.SYNOPSIS\n  Add-LogAnalytics adds resource group, log analytics workspace as a shared resource, using loganalytics.deploy.json file in the same directory.\n.DESCRIPTION\n  Creates a shared resource group, a storage account attached and a new log analytics workspace resource.\n.PARAMETER SubscriptionID \n    Mandatory. The Azure Subscription ID, such as \"9f241d6e-16e2-4b2b-a485-cc546f04799b\"\n.PARAMETER OrganizationName\n    Mandatory. Name of organization. (used to create the resource group and the common resources.\n.PARAMETER CostCenter\n    Optional. Cost center for this resource. Used for tags. Default is \"Administration\"\n.PARAMETER Environment\n    One for each region. So the default is 'mgmt' for the Environment for tags \n.PARAMETER Location\n    You will need to specify the location for regions other than West US 2\n.PARAMETER LocationAbbr\n    You will need to specify the location for regions other than West US 2\n.PARAMETER Owner\n    The owner is tagged in the resource group and resource\n.RETURN\n    The name of the resource group created\n.NOTES\n  Version:        1.0.1\n  Author:         Bruce Kyle\n  Creation Date:  6/5/2020\n  Purpose/Change: Update example\n  Requires:\n  - Connection to Azure\n  Copyright 2020 Stretegic Datatech LLC\n  License: MIT https://opensource.org/licenses/MIT\n\n.EXAMPLE\n    $ORGANIZATION_NAME = \"AzDays\"\n    $LOCATION = \"Central US\"\n    .\\Add-LogAnalytics.ps1 -SubscriptionID 9f241d6e-16e2-4b2b-a485-cc546f04799b `\n        -OrganizationName $ORGANIZATION_NAME -Location $LOCATION -LocationAbbr 'cus'\n#&gt;\n[CmdletBinding()]\n#--------[Params]---------------\nParam(\n    [Parameter(Mandatory=$false)] [string] $SubscriptionID,\n    [Parameter(Mandatory)] [string] $OrganizationName,\n    [Parameter(Mandatory=$false)] [string] $CostCenter = \"Administration\",\n    [Parameter(Mandatory=$false)] [string] $Environment='mgmt',\n    [Parameter(Mandatory=$false)] [string] $Location=\"West US 2\",\n    [Parameter(Mandatory=$false)] [string] $LocationAbbr='wus2',\n    [Parameter(Mandatory=$false)] [string] $Owner = $env:UserName\n)\n\nSet-StrictMode -Version Latest\n$ErrorActionPreference = \"Stop\"\n\ntry\n{\n\n  Set-AzContext -Name ($OrganizationName + \"Context\") -SubscriptionId $subscriptionID -Force\n\n  $Location_lc = $LOCATION -replace '\\s', ''\n  $Location_lc = $Location_lc.ToLower()\n\n    #############\n    # Deploy log analytics with storage account\n    #############\n\n    $deploymentName = $ResourceGroupName.substring(3) + \"-management-deployment\"\n    Write-Host \"Deployment name: \" $deploymentName\n\n    # accepting the defaults for the other items\n    $paramObject = @{\n         'organization' = $OrganizationName\n    }\n    $parameters = @{\n         'Name'                  = $deploymentName\n         'ResourceGroupName'     = $ResourceGroupName\n         'TemplateFile'          = '.\\loganalytics-deploy.json'\n         'TemplateParameterObject'    = $paramObject\n         'Verbose'               = $true\n    }\n\n    New-AzResourceGroupDeployment @parameters\n\n    $loganalyticsResourceID = @(Get-AzResourceGroupDeployment `\n        -ResourceGroupName  $ResourceGroupName `\n        -Name  $ResourceGroupName).Outputs.resourceID.value\n}\ncatch\n{\n    $loganalyticsResourceID = $null;\n     echo \"Completed Log analytics failed\"\n}\nfinally\n{\n    echo \"Completed Log analytics creation: $loganalyticsResourceID\"\n}\n\nreturn $loganalyticsResourceID\n</code></pre> <p>Note that Log Analytics workspace is not available in every region, and Log Analytics does not not need to be colocated in any particular region with your resources. It may be a good idea to have your analytics in a separate region to help query and mitigate during outages.</p> <p>Run the PowerShell script in this section to deploy the template. The template returns the workspace name that you will need in the next section.</p>"},{"location":"azure/compliance/log-analytics/#configure-workspace","title":"Configure workspace","text":"<p>Next, to configure workspace, you connect log analytics to the resources you want to track.</p> <p>In the portal, navigate to the Overview page of your newly created Log Analytics workspace as shown in the following illustration.</p> <p></p> <p>Azure provides out of the box Activity Logs. To add Activity Logs to Log Analytics, click the Azure Activity Logs link and select the subscriptions you want to analyze.</p>"},{"location":"azure/compliance/log-analytics/#platform-logs-and-platform-metrics","title":"Platform logs and Platform metrics","text":"<p>Platform logs provide detailed diagnostic and auditing information for Azure resources and the Azure platform they depend on. They are automatically generated although you need to configure some of the platform logs to be forwarded to Log Analytics. Platform metrics are collected by default and typically stored in the Azure Monitor metrics database.</p> <p>Each Azure resource requires its own diagnostic setting. The available categories will vary for different resource types.</p> <p>You can send these data to Log Analytics, Event Hubs, or Storage Account or any combination.</p> <p>This means that you will need to configure your resources to send their performance data and logs to Log Analytics as you build them.</p> <p>You can set logs at different layers of Azure:</p> <ul> <li>Resource logs. Operations within an Azure resource (the data plane), for example getting a secret from a Key Vault or making a request to a database. Resource logs were previously referred to as diagnostic logs.</li> <li>Activity log. Operations on each Azure resource in the subscription from the outside (the management plane) in addition to updates on Service Health events.</li> <li>Azure Active Directory logs. History of sign-in activity and audit trail of changes made in the Azure Active Directory.</li> </ul> <p>To analyze these logs in Log Analytics, you need send platform logs to your Log Analytics workspace.</p> <p>You can use PowerShell to create the diagnostics settings. For example, the following PowerShell code will send Key Vault diagnostics data to the Log Analytics workspace.</p> <pre><code>$LOG_ANALYTICS_RESOURCE_ID = \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourcegroups/oi-default-east-us/providers/microsoft.operationalinsights/workspaces/myworkspace\"\n$KEY_VAULE_RESOURCE_ID = \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourceGroups/myresourcegroup/providers/Microsoft.KeyVault/vaults/mykeyvault\"\nSet-AzDiagnosticSetting -Name KeyVault-Diagnostics -ResourceId  $KEY_VAULE_RESOURCE_ID `\n  -Category AuditEvent -MetricCategory AllMetrics -Enabled $true `\n  -WorkspaceId $LOG_ANALYTICS_RESOURCE_ID\n</code></pre> <p>Or you can configure your workspace using an ARM template.</p>"},{"location":"azure/compliance/log-analytics/#configure-your-workspace-using-arm-template","title":"Configure your workspace using ARM Template.","text":"<p>You may want to connect your workspace to one or more of the following.</p> <ol> <li>Add solutions to the workspace</li> <li>Create saved searches. To ensure that deployments don\u2019t override saved searches accidentally, an eTag property should be added in the \u201csavedSearches\u201d resource to override and maintain the idempotency of saved searches.</li> <li>Create saved function. The eTag should be added to override function and maintain idempotency.</li> <li>Create a computer group</li> <li>Enable collection of IIS logs from computers with the Windows agent installed</li> <li>Collect Logical Disk perf counters from Linux computers (% Used Inodes; Free Megabytes; % Used Space; Disk Transfers/sec; Disk Reads/sec; Disk Writes/sec)</li> <li>Collect syslog events from Linux computers</li> <li>Collect Error and Warning events from the Application Event Log from Windows computers</li> <li>Collect Memory Available Mbytes performance counter from Windows computers</li> <li>Collect IIS logs and Windows Event logs written by Azure diagnostics to a storage account</li> <li>Collect custom logs from Windows computer</li> </ol> <p>Azure documentation provides a sample ARM template to deploy that performs each of the steps. The following sample shows how to configure Log Analytics workspace from the documentation to set each of the steps.</p> <pre><code>{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"1.0.0.0\",\n  \"parameters\": {\n    \"workspaceName\": {\n      \"type\": \"string\",\n      \"metadata\": {\n        \"description\": \"Workspace name\"\n      }\n    },\n    \"sku\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"PerGB2018\",\n        \"Free\",\n        \"Standalone\",\n        \"PerNode\",\n        \"Standard\",\n        \"Premium\"\n      ],\n      \"defaultValue\": \"pergb2018\",\n      \"metadata\": {\n        \"description\": \"Pricing tier: pergb2018 or legacy tiers (Free, Standalone, PerNode, Standard or Premium) which are not available to all customers.\"\n      }\n    },\n    \"dataRetention\": {\n      \"type\": \"int\",\n      \"defaultValue\": 30,\n      \"minValue\": 7,\n      \"maxValue\": 730,\n      \"metadata\": {\n        \"description\": \"Number of days of retention. Workspaces in the legacy Free pricing tier can only have 7 days.\"\n      }\n    },\n    \"immediatePurgeDataOn30Days\": {\n      \"type\": \"bool\",\n      \"defaultValue\": \"[bool('false')]\",\n      \"metadata\": {\n        \"description\": \"If set to true, changing retention to 30 days will immediately delete older data. Use this with extreme caution. This only applies when retention is being set to 30 days.\"\n      }\n    },\n    \"location\": {\n      \"type\": \"string\",\n      \"allowedValues\": [\n        \"australiacentral\",\n        \"australiaeast\",\n        \"australiasoutheast\",\n        \"brazilsouth\",\n        \"canadacentral\",\n        \"centralindia\",\n        \"centralus\",\n        \"eastasia\",\n        \"eastus\",\n        \"eastus2\",\n        \"francecentral\",\n        \"japaneast\",\n        \"koreacentral\",\n        \"northcentralus\",\n        \"northeurope\",\n        \"southafricanorth\",\n        \"southcentralus\",\n        \"southeastasia\",\n        \"uksouth\",\n        \"ukwest\",\n        \"westcentralus\",\n        \"westeurope\",\n        \"westus\",\n        \"westus2\"\n      ],\n      \"metadata\": {\n        \"description\": \"Specifies the location in which to create the workspace.\"\n      }\n    },\n    \"applicationDiagnosticsStorageAccountName\": {\n      \"type\": \"string\",\n      \"metadata\": {\n        \"description\": \"Name of the storage account with Azure diagnostics output\"\n      }\n    },\n    \"applicationDiagnosticsStorageAccountResourceGroup\": {\n      \"type\": \"string\",\n      \"metadata\": {\n        \"description\": \"The resource group name containing the storage account with Azure diagnostics output\"\n      }\n    },\n    \"customLogName\": {\n      \"type\": \"string\",\n      \"metadata\": {\n        \"description\": \"The custom log name\"\n      }\n    }\n  },\n  \"variables\": {\n    \"Updates\": {\n      \"Name\": \"[Concat('Updates', '(', parameters('workspaceName'), ')')]\",\n      \"GalleryName\": \"Updates\"\n    },\n    \"AntiMalware\": {\n      \"Name\": \"[concat('AntiMalware', '(', parameters('workspaceName'), ')')]\",\n      \"GalleryName\": \"AntiMalware\"\n    },\n    \"SQLAssessment\": {\n      \"Name\": \"[Concat('SQLAssessment', '(', parameters('workspaceName'), ')')]\",\n      \"GalleryName\": \"SQLAssessment\"\n    },\n    \"diagnosticsStorageAccount\": \"[resourceId(parameters('applicationDiagnosticsStorageAccountResourceGroup'), 'Microsoft.Storage/storageAccounts', parameters('applicationDiagnosticsStorageAccountName'))]\"\n  },\n  \"resources\": [\n    {\n      \"apiVersion\": \"2017-03-15-preview\",\n      \"type\": \"Microsoft.OperationalInsights/workspaces\",\n      \"name\": \"[parameters('workspaceName')]\",\n      \"location\": \"[parameters('location')]\",\n      \"properties\": {\n        \"retentionInDays\": \"[parameters('dataRetention')]\",\n        \"features\": {\n          \"immediatePurgeDataOn30Days\": \"[parameters('immediatePurgeDataOn30Days')]\"\n        },\n        \"sku\": {\n          \"name\": \"[parameters('sku')]\"\n        }\n      },\n      \"resources\": [\n        {\n          \"apiVersion\": \"2015-03-20\",\n          \"name\": \"VMSS Queries2\",\n          \"type\": \"savedSearches\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          ],\n          \"properties\": {\n            \"eTag\": \"*\",\n            \"category\": \"VMSS\",\n            \"displayName\": \"VMSS Instance Count\",\n            \"query\": \"Event | where Source == \\\"ServiceFabricNodeBootstrapAgent\\\" | summarize AggregatedValue = count() by Computer\",\n            \"version\": 1\n          }\n        },\n        {\n          \"apiVersion\": \"2017-04-26-preview\",\n          \"name\": \"Cross workspace function\",\n          \"type\": \"savedSearches\",\n            \"dependsOn\": [\n             \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n            ],\n            \"properties\": {\n              \"etag\": \"*\",\n              \"displayName\": \"failedLogOnEvents\",\n              \"category\": \"Security\",\n              \"FunctionAlias\": \"failedlogonsecurityevents\",\n              \"query\": \"\n                union withsource=SourceWorkspace\n                workspace('workspace1').SecurityEvent,\n                workspace('workspace2').SecurityEvent,\n                workspace('workspace3').SecurityEvent,\n                | where EventID == 4625\"\n          }\n        },\n        {\n          \"apiVersion\": \"2015-11-01-preview\",\n          \"type\": \"datasources\",\n          \"name\": \"sampleWindowsEvent1\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          ],\n          \"kind\": \"WindowsEvent\",\n          \"properties\": {\n            \"eventLogName\": \"Application\",\n            \"eventTypes\": [\n              {\n                \"eventType\": \"Error\"\n              },\n              {\n                \"eventType\": \"Warning\"\n              }\n            ]\n          }\n        },\n        {\n          \"apiVersion\": \"2015-11-01-preview\",\n          \"type\": \"datasources\",\n          \"name\": \"sampleWindowsPerfCounter1\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          ],\n          \"kind\": \"WindowsPerformanceCounter\",\n          \"properties\": {\n            \"objectName\": \"Memory\",\n            \"instanceName\": \"*\",\n            \"intervalSeconds\": 10,\n            \"counterName\": \"Available MBytes\"\n          }\n        },\n        {\n          \"apiVersion\": \"2015-11-01-preview\",\n          \"type\": \"datasources\",\n          \"name\": \"sampleIISLog1\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          ],\n          \"kind\": \"IISLogs\",\n          \"properties\": {\n            \"state\": \"OnPremiseEnabled\"\n          }\n        },\n        {\n          \"apiVersion\": \"2015-11-01-preview\",\n          \"type\": \"datasources\",\n          \"name\": \"sampleSyslog1\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          ],\n          \"kind\": \"LinuxSyslog\",\n          \"properties\": {\n            \"syslogName\": \"kern\",\n            \"syslogSeverities\": [\n              {\n                \"severity\": \"emerg\"\n              },\n              {\n                \"severity\": \"alert\"\n              },\n              {\n                \"severity\": \"crit\"\n              },\n              {\n                \"severity\": \"err\"\n              },\n              {\n                \"severity\": \"warning\"\n              }\n            ]\n          }\n        },\n        {\n          \"apiVersion\": \"2015-11-01-preview\",\n          \"type\": \"datasources\",\n          \"name\": \"sampleSyslogCollection1\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          ],\n          \"kind\": \"LinuxSyslogCollection\",\n          \"properties\": {\n            \"state\": \"Enabled\"\n          }\n        },\n        {\n          \"apiVersion\": \"2015-11-01-preview\",\n          \"type\": \"datasources\",\n          \"name\": \"sampleLinuxPerf1\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          ],\n          \"kind\": \"LinuxPerformanceObject\",\n          \"properties\": {\n            \"performanceCounters\": [\n              {\n                \"counterName\": \"% Used Inodes\"\n              },\n              {\n                \"counterName\": \"Free Megabytes\"\n              },\n              {\n                \"counterName\": \"% Used Space\"\n              },\n              {\n                \"counterName\": \"Disk Transfers/sec\"\n              },\n              {\n                \"counterName\": \"Disk Reads/sec\"\n              },\n              {\n                \"counterName\": \"Disk Writes/sec\"\n              }\n            ],\n            \"objectName\": \"Logical Disk\",\n            \"instanceName\": \"*\",\n            \"intervalSeconds\": 10\n          }\n        },\n        {\n          \"apiVersion\": \"2015-11-01-preview\",\n          \"type\": \"dataSources\",\n          \"name\": \"[concat(parameters('workspaceName'), parameters('customLogName'))]\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', '/', parameters('workspaceName'))]\"\n          ],\n          \"kind\": \"CustomLog\",\n          \"properties\": {\n            \"customLogName\": \"[parameters('customLogName')]\",\n            \"description\": \"this is a description\",\n            \"extractions\": [\n              {\n                \"extractionName\": \"TimeGenerated\",\n                \"extractionProperties\": {\n                  \"dateTimeExtraction\": {\n                    \"regex\": [\n                      {\n                        \"matchIndex\": 0,\n                        \"numberdGroup\": null,\n                        \"pattern\": \"((\\\\d{2})|(\\\\d{4}))-([0-1]\\\\d)-(([0-3]\\\\d)|(\\\\d))\\\\s((\\\\d)|([0-1]\\\\d)|(2[0-4])):[0-5][0-9]:[0-5][0-9]\"\n                      }\n                    ]\n                  }\n                },\n                \"extractionType\": \"DateTime\"\n              }\n            ],\n            \"inputs\": [\n              {\n                \"location\": {\n                  \"fileSystemLocations\": {\n                    \"linuxFileTypeLogPaths\": null,\n                    \"windowsFileTypeLogPaths\": [\n                      \"[concat('c:\\\\Windows\\\\Logs\\\\',parameters('customLogName'))]\"\n                    ]\n                  }\n                },\n                \"recordDelimiter\": {\n                  \"regexDelimiter\": {\n                    \"matchIndex\": 0,\n                    \"numberdGroup\": null,\n                    \"pattern\": \"(^.*((\\\\d{2})|(\\\\d{4}))-([0-1]\\\\d)-(([0-3]\\\\d)|(\\\\d))\\\\s((\\\\d)|([0-1]\\\\d)|(2[0-4])):[0-5][0-9]:[0-5][0-9].*$)\"\n                  }\n                }\n              }\n            ]\n          }\n        },\n        {\n          \"apiVersion\": \"2015-11-01-preview\",\n          \"type\": \"datasources\",\n          \"name\": \"sampleLinuxPerfCollection1\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          ],\n          \"kind\": \"LinuxPerformanceCollection\",\n          \"properties\": {\n            \"state\": \"Enabled\"\n          }\n        },\n        {\n          \"apiVersion\": \"2015-03-20\",\n          \"name\": \"[concat(parameters('applicationDiagnosticsStorageAccountName'),parameters('workspaceName'))]\",\n          \"type\": \"storageinsightconfigs\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          ],\n          \"properties\": {\n            \"containers\": [\n              \"wad-iis-logfiles\"\n            ],\n            \"tables\": [\n              \"WADWindowsEventLogsTable\"\n            ],\n            \"storageAccount\": {\n              \"id\": \"[variables('diagnosticsStorageAccount')]\",\n              \"key\": \"[listKeys(variables('diagnosticsStorageAccount'),'2015-06-15').key1]\"\n            }\n          }\n        },\n        {\n          \"apiVersion\": \"2015-11-01-preview\",\n          \"location\": \"[parameters('location')]\",\n          \"name\": \"[variables('Updates').Name]\",\n          \"type\": \"Microsoft.OperationsManagement/solutions\",\n          \"id\": \"[concat('/subscriptions/', subscription().subscriptionId, '/resourceGroups/', resourceGroup().name, '/providers/Microsoft.OperationsManagement/solutions/', variables('Updates').Name)]\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          ],\n          \"properties\": {\n            \"workspaceResourceId\": \"[resourceId('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          },\n          \"plan\": {\n            \"name\": \"[variables('Updates').Name]\",\n            \"publisher\": \"Microsoft\",\n            \"product\": \"[Concat('OMSGallery/', variables('Updates').GalleryName)]\",\n            \"promotionCode\": \"\"\n          }\n        },\n        {\n          \"apiVersion\": \"2015-11-01-preview\",\n          \"location\": \"[parameters('location')]\",\n          \"name\": \"[variables('AntiMalware').Name]\",\n          \"type\": \"Microsoft.OperationsManagement/solutions\",\n          \"id\": \"[concat('/subscriptions/', subscription().subscriptionId, '/resourceGroups/', resourceGroup().name, '/providers/Microsoft.OperationsManagement/solutions/', variables('AntiMalware').Name)]\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          ],\n          \"properties\": {\n            \"workspaceResourceId\": \"[resourceId('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          },\n          \"plan\": {\n            \"name\": \"[variables('AntiMalware').Name]\",\n            \"publisher\": \"Microsoft\",\n            \"product\": \"[Concat('OMSGallery/', variables('AntiMalware').GalleryName)]\",\n            \"promotionCode\": \"\"\n          }\n        },\n        {\n          \"apiVersion\": \"2015-11-01-preview\",\n          \"location\": \"[parameters('location')]\",\n          \"name\": \"[variables('SQLAssessment').Name]\",\n          \"type\": \"Microsoft.OperationsManagement/solutions\",\n          \"id\": \"[concat('/subscriptions/', subscription().subscriptionId, '/resourceGroups/', resourceGroup().name, '/providers/Microsoft.OperationsManagement/solutions/', variables('SQLAssessment').Name)]\",\n          \"dependsOn\": [\n            \"[concat('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          ],\n          \"properties\": {\n            \"workspaceResourceId\": \"[resourceId('Microsoft.OperationalInsights/workspaces/', parameters('workspaceName'))]\"\n          },\n          \"plan\": {\n            \"name\": \"[variables('SQLAssessment').Name]\",\n            \"publisher\": \"Microsoft\",\n            \"product\": \"[Concat('OMSGallery/', variables('SQLAssessment').GalleryName)]\",\n            \"promotionCode\": \"\"\n          }\n        }\n      ]\n    }\n  ],\n  \"outputs\": {\n    \"workspaceName\": {\n      \"type\": \"string\",\n      \"value\": \"[parameters('workspaceName')]\"\n    },\n    \"provisioningState\": {\n      \"type\": \"string\",\n      \"value\": \"[reference(resourceId('Microsoft.OperationalInsights/workspaces', parameters('workspaceName')), '2015-11-01-preview').provisioningState]\"\n    },\n    \"source\": {\n      \"type\": \"string\",\n      \"value\": \"[reference(resourceId('Microsoft.OperationalInsights/workspaces', parameters('workspaceName')), '2015-11-01-preview').source]\"\n    },\n    \"customerId\": {\n      \"type\": \"string\",\n      \"value\": \"[reference(resourceId('Microsoft.OperationalInsights/workspaces', parameters('workspaceName')), '2015-11-01-preview').customerId]\"\n    },\n    \"sku\": {\n      \"type\": \"string\",\n      \"value\": \"[reference(resourceId('Microsoft.OperationalInsights/workspaces', parameters('workspaceName')), '2015-11-01-preview').sku.name]\"\n    },\n    \"retentionInDays\": {\n      \"type\": \"int\",\n      \"value\": \"[reference(resourceId('Microsoft.OperationalInsights/workspaces', parameters('workspaceName')), '2015-11-01-preview').retentionInDays]\"\n    },\n    \"immediatePurgeDataOn30Days\": {\n      \"type\": \"bool\",\n      \"value\": \"[reference(resourceId('Microsoft.OperationalInsights/workspaces', parameters('workspaceName')), '2015-11-01-preview').features.immediatePurgeDataOn30Days]\"\n    },\n    \"portalUrl\": {\n      \"type\": \"string\",\n      \"value\": \"[reference(resourceId('Microsoft.OperationalInsights/workspaces', parameters('workspaceName')), '2015-11-01-preview').portalUrl]\"\n    }\n  }\n}\n</code></pre> <p>As owner of the workspace, you can soon tell whether you are able to see or manage sensitive data. If so, you can set up access control.</p>"},{"location":"azure/compliance/log-analytics/#set-up-access-control","title":"Set up access control","text":"<p>Not everyone should have access to the logs and be able to build queries. You can control access assigning groups to build in roles or use configure custom role based on your particular needs.</p>"},{"location":"azure/compliance/log-analytics/#using-built-in-roles-for-log-analytics-workspaces","title":"Using built-in roles for Log Analytics workspaces","text":"<p>Azure has two built-in user roles for Log Analytics workspaces:</p> <ul> <li>Log Analytics Reader</li> <li>Log Analytics Contributor</li> </ul> <p>Members of the Log Analytics Reader role can:</p> <ul> <li>View and search all monitoring data.</li> <li>View monitoring settings, including viewing the configuration of Azure diagnostics on all Azure resources.</li> </ul> <p>Members of the Log Analytics Contributor role can:</p> <ul> <li>Includes all the privileges of the Log Analytics Reader role, allowing the user to read all monitoring data</li> <li>Create and configure Automation accounts (permission must be granted at the resource group or subscription scope)</li> <li>Add and remove management solutions (permission must be granted at the resource group or subscription scope)</li> <li>Read storage account keys</li> <li>Configure the collection of logs from Azure Storage</li> <li>Edit monitoring settings for Azure resources, including<ul> <li>Adding the VM extension to VMs</li> <li>Configuring Azure diagnostics on all Azure resources</li> </ul> </li> </ul> <p>The following PowerShell commmand shows how you can set up a group named Log Analytics Reader Group.</p> <p><pre><code>Install-Module azuread\nNew-AzureADGroup -Description \"Log Analytics Reader Group\" -DisplayName \"Log Analytics Reader Group\" -MailEnabled $false -SecurityEnabled $true -MailNickName \"LogAnalyticsReaderGroup\"\n</code></pre> You can then assign users to the group to managed the access.</p>"},{"location":"azure/compliance/log-analytics/#using-custom-roles-for-log-analytics-workspaces","title":"Using custom roles for Log Analytics workspaces","text":"<p>You may want to set up a custom role that combines permissions.\u00a0This provides more granular control of who accesses data in Log analytics workspace. You can fine tune who has permissions to the data in workspace using workspace permissions, which you can can combine into custom roles. See Azure custom roles.</p>"},{"location":"azure/compliance/log-analytics/#role-based-access-control-best-practices","title":"Role based access control best practices","text":"<p>Assign\u00a0roles to security groups instead of individual users to reduce the number of assignments.</p> <p>The following script sets a group named Log Analytics Reader Group to the resource group.</p> <pre><code>Install-Module azuread\nNew-AzureADGroup -Description \"Log Analytics Reader Group\" -DisplayName \"Log Analytics Reader Group\" -MailEnabled $false -SecurityEnabled $true -MailNickName \"LogAnalyticsReaderGroup\"\n</code></pre> <p>Users assigned to the Log Analytics Reader Group in Azure Active Directory would have Log Analytics Reader permissions in the resources within the resource group.</p>"},{"location":"azure/compliance/log-analytics/#set-up-your-queries","title":"Set up your queries","text":"<p>Once you have data coming into Log Analytics, you will want to set up your queries. The art for Cloud Engineers is to build queries that provide visibility into your own cloud operations.</p> <p>To get started, Azure provides some samples. To find the samples, navigate to your Log Analytics workspace. Click Logs in the General panel. You can find some example queries as shown in the following illustration.</p> <p></p>"},{"location":"azure/compliance/log-analytics/#building-queries-in-log-analytics-workspace","title":"Building queries in Log Analytics workspace","text":"<p>When you examine the queries, you can see they are very SQL-like. You can use Azure Data Explorer to explore the tables and columns available to you.</p> <p>Azure Monitor Logs is based on &gt;Azure Data Explorer, and log queries are written using the same Kusto query language (KQL). If you are familiar with SQL, Kusto queriers will be similar (see SQL to Kusto query translation for more details).</p> <p>As an example, the following query counts how many rows in the Logs table have the value of the Level column equals the string Critical:</p> <pre><code>Logs\n| where Level == \"Critical\"\n| count\n</code></pre>"},{"location":"azure/compliance/log-analytics/#next-steps","title":"Next steps","text":"<p>In this article, you learned how to set up Log Analytics Workspace and how to retrieve the workspace.</p> <p>As you build each resource, you will want to consider how you will monitor and then send data to Log Analytics for your Cloud Engineers to analyze. You will want to associate your resources to Log Analytics as you deploy each resource.</p> <p>Next steps:</p> <ul> <li>Set up Security Center, as described in our next blog post.</li> <li>Learn about how to Collect Azure platform logs in Log Analytics workspace in Azure Monitor</li> <li>Use a resource manager template to enable log alert rules</li> <li>Use a resource manager template to enable your virtual machines to work with the workspace</li> <li>Use a resource manager template to enable Azure Monitor to work with the workspace</li> <li>Use a resource manager template to enable your AKS cluster to work with the workspace</li> <li>Explore Azure Policy definitiona for Azure Monitor</li> <li>Explore Azure Monitor for containers overview</li> </ul>"},{"location":"azure/compliance/log-analytics/#resources","title":"Resources","text":"<ul> <li>Overview of log queries in Azure Monitor.</li> <li>Manage access to log data and workspaces in Azure Monitor.</li> <li>Manage app and resource access using Azure Active Directory groups.</li> <li>Using Azure Monitor Logs with Azure Kubernetes Service (AKS).</li> <li>Getting started with Kusto.</li> </ul>"},{"location":"azure/compliance/regulatory-compliance/","title":"Walkthrough using Azure Policy to audit and enforce compliance","text":"<p> Use Azure Policy to manage and enforce your standards for governance and compliance and to assess that compliance at scale. When you implement Azure Policy, you are effectively adding guard-rails for your users. But you also have a way to audit your organization compliance against a particular policy.</p> <p>In this walkthrough, you will learn the implications of using a Policy in Azure. For this walkthrough, you will use Azure CLI to create a storage account that will not be compliant, but allowing its contents to be accessed using HTTP. Then you will add a Policy that requires HTTPS, and see how you can audit existing, non-compliant resource. You will audit the resource using the portal and using PowerShell script. Then you will create another non-compliant resource and see how Azure blocks the resource during creation.</p>"},{"location":"azure/compliance/regulatory-compliance/#prerequisites","title":"Prerequisites","text":"<ul> <li>It is helpful that you understand resource groups and about storage accounts. Although the sample provides code, you will want to know how to upload a small file into Azure blob storage. You can use the portal or AzCopy.</li> <li>The sample shows how to use specific role, but you should understand about role based access control roles.</li> <li>You will need owner or contributor access in your subscription.</li> </ul>"},{"location":"azure/compliance/regulatory-compliance/#definitions","title":"Definitions","text":"<p>First let\u2019s start with some definitions:</p> <p>Business rule is a standard that the business wants to audit or wants to insure is compliance. Often, but not always, business rules follow compliance standards, such as ISO 27001 or NIST.</p> <p>Resource Provider is service that supplies Azure resources. For example, a common resource provider is Microsoft.Storage, which supplies the storage resource. The name of a resource type is in the format:\u00a0<code>{resource-provider}/{resource-type}</code>.</p> <p>Resource is a manageable item that is available through Azure. Virtual machines, storage accounts, web apps, databases, and virtual networks are examples of resources. when the resource is created, it has an id. The id has a field that has a format like <code>/subscriptions//resourceGroups//providers/Microsoft.Compute/disks/</code>.</p> <p>Resources have properties that you set when you create the resource. For example, when you create a storage account, you set its location.</p> <p>Azure Policy examines properties on resources that are represented in Resource Manager and properties of some Resource Providers. For example the location of a resource is a property that a policy can audit.</p> <p>Policy definition is the JSON implementation of a business rule.</p> <p>Several business rules can be grouped together into a Policy initiative.</p> <p>The policy definition or initiative is assigned to any scope that can be a management groups, subscriptions, resource groups, or individual resources.</p> <p>A policy assignment applies the policy to all resources within the scope. For example, if the policy</p> <p>For more information, see:</p> <ul> <li>What is Azure Resource Manager?</li> <li>Azure resource providers and types</li> <li>What is Azure Policy?</li> </ul>"},{"location":"azure/compliance/regulatory-compliance/#identify-the-business-requirements","title":"Identify the business requirements","text":"<p>Let\u2019s begin by defining the compliance requirements.</p> <p>The requirements should clearly identify both the \u201cto be\u201d and the \u201cnot to be\u201d resource states.</p> <p>For example in this case:</p> <p>Compliance Standard</p> <p>ISO 27001</p> <p>Control</p> <p>A.10.1.1 Policy on the use of cryptographic controls</p> <p>Requirement</p> <p>ISO requirement: Audit secure transfer to storage accounts Our business requirement: Require secure transfer to storage accounts</p> <p>Rules</p> <ul> <li>Each storage account must be enabled for HTTPS</li> <li>Each storage account must be disabled for HTTP</li> </ul> <p>First, let\u2019s figure out the resource property that we want to build our policy on.</p>"},{"location":"azure/compliance/regulatory-compliance/#explore-resource-properties-that-might-want-to-check","title":"Explore resource properties that might want to check","text":"<p>Each resource in Azure is built on a set of APIs that are defined at the top level as Azure resource providers. A resource provider is [uhm] a service that provides resources, such as storage.</p> <p>For the list of the providers, see Resource providers for Azure services. To get a list of providers and the status of whether they are installed in your subscription, use the following command:</p> <pre><code>az provider list --query \"[].{Provider:namespace, Status:registrationState}\" --out table\n</code></pre> <p>Let\u2019s start by reviewing the Microsoft.Storage resource provider. Resource Manager template reference for the storage account resource gives you a (nearly all) the property. In this case, the <code>supportsHttpsTrafficOnly</code> will be the one we use.</p> <p></p> <p>In the <code>StorageAccountPropertiesCreateParameters object</code> is the <code>supportsHttpsTrafficOnly</code>.</p> <p></p>"},{"location":"azure/compliance/regulatory-compliance/#another-way-to-explore-resources","title":"Another way to explore resources","text":"<p>Once you have created a resource, you can inspect their properties. Use the Azure Resource Explorer to inspect the context of your subscription. You can browse by providers, subscriptions, resource groups, and resources.</p>"},{"location":"azure/compliance/regulatory-compliance/#find-the-property-alias","title":"Find the property alias","text":"<p>We need to map the property we found to it\u2019s alias. When you create a policy, it uses aliases to restrict what values or conditions are allowed. Each alias maps to paths in different API versions for a given resource type. During policy evaluation, the policy engine gets the property path for that API version.</p> <p>Use the following cli to get the alias used by the Azure Policy.</p> <pre><code>az provider show --namespace Microsoft.Storage --expand \"resourceTypes/aliases\" --query \"resourceTypes[].aliases[].name\"\n</code></pre> <p>As shown in the following illustration, the results show that <code>supportsHttpsTrafficOnly</code> is supported.</p> <p></p> <p>This means we can write a policy based on <code>supportsHttpsTrafficOnly</code>.</p> <p>Another way to query the aliases is to use Azure Graph. The following code installs Azure Graph into the CLI and queries the aliases. The results are provided in a heirarchy that may be easier to view.</p> <pre><code>az extension add --name resource-graph\naz graph query -q \"Resources | where type=~'microsoft.storage/storageaccounts' | limit 1 | project aliases\"\n</code></pre> <p>Before we go install a custom policy, let\u2019s create a sample resource that will not be compliant.</p>"},{"location":"azure/compliance/regulatory-compliance/#create-a-non-compliant-storage-account","title":"Create a non-compliant storage account","text":"<p>Let\u2019s begin by creating a storage account resource that allows applications to access the storage, create a container, and upload a file. Log into Azure using <code>az login</code>, then use the following code to create a resource group, storage account, and storage container; grant yourself contributor access to the storage account, create a file and upload it.</p> <pre><code>RESOURCE_GROUP_NAME=\"rg-wus2-storagepolicy\"\nLOCATION=\"west us 2\"\nRANDOM=$$\nSTORAGE_ACCOUNT_NAME=ststoragepolicy$RANDOM\nSTORAGE_CONTAINER_NAME=thefolder\nCOST_CENTER=demo\nENVIRONMENT=\"testing it\"\n\n# create resource group\naz group create \\\n    --name $RESOURCE_GROUP_NAME \\\n    --location \"$LOCATION\" \naz group update -n $RESOURCE_GROUP_NAME \\\n    --set tags.'Cost Center'=\"$COST_CENTER\" tags.'Environment'=\"$ENVIRONMENT\"\n\n# get your sign in name and assign yourself 'Storage Blob Data Contributor' \n# permissions to the resource group. The storage account will inherit your role\nUSER==$(az ad signed-in-user show --query userPrincipalName -o tsv)\naz role assignment create --role \"Storage Blob Data Contributor\" \\\n    --assignee $USER --resource-group $RESOURCE_GROUP_NAME\n\n# it takes a couple minutes for the role to propogate once it is created\nsleep 2m\n\naz storage account create \\\n    --name $STORAGE_ACCOUNT_NAME \\\n    --resource-group $RESOURCE_GROUP_NAME \\\n    --location \"$LOCATION\" \\\n    --sku Standard_LRS \\\n    --encryption-services blob \\\n    --https-only false \\\n    --tags Cost Center=$COST_CENTER Environment=$ENVIRONMENT\n\naz storage container create \\\n    --account-name $STORAGE_ACCOUNT_NAME \\\n    --name $STORAGE_CONTAINER_NAME \\\n    --auth-mode login\n\n# create a helloworld.html page for storage\ncat &lt;&lt;EOF &gt; helloworld.html\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;head&gt;\n        &lt;title&gt;Hello World Page&lt;/title&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\nHello World!\n    &lt;/body&gt;\n&lt;/html&gt;\nEOF\n\n# upload the file\naz storage blob upload \\\n    --account-name $STORAGE_ACCOUNT_NAME \\\n    --container-name $STORAGE_CONTAINER_NAME \\\n    --name helloworld.html \\\n    --file helloworld.html \\\n    --auth-mode login\n\naz storage blob list \\\n    --account-name $STORAGE_ACCOUNT_NAME \\\n    --container-name $STORAGE_CONTAINER_NAME \\\n    --output table \\\n    --auth-mode login\n</code></pre> <p>Note: For the demo to work, you will explicitly need to assign the\u00a0Storage Blob Data Contributor role to yourself. Even though you are the account owner, you need explicit permissions to perform data operations against the storage account.</p> <p>The storage account is intentionally created with <code>--https-only false</code>.</p> <p>Next, before we create our custom policy, let\u2019s test to see if there is a voilation of Azure policy from our existing policies.</p>"},{"location":"azure/compliance/regulatory-compliance/#test-against-current-policies","title":"Test against current policies","text":"<p>Does the new storage resource already violate one of our policies? Log into the Azure Portal, search in the search bar for Policy. And there is an intiative that was installed by Azure Security Center that flags the storage resource as not being compliant, as shown in the following illustration.</p> <p></p> <p>Click through the links initiative and you can see resource that we just created and the rule that was violated, as shown in the following illustration.</p> <p></p> <p>Our business requirement is to now allow the creation of storage accounts, not to just audit. In the next step, create a custom policy to stop anyone from creating a storage account that is non-compliant.</p>"},{"location":"azure/compliance/regulatory-compliance/#create-custom-policy-to-deny-the-non-compliant-storage-creation","title":"Create custom policy to deny the non-compliant storage creation","text":"<p>It is possible to go into the portal and just change the policy. You could go into the initiative, edit the assignment of the policy, and change the parameter from <code>Audit</code> to <code>Deny</code>. But in our world where you want to track the changes and to automate your new initiatives, you will want to create a custom policy that denies creation of the non-compliant resource.</p> <p>The following JSON (from the Microsoft documentation Tutorial: Create a custom policy definition) shows the policy.</p> <pre><code>{\n    \"properties\": {\n        \"displayName\": \"Deny storage accounts not using only HTTPS\",\n        \"description\": \"Deny storage accounts not using only HTTPS. Checks the supportsHttpsTrafficOnly property on StorageAccounts.\",\n        \"mode\": \"all\",\n        \"parameters\": {\n            \"effectType\": {\n                \"type\": \"string\",\n                \"defaultValue\": \"Deny\",\n                \"allowedValues\": [\n                    \"Deny\",\n                    \"Disabled\"\n                ],\n                \"metadata\": {\n                    \"displayName\": \"Effect\",\n                    \"description\": \"Enable or disable the execution of the policy\"\n                }\n            }\n        },\n        \"policyRule\": {\n            \"if\": {\n                \"allOf\": [\n                    {\n                        \"field\": \"type\",\n                        \"equals\": \"Microsoft.Storage/storageAccounts\"\n                    },\n                    {\n                        \"field\": \"Microsoft.Storage/storageAccounts/supportsHttpsTrafficOnly\",\n                        \"notEquals\": \"true\"\n                    }\n                ]\n            },\n            \"then\": {\n                \"effect\": \"[parameters('effectType')]\"\n            }\n        }\n    }\n}\n</code></pre> <p>Several items to note from the code:</p> <ul> <li>The <code>displayName</code> and <code>description</code> elements show the intent of the policy.</li> <li>The <code>effectType</code> element on lines 7 to 18 defaults to <code>Deny</code>, but allows you to set the effect to <code>Disabled</code>, which allows you to turn off the rule.</li> <li>The <code>policyRule</code> requies both:<ul> <li>The storage account type is <code>Microsoft.Storage/storageAccounts</code> and</li> <li>The storage account <code>supportsHttpsTrafficOnly</code> property is not true</li> </ul> </li> </ul> <p>Next, let\u2019s split up our policy so we can create the policy definition.</p>"},{"location":"azure/compliance/regulatory-compliance/#create-the-policy-definition","title":"Create the policy definition","text":"<p>The policy definition consists of rules and parameters for those rules. Both are declared in JSON. The following script uses Bash to create two files, one for the rules and one to define the parameters.</p> <pre><code>mkdir requirehttps &amp;&amp; cd requirehttps\ncat &lt;&lt;EOF &gt; requirehttps.azurepolicy.json\n{\n  \"if\": {\n    \"allOf\": [\n      {\n        \"field\": \"type\",\n        \"equals\": \"Microsoft.Storage/storageAccounts\"\n      },\n      {\n        \"field\": \"Microsoft.Storage/storageAccounts/supportsHttpsTrafficOnly\",\n        \"notEquals\": \"true\"\n      }\n    ]\n  },\n  \"then\": {\n    \"effect\": \"[parameters('effectType')]\"\n  }\n}\nEOF\n\nRULES_FILE_URL=\"./requirehttps.azurepolicy.json\"\n\ncat &lt;&lt;EOF &gt; requirehttps.azurepolicy.parameters.json\n{\n  \"effectType\": {\n    \"type\": \"string\",\n    \"defaultValue\": \"Deny\",\n    \"allowedValues\": [\n      \"Deny\",\n      \"Disabled\"\n    ],\n    \"metadata\": {\n      \"displayName\": \"Effect\",\n      \"description\": \"Enable or disable the execution of the policy\"\n    }\n  }\n}\nEOF\n\nPARAMETERS_FILE_URL=\"./requirehttps.azurepolicy.parameters.json\"\n</code></pre> <p>Use <code>az policy definition create</code> to create the policy definition. See Create a policy definition with Azure CLI. The following code creates a policy definition and saves the policy definition to a subscription. It does not assign the policy to the subscription.</p> <pre><code>SUBSCRIPTION_ID=c2b15f36-f522-451c-84e3-a4fc54056617\n\naz policy definition create --name \"denyStorageAccountNotUsingHttps\" \\\n  --display-name \"Deny storage accounts not using only HTTPS\" \\\n  --description \"Deny storage accounts that are not using only HTTPS. Checks the supportsHttpsTrafficOnly property on Microsoft.Storage/storageAccounts provider.\" \\\n  --rules $RULES_FILE_URL \\\n  --params  $PARAMETERS_FILE_URL \\\n  --subscription $SUBSCRIPTION_ID\n</code></pre> <p>Once you have defined the policy, you can find it in the portal. Open the portal, search Policy, click Definitions to all the the policies. Search on https. The following illustration shows the definition of the matchint policies.</p> <p></p> <p>Click Deny storage accounts not using only HTTPS to see your policy definition.</p> <p>You may want to consider the following best practices:</p> <p>You could have saved the policy to a management group, which is then available as a the policy definition to all of the subscriptions associated with the management group.</p> <p>The rule and param URLs can point to your rules in GitHub or Azure DevOps.</p> <p>Now that we have a definition available in our subsription, you need to apply it.</p>"},{"location":"azure/compliance/regulatory-compliance/#apply-the-policy-to-a-scope","title":"Apply the policy to a scope","text":"<p>Now that your policy has been defined and available in your subscription, you need to assign the policy. This step sets the policy to a particular scope.</p> <p>Valid scopes are management group, subscription, resource group, and resource as shown in this table:</p> <p>Management group</p> <p><code>/providers/Microsoft.Management/managementGroups/MyManagementGroup</code></p> <p>Subscription</p> <p><code>/subscriptions/c2b15f36-f522-451c-84e3-a4fc54056617</code></p> <p>Resource group</p> <p><code>/subscriptions/c2b15f36-f522-451c-84e3-a4fc54056617/resourceGroups/myGroup</code></p> <p>Resource</p> <p><code>/subscriptions/c2b15f36-f522-451c-84e3-a4fc54056617/resourceGroups/myGroup/providers/Microsoft.Compute/virtualMachines/myVM</code></p> <p>You can assign the policy in the portal. See</p> <p>To automate it, use an Azure CLI script command <code>az policy assignment create</code> to assign the policy definition to the scope. The following sample assigns the policy to the subscription scope.</p> <pre><code>SCOPE=/subscriptions/$SUBSCRIPTION_ID\n\naz policy assignment create --name \"Require https for storage in subscription\" --scope $SCOPE \\\n  --policy \"denyStorageAccountNotUsingHttps\" \\\n  --params '{ \"effectType\" : { \"value\": \"Deny\" } }'\n</code></pre> <p>As it assigns the policy, it also provides the parameters for this particular assignment.</p> <p>Now that you have the policy assigned, what happened to the non-compliant resource?</p>"},{"location":"azure/compliance/regulatory-compliance/#check-compliance","title":"Check compliance","text":"<p>Once you have assigned the policy to your subscription, you will want to check compliance. The resource created in an earlier step still fails based on the Security Center rule. But it also fails based on the new custom rules you added in the previous step.</p> <p>To view, open Policy blade in the Azure portal. Click Compliance.</p> <p></p> <p>It takes a few minutes to scan your resources.</p> <p>Assigning a policy with a \u201cdeny\u201d effect may take up to 30 mins (average case) and 1 hour (worst case) to start denying the creation of non-compliant resources.</p> <p>You can query for non-compliant resources in PowerShell. The following cmdlet returns the details for all non-compliant storage accounts.</p> <pre><code>Get-AzPolicyState -Filter \"ResourceType eq '/Microsoft.Storage/storageAccounts'\"\n</code></pre>"},{"location":"azure/compliance/regulatory-compliance/#creation-of-a-non-compliant-resource-now-fails","title":"Creation of a non-compliant resource now fails","text":"<p>To test that the policy denies creation of an non-compliant resource, run the script in the previous section to create a non-compliant storage account. The following code tries to create a new storage account in the same resource group.</p> <pre><code>RESOURCE_GROUP_NAME=\"rg-wus2-storagepolicy\"\nLOCATION=\"west us 2\"\nRANDOM=$$\nSTORAGE_ACCOUNT_NAME=ststoragepolicy$RANDOM\nSTORAGE_CONTAINER_NAME=thefolder\nCOST_CENTER=demo\nENVIRONMENT=\"testing it\"\n\naz storage account create \\\n    --name $STORAGE_ACCOUNT_NAME \\\n    --resource-group $RESOURCE_GROUP_NAME \\\n    --location \"$LOCATION\" \\\n    --sku Standard_LRS \\\n    --encryption-services blob \\\n    --https-only false \\\n    --tags Cost Center=$COST_CENTER Environment=$ENVIRONMENT\n</code></pre> <p>Once completed, you will see an error message.</p> <p></p> <p>When you go into the portal, click Compliance in the Policy page to see results. Click Require https for storage in subscription to see the summary of non-compliance. Notice that the existing resource is audited as not compliant. And the denial for the creating of the non-compliant storage account is shown.</p> <p></p>"},{"location":"azure/compliance/regulatory-compliance/#view-compliance-in-log-analytics","title":"View compliance in Log Analytics","text":"<p>Because you installed a Log Analytics workspace as described in the post Setting up Log Analytics workspace for production in enterprise, you can view <code>AzureActivity</code> from the Activity Log Analytics solution tied to your subscription.</p> <p></p>"},{"location":"azure/compliance/regulatory-compliance/#apply-policy-using-a-arm-template","title":"Apply policy using a ARM template","text":"<p>The documentation Quickstart: Create a policy assignment to identify non-compliant resources by using a Resource Manager template shows how you can deploy a policy to a resource group.</p> <p>You will need the <code>policyDefinitionID</code>. Use the following command to get the properties of the policy you want to apply.</p> <pre><code>az policy definition show --name denyStorageAccountNotUsingHttps\n</code></pre> <p>Use the properties when you deploy the ARM template.</p>"},{"location":"azure/compliance/regulatory-compliance/#summary","title":"Summary","text":"<p>This was a deep-dive walkthrough into how to define and implement your own policies, which build-in governance best practices for your users. You learned the workflow of a custom policy and how to deploy the policy into either a subscription or management group. And you learned how to check your compliance with all your initiatives.</p>"},{"location":"azure/compliance/regulatory-compliance/#next-steps","title":"Next steps","text":"<ul> <li>Define your own specific bsuiness requirements for your own compliance initiatives. Review the Azure Policy security baseline for Azure Security Benchmark.</li> <li>Define tags so you can idenity who ownership of resources. Define business rules, policy definition, and assign policies so you will know who to contact and when to review resources as policies change.</li> <li>Review the built-in policy definitions and initiatives to help you in quickly building your own compliance initiatives. Note how the policies have versions</li> <li>Follow and update policies that are on GitHub</li> <li>Understand Azure Policy effects</li> <li>Learn about Blueprints that can group together your policies, resource groups, and role assignments for automation</li> <li>Design Policy as Code workflows</li> <li>Define alerts to watch for non-compliance in Azure Monitor logs.</li> </ul>"},{"location":"azure/compliance/regulatory-compliance/#resources","title":"Resources","text":"<ul> <li>Quickstart: Create, download, and list blobs with Azure CLI</li> <li>Authorize with Azure AD credentials</li> <li>Quickstart: Create a policy assignment to identify non-compliant resources with Azure CLI</li> <li>Azure resource providers and types</li> </ul>"},{"location":"azure/compliance/resource-management/","title":"Organize Azure resources using management group, tags, naming convention","text":"<p>Once you have set up your Azure administrators, you can begin to consider how to organize your cloud into management groups, subscriptions, resource groups. You will want to develop a naming standard, and way to tag resources.</p> <p>Although you may be focused initially on just getting your resources deployed, you will want to be able to manage them. For example, a year from now you may want to know who is responsible for the virtual machine that is no longer doing anything, but is costing money. In other words, you may want lifecycle management.</p> <p>You may want the ability to charge a set of resources to a cost center and to budget those resources. For example, you may want to receive alerts for both the users and for your administrators when costs are out of line with expectations.</p> <p>And as we all know, it is easier to organize as you go. In this article, you will learn about some key points in organizing your Azure resources.</p> <p>In this article, learn about the Azure hierarchy, naming standards, and resource tags.</p> <p>As you deploy resources, you will want specific standards specified for your cloud admins and users. The payoff is that you will be able to maintain your cloud at scale. You can figure out who or what team is responsible for the resource, determine whether the resource is still needed, and figure out what that resource is costing. And it begins with specific standards for:</p> <ul> <li>Management hierarchy</li> <li>Names</li> <li>Tags</li> </ul> <p>Let\u2019s start with management hierarchy.</p>"},{"location":"azure/compliance/resource-management/#management-hierarchy","title":"Management hierarchy","text":"<p>You can think of Azure in four levels for your management: management group, subscriptions, resource groups, and resources. The following illustration shows a partial management hierarchy for Azure.</p> <p></p> <p>The illustration shows:</p> <ul> <li>Management groups provide a way to manage access, policies, and compliance across multiple subscriptions. The management groups themselves be have their own heirarchy.</li> <li>Subscriptions are Azure\u2019s way to associate user accounts and resources. You an provide subscriptions with policies, limits or quotas on what users can create and use. Subscriptions are how you manage your costs.</li> <li>Resource groups is a logical container for a set of resources that you manage together. Typically resources within a resource group have the same lifecycle, meaning they are often created or managed as a single unit.</li> <li>Resources are instances of the service, such as storage or a database.In a later post, you will learn how to set up your management group.</li> </ul>"},{"location":"azure/compliance/resource-management/#naming-standards","title":"Naming standards","text":"<p>Naming standards help you quickly identify resources for your scripts and in your billing statements.</p> <p>Resources themselves have different naming characteristics. So your standard may have vary depending on the type of resource. You may want to bookmark this link Naming rules and restrictions for Azure resources and refer back to it often when naming your resources.</p> <p>Many organizations use the Azure naming convention suggested in the Cloud Adoption Framework. It is important to note that many (if not most) of the templates posted follow different naming conventions. That means that you off-the-shelf templates may need to be updated for your own naming convention.</p> <p>Your resource group and resource names are readable. For example, they might look like:</p> <pre><code># sample resource group name\nrg-wu2-prod-azdays-01\n\n# sample virtual network  name\nvnet-wu2-prod-azdays-01\n\n# sample strorage account name\nstwu2prazdays89304\n</code></pre> <p>You can actually tell a lot just by the name of the resource, such as:</p> <ol> <li>resource group: <code>rg-</code></li> <li>region: <code>wu2-</code></li> <li>environment (dev, stag, prod): <code>prod-</code></li> <li>product name: <code>azdays-</code></li> <li>iteration number or something a random number: <code>01</code></li> </ol> <p>Not all resources will support that format. Of note:</p> <ul> <li>Storage accounts must only have small case letters and no hyphens</li> <li>Virtual machines in Windows must only have 15 characters.</li> <li>You may need to use a random number, particularly for storage accounts or log analytics resources that must be unique across all of azure., to make them unique</li> </ul> <p>You can check the length and resource name restrictions.</p> <p>When you use Azure Resource Manager templates, they will often copy the tags from the resource group to the resources themselves. Not all do. So be sure to check. You can learn more about ARM templates in later posts here or at</p>"},{"location":"azure/compliance/resource-management/#tags","title":"Tags","text":"<p>Resource groups and resources themselves should be tagged. Tags help you identify resources for queries. Tags help you with metadata for your resources. It will help answer questions like: who requested the resource, how long should it be available, who to contact to remove it, what application do these resources belong to, and which cost center is responsible for this.</p> <p>Microsoft offers a list of suggested tags. The minimum I suggest is:</p> <ul> <li>Cost Center</li> <li>Application</li> <li>Owner</li> <li>Review date</li> <li>Environment</li> </ul> <p>You can update tags using scripts. See Use tags to organize your Azure resources and management hierarchy for examples on updating tags using PowerShell or Azure CLI.</p> <p>In a later post, you will learn how to require tags in your resources. For now, you can view Enforce resource tagging with Azure Policy.</p>"},{"location":"azure/compliance/resource-management/#use-powershell-scripts-to-create-a-resource-group","title":"Use PowerShell scripts to create a resource group","text":"<p>The following script shows an example how how to create a resource group using the suggested naming convention and tags.</p> <pre><code>$REGION_ABBR = \"wu2\"\n$ENVIRONMENT = \"dev\"\n$PROJECT = \"azdays\"\n$ITERATION = \"01\"\n$LOCATION = \"West US 2\"\n\n$RESOURCE_GROUP_NAME = \"rg-\" + $REGION_ABBR + \"-\" + $ENVIRONMENT + \"-\" + $PROJECT + \"-\" + $ITERATION\n$TAGS = @{ \"Cost Center\" = \"AzDays\"; \"Location\"=$LOCATION }\n\n# new resource group\nNew-AzResourceGroup -Name $RESOURCE_GROUP_NAME -Location $LOCATION -Tags $TAGS\n</code></pre> <p>The following script gets the list of resource groups based on the tag.</p> <pre><code>(Get-AzResourceGroup -Tag @{ \"Cost Center\"=\"AzDays\" }).ResourceGroupName\n</code></pre> <p>Next, we\u2019ll do the same with CLI.</p>"},{"location":"azure/compliance/resource-management/#use-azure-cli-bash-scripts-with-a-resource-group","title":"Use Azure CLI (Bash) scripts with a resource group","text":"<p>The following script shows an example how how to create a resource group using the suggested naming convention and tags.</p> <pre><code>TAG=\"Cost Center=AzDays\"\n\n# az group list --tag supports a single tag only\naz group list --tag \"$TAG\"\n</code></pre> <p>The syntax for tags is a bit quirky, perhaps not for Bash users. But take a quick note of the syntax in the previous code. <code>$LOCATION</code> has a space and requires quotes to pass into the CLI command. And <code>$TAGS</code> is passed in as an array of string. In the example, it also shows no spaces between the tags\u2019 key and value.</p>"},{"location":"azure/compliance/resource-management/#query-resources-using-tags-using-cli","title":"Query resources using tags using CLI","text":"<p>In your Azure CLI scripts, get a list of resource groups or resources by using the the <code>--tags</code> parameter. The following shows how to get a list of resource groups and resources by specifying the tag, however the tag cannot have a space.</p> <pre><code># get resource groups with a tag which does not have spaces\naz group list --tag Environment=Dev -o json\n\n# get a list of resources with a tag without spaces\naz resource list --tag Environment=Dev -o json\n</code></pre> <p>You can then get a list of resource groups based on a single tag.</p> <pre><code>TAG=\"Cost Center=AzDays\"\n\n# az group list --tag supports a single tag only\naz group list --tag \"$TAG\"\n</code></pre>"},{"location":"azure/compliance/resource-management/#set-resource-tags-using-cli","title":"Set resource tags using CLI","text":"<p>The following is a full example of how to copy all of tags from the resource group to all of its resources.</p> <pre><code># adapted from https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/tag-resources#handling-spaces \nRESOURCE_GROUP_NAME='rg-wu2-prod-azdays-01'\n\njsontags=$(az group show --name $RESOURCE_GROUP_NAME --query tags -o json)\ntags=$(echo $jsontags | tr -d '{}\"' | sed 's/: /=/g' | sed \"s/\\\"/'/g\" | sed 's/, /,/g' | sed 's/ *$//g' | sed 's/^ *//g')\norigIFS=$IFS\nIFS=','\nread -a tagarr &lt;&lt;&lt; \"$tags\"\nresourceids=$(az resource list -g $RESOURCE_GROUP_NAME --query [].id --output tsv)\nfor id in $resourceids\ndo\n  az resource tag --tags \"${tagarr[@]}\" --id $id\ndone\nIFS=$origIFS\n</code></pre>"},{"location":"azure/compliance/resource-management/#use-tags-and-naming-convention-with-arm-templates","title":"Use tags and naming convention with ARM templates","text":"<p>In the next few posts, you can learn how to pass tags into deployments scripts. Look for posts that include Boilerplate in the title.</p>"},{"location":"azure/compliance/resource-management/#query-log-analytics-based-on-tags","title":"Query Log Analytics based on tags","text":"<p>Once you get started with Log Analytics, you may want to query resource groups ro resources based on their tags. As of this writing, you will need to use a workaround as the feature in log analytics is not supported. You can upvote the feature at Log Analytics query with tags.</p> <p>You can leverage Azure serverless offerings (including Logic Apps and Functions) to get this data into your Log Analytics workspaces. See Query Azure VM Tags from Log Analytics.</p>"},{"location":"azure/compliance/resource-management/#summary","title":"Summary","text":"<p>In this article you identified three ways to help you organize your resources:</p> <ul> <li>Management groups</li> <li>Naming convention</li> <li>Tags</li> </ul> <p>In posts coming up, learn how to set up your management group.</p>"},{"location":"azure/compliance/resource-management/#resources","title":"Resources","text":"<ul> <li>Organize your Azure resources</li> <li>Recommended naming and tagging conventions</li> <li>Use tags to organize your Azure resources and management hierarchy</li> <li>Everything you need to know about resource tagging in Azure</li> </ul>"},{"location":"azure/compliance/security-center/","title":"Setting up Security Center for production in enterprise","text":"<p>Security Center provides out of the box policies and a dashboard to identify possible security issues with your subscription.</p> <p>To start with Security Center has a good set of policies that will help you do basic audits and provide security alerts.</p>"},{"location":"azure/compliance/security-center/#use-security-center-to-meet-your-cloud-requirements","title":"Use Security Center to meet your cloud requirements","text":"<p>In this article, you will be able to meet the following requirements:</p> <ul> <li>Set up ways for your security team, developers, and operations to quickly audit subscriptions.</li> <li>Mitigate security issues</li> </ul> <p>In a later post, you will learn how to add additional policies, set up a management group as a way to manage policies across multiple subscriptions.</p>"},{"location":"azure/compliance/security-center/#definitions","title":"Definitions","text":"<p>First some definitions.</p> <ul> <li>Management Group. This is a logical container that allow Azure administrators to manage access, policy, and compliance across multiple Azure Subscriptions.</li> <li>Security Center. A unified infrastructure security management system that strengthens the security posture of your data centers. And it provides advanced threat protection across your hybrid workloads in the cloud \u2013 whether they\u2019re in Azure or not.</li> <li>Policy. A feature of Azure that helps you enforce organizational standards and to assess compliance at-scale.</li> <li>Scope. In this article a scope refers to what level you assign permissions. In this article you can assign permissions at the subscription level, the resource group level and to individual resources.</li> <li>Log Analytics. The repository that receives the data from Security Center for alerting and analysis.</li> </ul>"},{"location":"azure/compliance/security-center/#about-security-center","title":"About Security Center","text":"<p>Security Center sets up Azure Policies that provide guardrails for your subscription. Security Center comes with a set of default policies. At a high level, the policies check:</p> <ul> <li>Failure to deploy system updates on virtual machines (VMs).</li> <li>Unnecessary exposure to the Internet through public-facing endpoints.</li> <li>Unencrypted data in transit or storage.</li> </ul> <p>You can customize your security policy to focus your own areas of emphasis. For example:</p> <ul> <li>Add check for web application firewalls</li> <li>Add storage encryption</li> <li>Apply your policy to multiple Azure subscriptions.</li> </ul> <p>And then, it provides a dashboard that gives you visibility into your security posturea at a glance. The following illustration shows a Security Center dashboard.</p> <p></p> <p>Gain visibility across your environment to verify compliance with regulatory requirements, such as CIS, PCI DSS, SOC, and ISO.</p> <p>It also provides for suggestions and in many cases, provides a way to immediately remedy the issue.</p> <p>Use the dashboard to drill into specific requirements. In many cases, Security Center provides a single click solution to remediate the issue. For others, you will need deeper solution. The following illustration shows an example summary of the solutions to make a virtual machine more secure.</p> <p></p> <p>Many of the resources were misconfigured when they were built, often using the defaults in the Azure portal. This means that is it easy for your users and cloud administrators to create resoources that may not be as protected as you want.</p> <p>The default configurations of many resources provided in the portal will not pass a rigorous security audit. As enterprise administrators you will need to provide other ways for your users to create those resources, such as through deployment in Azure DevOps of sets of resources that will pass your audits.</p> <p>This blog will highlight ways to provide your users with compliant resources and ways for you to build an approval process within your organization. But for now, you can look to:</p> <ul> <li>Deployment of compliant ARM templates (or Terraform templates) through Azure DevOps</li> <li>Deployment of resources, policies, and using Azure Blueprints</li> </ul> <p>But first, you will need to set up Security Center Standard.</p>"},{"location":"azure/compliance/security-center/#upgrade-security-center-to-standard","title":"Upgrade Security Center to Standard","text":"<p>Security Center provides a good set of policies and dashboards through the free portal. But for enterprise and for production, you will want the standard version.</p> <p>As explained in the documentation: \u201cThe standard tier also adds threat protection capabilities, which use built-in behavioral analytics and machine learning to identify attacks and zero-day exploits, access and application controls to reduce exposure to network attacks and malware, and more. In addition, standard tier adds vulnerability scanning for your virtual machines. You can try the standard tier for free. Security Center standard supports Azure resources including VMs, Virtual machine scale sets, App Service, SQL servers, and Storage accounts. If you have Azure Security Center standard, you can opt out of support based on resource type.\u201d</p> <p>Review Security Center pricing. When you upgrade to the Standard tier of Azure Security Center, you are automatically enrolled and Security Center by default protects all your resources unless you explicitly decide to opt-out.</p> <p>Some of the most important features provided by Security Center standard, are:</p> <ul> <li>Just in Time VM access to virtual machines. Your admins and users will occasionally need to access a VM inside the virtual network. JIT closes down the RDP and SSH ports and opens them only to the users who are authorized.</li> <li>Adaptive application controls is an intelligent, automated, end-to-end solution from Azure Security Center which helps you white-list which applications can run on your Azure and non-Azure machines (Windows and Linux).</li> <li>Adaptive Network Hardening improves your security posture by hardening the network security group (NSG) rules, based on the actual traffic patterns</li> <li>Security alerts are the notifications that Security Center generates when it detects threats on your resources.</li> <li>Threat protection. You can enable threat protection for Azure Storage accounts at either the subscription level or resource level. You can enable threat protection for Azure SQL Database SQL servers at either the subscription level or resource level. You can enable threat protection for Azure Database for MariaDB/ MySQL/ PostgreSQL at the resource level only.</li> <li>Microsoft Defender ATP</li> </ul> <p>Security Center comes with a set of built in policy definitions that are discussed in a following section in more detail.</p> <p>But first, you should follow the documentation to learn how to upgrade to Security Center Standard in the portal. Or you can automate the process to onboard new subscriptions to use Security Center standard as explained in the next section.</p>"},{"location":"azure/compliance/security-center/#onboard-security-center-using-powershell","title":"Onboard Security Center using PowerShell","text":"<p>In the previous article, you learned how to set up Log Analytics. Log Analytics can act as a central repository for your Security Center data and is a prerequisite for this step.</p> <p>Get the log analytics workspace ID from the PowerShell cmdlet that you used to set up the workspace, or you can get it from the portal.</p> <p>To find your workspace, open the portal and in the search bar at the top of the page, type Log Analytics to see the list of your workspaces as shown in the following illustration.</p> <p></p> <p>Then click on the workspace that you want Security Center to conntect to. The workspace ID is shown in the Overview section.</p> <p></p> <p>Next, use the following cmdlet to set your subscription to use Security Center standard, sets up your security administrator to receive email and phone for security alerts, and attaches the policy to your Log Analytics workspace.</p> <pre><code>#Requires -Version 7.0\n#Requires -Modules PowerShellGet, Az.Resources, Az.Security\n&lt;#\n.SYNOPSIS\n  Sets up Security Center and the admin alerts for the subscription\n.DESCRIPTION\n  Automatically sets Security Center standard tier to the subscription.\n.PARAMETER OrganizationName\n    Used to create the management group name\n.PARAMETER LogAnalyticsWorkplaceId\n    The resource ID for the Log Analytics workplace\n.PARAMETER SecurityAdminEmail  \n    The email for security notifications\n.PARAMETER SecurityAdminPhone\n    The phone number to send security notifications\n.OUTPUTS\n  If the creation was successful, it return the management group name; otherwise, null.\n.NOTES\n  Version:        1.0\n  Author:         Bruce Kyle\n  Creation Date:  6/18/2020\n  Purpose/Change: Initial script development\n  Copyright 2020 Stretegic Datatech LLC\n  License: MIT https://opensource.org/licenses/MIT\n\n.EXAMPLE\n  .\\New-ManagementGroup.ps1 \"Strategic Datatech LLC\"\n.EXAMPLE\n$SubscriptiondID = 9f241d6e-16e2-4b2b-a485-cc546f04799b\n$OrganizationName = \"Strategic Datatech LLC\"\n$SecurityAdminEmail = \"security@strategicdatatech.com\"\n$SecurityAdminPhone = 2065557878\n$workspaceID = @(.\\Add-LogAnalytics.ps1 -SubscriptionID $SubscriptionID -OrganizationName $OrganizationName)\n.\\Set-SubscriptionSecurity.ps1 SubscriptionID $SubscriptionID `\n    -LogAnalyticsWorkplaceId $workspaceID `\n    -SecurityAdminEmail $SecurityAdminEmail `\n    -SecurityAdminPhone $SecurityAdminPhone\n#&gt;\n\n## Run as Admin\n[CmdletBinding()]\nParam(\n    [Parameter(Mandatory)] [string] $SubscriptionID,\n    [Parameter(Mandatory)] [string] $LogAnalyticsWorkplaceId,\n    [Parameter(Mandatory)] [string] $SecurityAdminEmail,\n    [Parameter(Mandatory)] [string] $SecurityAdminPhone\n)\n\nSet-StrictMode -Version Latest\n$ErrorActionPreference = \"Stop\"\n\nSet-AzContext -Subscription $SubscriptionID\nRegister-AzResourceProvider -ProviderNamespace 'Microsoft.Security'\n\nSet-AzSecurityPricing -Name \"default\" -PricingTier \"Standard\"\n\necho \"Subscriptions has workspace id: $workspaceID\"\n\nSet-AzSecurityWorkspaceSetting -Name \"default\" `\n    -Scope \"/subscriptions/$SubscriptionID\" `\n    -WorkspaceId $LogAnalyticsWorkplaceId\n\nSet-AzSecurityAutoProvisioningSetting -Name \"default\" -EnableAutoProvision\n\nSet-AzSecurityContact -Name \"default1\" -Email $SecurityAdminEmail -Phone $SecurityAdminPhone -AlertAdmin -NotifyOnAlert\n\nRegister-AzResourceProvider -ProviderNamespace 'Microsoft.PolicyInsights'\n$Policy = Get-AzPolicySetDefinition | where {$_.Properties.displayName -EQ '[Preview]: Enable Monitoring in Azure Security Center'}\nNew-AzPolicyAssignment -Name 'ASC Default &lt;d07c0080-170c-4c24-861d-9c817742786c&gt;' -DisplayName 'Security Center Default $SubscriptionID ' -PolicySetDefinition $Policy -Scope '/subscriptions/$SubscriptionID '\n</code></pre>"},{"location":"azure/compliance/security-center/#review-the-security-center-built-in-policies","title":"Review the Security Center built-in policies","text":"<p>Security Center comes with a set of built in Policies for Security Center. Most of the policies are instantly recongizable. For example:</p> <ul> <li>System updates should be installed on your machines</li> <li>Role-Based Access Control (RBAC) should be used on Kubernetes Services</li> <li>Security Center standard pricing tier should be selected</li> <li>Subnets should be associated with a Network Security Group</li> <li>There should be more than one owner assigned to your subscription</li> <li>Vulnerabilities in container security configurations should be remediated</li> <li>Vulnerabilities in security configuration on your machines should be remediated</li> <li>External accounts with owner permissions should be removed from your subscription</li> </ul> <p>Many of the policies can be used out of the box. But a few will need to be updated based on your own compliance needs. It is a best practice to use your infrastructure as code practices (checking the changes in policies as code into Azure DevOps and deploying them through DevOps.)</p> <p>For example, as you learned in the previous blog post, Setting up your enterprise Azure subscription administrators, multifactor authentication should be required on all users, except for two emergency owners. Yet the default policy applied by Security Center provides MFA should be enabled on accounts with owner permissions on your subscription. In this rare case, this means you need to create a custom policy in Security Center.</p> <p>Also, you will want to be aware of the following:</p> <p>NOTE: When you deploy new resources, the Security Center dashboard will not update immediately. You will need to wait a few minutes (20 minutes to an hour) to receive reports on the new resources. Also, some resources that have been deleted may continue to impact your score. So review the dashboard information closely to determine the actual impact on your systems.</p>"},{"location":"azure/compliance/security-center/#next-steps","title":"Next steps","text":"<p>In this article, you learned the basics of Security Center, its features, the importance of using Security Center standard in production environments, and how you can connect it to Log Analytics.</p> <p>Next, you will want to dive deper to:</p> <ul> <li>Strengthen security posture</li> <li>Remediate recommendations in Azure Security Center</li> <li>See more about Working with security policies</li> <li>Download and review Security best practices for Azure solutions</li> </ul> <p>If you want to do fine-grained selection of the tiers available for each of the service, you can adapt an Azure ARM template to deploy and update Security Center tiers.</p>"},{"location":"azure/compliance/security-center/#references","title":"References","text":"<ul> <li>Security Center pricing</li> <li>Azure Security Center documentation</li> </ul>"},{"location":"azure/compliance/setup-management-group/","title":"Setting up Management Group for production in enterprise","text":"<p>Once you have set up your first subscription, you can set up your Management Group.</p> <p>In Azure, management groups are a way to group your subscriptions. When you apply policies and governance to your management group, all of the subscriptions within a management group automatically inherit the conditions applied. Enterprises want management groups as a way to scale your operations no matter how many subscriptions you may have.</p> <p>For example, you may want to restrict the regions available for your resources to those within a particular region. A policy that reflects that can be applied to a management group and will automatically be applied to all management groups, all subscriptions, and all resources under that management group.</p> <p>IMPORTANT: All subscriptions within a single management group must trust the same Azure Active Directory tenant.</p>"},{"location":"azure/compliance/setup-management-group/#managing-policies-and-role-base-access-controls-at-scale","title":"Managing Policies and Role Base Access Controls at scale","text":"<p>As an enterprise admin, you will need to manage multiple subscriptions. And you will want to be able to show how each subscription meets the goals of your compliance. There are two tools that help you manage these in Azure.</p> <ul> <li>Azure Policy helps to enforce organizational standards and to assess compliance at-scale. Use Policies to impplement governance for resource consistency, regulatory compliance, security, cost, and management.</li> <li>Azure role-based access control is an authorization system built on Azure Resource Manager that provides fine-grained access management of Azure resources.</li> </ul> <p>Each of these can be deployed in management group and automatically inherited by the subscriptions and resources.</p> <p>In addition, you can apply resource templates to management groups.</p>"},{"location":"azure/compliance/setup-management-group/#architecture-considerations","title":"Architecture considerations","text":"<p>Each Azure Active Directory directory is given a single top-level management group called the \u201cRoot\u201d management group. This root management group is built into the hierarchy to have all management groups and subscriptions fold up to it.</p> <p>You can create a heirarchy that you can apply a policy, for example the regions in available for resources. This policy will inherit onto all the subscriptions that are descendants of that management group and will apply to all VMs under those subscriptions.</p> <p>You can slso use management groups is to provide user access to multiple subscriptions. By moving multiple subscriptions under that management group, you can create one role-based access control (RBAC) assignment on the management group.</p> <p>The following illustration shows a simple heirarchy with policies applied to the Root Management Group and others.</p> <p></p>"},{"location":"azure/compliance/setup-management-group/#create-a-management-group","title":"Create a management group","text":"<p>The Azure AD Global Administrator needs to elevate themselves to the User Access Administrator role of this root group initially. After elevating access, the administrator can assign any RBAC role to other directory users or groups to manage the hierarchy. As administrator, you can assign your own account as owner of the root management group.</p> <p>The documentation shows how you can set up a Management Group using the portal, PowerShell, and the Azure CLI.</p>"},{"location":"azure/compliance/setup-management-group/#sample-powershell-cmdlet-to-set-up-your-azure-management-groups","title":"Sample PowerShell cmdlet to set up your Azure management groups","text":"<p>You may want to use scripts so you can support subscriptions and resources using your management groups at scale that support a consistent naming convention. The following code creates a root management group.</p> <pre><code>#Requires -Version 7.0\n#Requires -Modules PowerShellGet, Az.Resources\n&lt;#\n.SYNOPSIS\n  Creates a management group\n.DESCRIPTION\n  Creates a management group\n.PARAMETER OrganizationName\n  Used to create the management group name\n.OUTPUTS\n  If the creation was successful, it return the management group name; otherwise, null.\n.NOTES\n  Version:        1.0\n  Author:         Bruce Kyle\n  Creation Date:  6/25/2020\n  Purpose/Change: Initial script development\n  Copyright 2020 Stretegic Datatech LLC\n  License: MIT https://opensource.org/licenses/MIT\n\n.EXAMPLE\n  .\\New-ManagementGroup.ps1 -OrganizationName \"Strategic Datatech LLC\"\n#&gt;\n\n[CmdletBinding()]\nParam(\n    [Parameter(Mandatory)] [string] $OrganizationName\n)\n\nSet-StrictMode -Version Latest\n$ErrorActionPreference = \"Stop\"\n\ntry {\n    $ManagementGroupName = $OrganizationName -replace '\\s',''\n    $ManagementGroupName = $ManagementGroupName'Root'\n    Write-Host \"Creating management group for $OrganizationName\"\n    New-AzManagementGroup -GroupName $ManagementGroupName -DisplayName \"$OrganizationName Root Group\"\n}\ncatch\n{\n    Write-Host \"An error occurred creating management group:\"\n    Write-Host $_\n    $ManagementGroupName = null;\n}\n\nreturn $ManagementGroupName\n</code></pre> <p>Next, use the root management group to create a new sub group for developers. The following code shows how you can create a subgroup.</p> <pre><code>#Requires -Version 7.0\n#Requires -Modules PowerShellGet, Az.Resources\n&lt;#\n.SYNOPSIS\n  Creates a management group\n.DESCRIPTION\n  Creates a management group\n.PARAMETER OrganizationName\n  Used to create the management group name\n.OUTPUTS\n  If the creation was successful, it return the management group name; otherwise, null.\n.NOTES\n  Version:        1.0\n  Author:         Bruce Kyle\n  Creation Date:  6/25/2020\n  Purpose/Change: Initial script development\n  Copyright 2020 Stretegic Datatech LLC\n  License: MIT https://opensource.org/licenses/MIT\n\n.EXAMPLE\n  $OrganizationName = \"Strategic Datatech LLC\"\n  $TeamName = \"Dev\"\n  $ParentManagementGroupName = @(.\\New-ManagementGroup.ps1 -OrganizationName $OrganizationName)\n  .\\New-ManagementSubGroup.ps1 -ParentManagementGroup $ParentManagementGroupName `\n      -OrganizationName $OrganizationName -TeamName $TeamName\n#&gt;\n\n[CmdletBinding()]\nParam(\n    [Parameter(Mandatory)] [string] $ParentManagementGroup,\n    [Parameter(Mandatory)] [string] $OrganizationName,\n    [Parameter(Mandatory)] [string] $TeamName\n)\n\nSet-StrictMode -Version Latest\n$ErrorActionPreference = \"Stop\"\n\ntry {\n    $OrganizationName = $OrganizationName -replace '\\s',''\n    $TeamName = $TeamName -replace '\\s',''\n\n    $parentGroup = Get-AzManagementGroup -GroupName $ParentGroupName\n    $groupName = ({0}{1} -f  $OrganizationName,$TeamName)\n    Write-Host \"Creating management group $groupName \"\n    New-AzManagementGroup -GroupName $groupName -DisplayName \"$TeamName Group\" `\n        -ParentId $parentGroup.id\n}\ncatch\n{\n    Write-Host \"An error occurred creating management group:\"\n    Write-Host $_\n    $ManagementGroupName = null;\n}\n\nreturn $ManagementGroupName\n</code></pre> <p>Finally, use the following code to set the subscription to your subgroup.</p> <pre><code>#Requires -Version 7.0\n#Requires -Modules PowerShellGet, Az.Resources\n&lt;#\n.SYNOPSIS\n    Sets the subscription to a management group \n.DESCRIPTION\n    Sets the subscription to a management group \n.PARAMETER ManagementGroupName\n    Used to create the management group name\n.PARAMETER SubscriptionID\n    The subscription id\n.NOTES\n  Version:        1.0\n  Author:         Bruce Kyle\n  Creation Date:  6/25/2020\n  Purpose/Change: Initial script development\n  Copyright 2020 Stretegic Datatech LLC\n  License: MIT https://opensource.org/licenses/MIT\n\n.EXAMPLE\n  .\\Set-ManagementGroup.ps1 -ManagementGroupName $ManagementGroupName `\n    -SubscriptionId 9f241d6e-16e2-4b2b-a485-cc546f04799b\n#&gt;\n\n[CmdletBinding()]\nParam(\n    [Parameter(Mandatory)] [string] $ManagementGroupName,\n    [Parameter(Mandatory=$false)] [string] $SubscriptionId\n)\n\nSet-StrictMode -Version Latest\n$ErrorActionPreference = \"Stop\"\n\ntry {\n    if ($SubscriptionId -eq $null) {\n        $SubscriptionId = (Get-AzContext).Subscription.SubscriptionId\n    }\n    Write-Host \"Assigning subscription '$SubscriptionId' to management group '$ManagementGroupName'\"\n    New-AzManagementGroupSubscription -GroupName $ManagementGroupName -SubscriptionId $SubscriptionId\n}\ncatch\n{\n    Write-Host \"An error occurred assigning management group:\"\n    Write-Host $_\n    $OrganizationName = null;\n}\n</code></pre> <p>You have now created a management group, a sub-management group and and assigned the subscription to the sub-management group.</p> <p>In our next post, you will learn how to create policies that you want to apply to all the subscriptions in your management group using Azure Blueprints.</p>"},{"location":"azure/compliance/setup-management-group/#next-steps","title":"Next steps","text":"<p>You will want to learn more about:</p> <ul> <li>Azure Policy built-in policy that you can apply to your management group</li> <li>Azure built-in roles that provide a good starting point to assign to users, groups, service principals, and managed identities.</li> <li>Azure Blueprints that provide sets of pre-built policies, role based access controls, and Rsource Manager templates that you can apply to a management group to support specific compliance initiatives, such as ISO, PCI, NIST and many others.</li> </ul>"},{"location":"azure/compliance/setup-management-group/#references","title":"References","text":"<ul> <li>Organize your resources with Azure management groups</li> </ul>"},{"location":"azure/compliance/setup-subscription/","title":"Setting up your enterprise Azure subscription administrators","text":"<p>Microsoft makes it easy to get started using Azure \u2014 sign up for a free subscription and get started. The tutorial show you how to use the portal to create virtual machines, storage, backups. All good.</p> <p>And then it comes time to take your applications into production. You may realize that you need to show auditors your security methods. And you want to be sure to protect your customer data. Or you may have cloud sprawl and want to control costs.</p> <p>And you have had a good conversations about your requirements. What then?</p> <p>This article shows you how to get your subscription up and running using some important best practices for your administrators. It shows how to set up Security Center and how to set up policies that can be used to help your security team validate that you are using best practices.</p> <p>This step is part Ready Phase of the Cloud Adoption Framework.</p>"},{"location":"azure/compliance/setup-subscription/#definitions","title":"Definitions","text":"<p>Let\u2019s start with some definitions and some links on where to learn more about each.</p> <ul> <li>Tenant. A tenant\u00a0represents an organization in Azure Active Directory. It\u2019s a dedicated Azure AD service instance that an organization receives and owns when it signs up for a Microsoft cloud service such as Azure, Microsoft Intune, or Microsoft 365. Each Azure AD tenant is distinct and separate from other Azure AD tenants.</li> <li>Subscription. This is essentially a way to manage and bill a set of resources. Permissions are granted at the subscription, resource group and resource levels. Billing is provided as part of signing up for a subscription.</li> <li>Resource Groups. A group of resources. A resource is like a storage account. A virtual machine is actually several resources, including the compute and its virtual network.</li> <li>Policy. A feature of Azure that helps you enforce organizational standards and to assess compliance at-scale.</li> <li>Scope. In this artifcle a scope refers to what level you assign permissions. In this article you can assign permissions at the subscription level, the resource group level and to individual resources.</li> <li>Multi-factor Authentication (MFA). MFA can be implemented in a cloud-only setting, which is what is described in this article. But MFA can also be set up with hybrid identities using Azure AD Connect, Azure MFA with RADIUS Authentication.</li> <li>Administrators. In this article, the admins refer to the subscription owners who can see everything, create just about anything and add and remove users. There are two kinds of administrators discussed. The first are e_mergency admins,_ sometimes known as break-glass admins, who can log in when MFA fails or some other emergency. The other is the global admin or subscription owner who is required to log in using MFA.</li> </ul> <p>A tenant can have many subscriptions and management groups. A subscription is tied to a single management group. Resources, such as virtual machines and storage accounts, are grouped together in a resource group. A subscription can have many management groups.</p>"},{"location":"azure/compliance/setup-subscription/#requirements","title":"Requirements","text":"<p>In this article, you will be able to meet the following requirements:</p> <ul> <li>An Active Directory tenant, that can register and manage apps, have access to Microsoft 365 data, and deploy custom Conditional Access and tenant restrictions.</li> <li>Set up emergency administrators, to provide access to the subscription in break-glass scenarios, such as domain name lost, data center fails, Active Directory no longer syncs, product hosted in subscription sold to another company. These users do not have multifactor authentication.</li> <li>Define users and admin roles suitable in your organization.</li> <li>Set up multifactor authentication for developers, security admins, operations admins, managers \u2014 anyone who has access to the subscription itself.</li> </ul> <p>Let\u2019s begin by choosing your Azure Active Directory tenant.</p>"},{"location":"azure/compliance/setup-subscription/#choose-your-tenant","title":"Choose your tenant","text":"<p>This section applies if you are managing your own subscription. If you are using a Cloud Solution Provider for your subsription, the CSP will control your tenant and has added your users to administer the subscription.</p> <p>Start at the tenant in Azure Active Directory. Each subscription is tied to a single tenant. And the admins of that tenant control who has access to add/remove users and groups, who can create resources, and the control each user has.</p> <p>If you have Microsoft 365, you already have a tenant.</p> <p></p> <p>(Illustration from Sign up for an Azure subscription with your Office 365 account)</p> <p>Your tenant is important.\u00a0The documents describes how tightly a tenant is tied to the subscription:</p> <p>An Azure subscription has a trust relationship with Azure Active Directory (Azure AD). A subscription trusts Azure AD to authenticate users, services, and devices.</p> <p>Multiple subscriptions can trust the same Azure AD directory. Each subscription can only trust a single directory.</p> <p>If your subscription expires, you lose access to all the other resources associated with the subscription. However, the Azure AD directory remains in Azure. You can associate and manage the directory using a different Azure subscription.</p> <p>All of your users have a single home directory for authentication. Your users can also be guests in other directories. You can see both the home and guest directories for each user in Azure AD.</p> <p>(See Associate or add an Azure subscription to your Azure Active Directory tenant)</p> <p>If you start out with a subscription tied to a tenant and then change your mind about who the owners of the subscription should be (in other words, set the subscription to a new tenant) here\u2019s a list of what breaks:</p> <ul> <li>Users that have been assigned roles using RBAC will lose their access</li> <li>Service Administrator and Co-Administrators will lose access</li> <li>If you have any key vaults, they\u2019ll be inaccessible and you\u2019ll have to fix them after association</li> <li>If you have any managed identities for resources such as Virtual Machines or Logic Apps, you must re-enable or recreate them after the association</li> <li>If you have a registered Azure Stack, you\u2019ll have to re-register it after association</li> <li>In particular, moving your Azure Kubernetes Service (AKS) cluster to a different subscription, or moving the cluster-owning subscription to a new tenant, causes the cluster to lose functionality due to lost role assignments and service principal\u2019s rights.</li> </ul> <p>So the first steps are to create a new subscription:</p> <p>Step 1. Create a subscription using one of these techniques:</p> <ul> <li>Create your first subscription.</li> <li>You can get one through a Cloud Solution Provider, who provisions your subscription.</li> <li>See Create an additional Azure subscription if you have an Enterprise Agreement (EA), Microsoft Customer Agreement (MCA), Microsoft Partner Agreement (MPA). You can even set up your subscription using scripts.</li> </ul> <p>Step 2. Associate that subscription with your tenant.</p> <p>The admin of the new tenant will control who the subscription owners and admins are, who can add and remove users and groups, who can associate another tenant, who must use multifactor authentication and who the emergency admins are. And who can associate the subscription to a management group.</p>"},{"location":"azure/compliance/setup-subscription/#set-up-your-admins","title":"Set up your admins","text":"<p>At the heart of the tenant are the emergency admins. These credentials are the ones you lock in a safe somewhere. They are the only ones who have access to the subscription without multifactor authentication.</p> <p>Here\u2019s the steps to set up your admins. You will create three accounts: two without multi-factor authentication and one with MFA.</p> <ol> <li>Create\u00a0two user accounts perhaps named something like emergencyadministrator1@yourtenantname.onmicrosoft.com\u00a0and\u00a0emergencyadministrator2@yourtenantname.onmicrosoft.com. These users are your\u00a0emergency access account admins. Use a password that is unreasonably long and complex, such as two Guid. These two accounts will NOT require multifactor authentication and you will not be using them to log in once you have set up the subscription owner.</li> <li>Log in using emergencyadministrator1@yourtenantname.onmicrosoft.com</li> <li>Create a new user account and assign that user to the Global owner role.</li> <li>Log out of emergencyadministrator1.</li> <li>From the new global owner account, sign up for Azure AD Priveleged Identity Management account for Azure AD Premium P2 and assign the global owner to Priveleged Identity Management.\u00a0Assign the global owner actual user account to Azure AD Premium.</li> <li>Take the emergency admin user names and passwords and put them in a physical safe somewhere, hopefully never to be used again.</li> <li>Create\u00a0a group\u00a0of users named\u00a0Subscription Owners. Add the emergency users to the group and grant the group Owner permissions to the subscription. In a following step, you will add\u00a0Require all users to have MFA except for the emergency owners.</li> </ol>"},{"location":"azure/compliance/setup-subscription/#some-notes-on-setting-up-the-admin","title":"Some notes on setting up the admin","text":"<p>There will be an additional cost per user for the Azure AD Priveleged Identity Management. Can be purchased based on how many users have Priveleged Identity Management. For more information, see Critical items to do right now.</p> <p>It is a best practice for a password for an emergency access account to be separated into two or three parts, written on separate pieces of paper, and stored in secure, fireproof safes that are in secure, separate locations.</p> <p>Note that you will want to set up to monitor sign-in and audit logs of the emergency accounts once you have set up log analytics. For more details, see Monitor sign-in and audit logs.</p> <p>Only Global administrators and Privileged Role administrators can delegate administrator roles. To reduce risk to your business, assign this role to the fewest possible people in your organization. You should have three Global administrators.</p> <p>Your two break-glass admins user names and passwords should be locked in a safe. And one who is assigned Azure AD Premium P2. This aligns with the policy, A maximum of 3 owners should be designated for your subscription.</p> <p>The person who signs up for the Azure AD organization becomes a global administrator. There can be more than one global administrator at your company. Global admins can reset the password for any user and all other administrators.</p>"},{"location":"azure/compliance/setup-subscription/#set-up-multi-factor-authentication-policy","title":"Set up Multi-factor Authentication Policy","text":"<p>Next the global administrator owenr should set up the multi-factor authentication policy and assign additional subscription owners or assign other admin roles that are described in a following section.</p> <p>Create a Conditional Access policy for MFA. Include all users, except for the two emergency admins. See Create a Conditional Access Policy in the Azure Active Directory documentation for details.</p> <p>Follow the steps in the documentation for setting up conditional access in MFA. As you set up the policy, take special note to:</p> <ol> <li>Include all users</li> <li>Exclude your organization\u2019s emergency access or break-glass accounts.</li> <li>Under Access controls grant, require multi-factor authentication.</li> </ol> <p>Now that everyone must use MFA (except for your break-glass admins), from now on you will want to grant least permissions for your adminstrators.</p>"},{"location":"azure/compliance/setup-subscription/#set-additional-administrators-with-limited-permissions","title":"Set additional administrators with limited permissions","text":"<p>Rather than give your admins owner access to the subacription, you can limit their roles.</p> <p>As best practice, you will want to limit the administrator powers to access data and apps. See Administrator role permissions in Azure Active Directory.</p> <p>Azure provides many out-of-the-box administrator roles. You can assign individual to built in administrator roles that may apply to Azure such as:</p> <ul> <li>Application Administrator</li> <li>Application Developer</li> <li>Authentication Administrator</li> <li>Azure DevOps Administrator</li> <li>Azure Information Protection Administrator</li> <li>B2C IEF Keyset Administrator</li> <li>B2C IEF Policy Administrator</li> <li>Billing Administrator</li> <li>Cloud Application Administrator</li> <li>Cloud Device Administrator</li> <li>Compliance Administrator</li> <li>Compliance Data Administrator</li> <li>Conditional Access Administrator</li> <li>Customer Lockbox access approver</li> <li>Device Administrator</li> <li>Directory Readers</li> <li>External Id User Flow Administrator</li> <li>External Id User Flow Attribute Administrator</li> <li>External Identity Provider Administrator</li> <li>Global Administrator / Company Administrator</li> <li>Global Reader</li> <li>Groups Administrator</li> <li>Guest Inviter</li> <li>Helpdesk Administrator</li> <li>Hybrid Identity Administrator</li> <li>Message Center Privacy Reader</li> <li>Message Center Reader</li> <li>Modern Commerce Administrator</li> <li>Network Administrator</li> <li>Office Apps Administrator</li> <li>Password Administrator</li> <li>Power BI Administrator</li> <li>Power Platform Administrator</li> <li>Privileged Authentication Administrator</li> <li>Privileged Role Administrator</li> <li>Reports Reader</li> <li>Search Administrator</li> <li>Search Editor</li> <li>Security Administrator</li> <li>Security operator</li> <li>Security Reader</li> <li>Service Support Administrator</li> <li>User Administrator</li> </ul> <p>Many of these roles access to view, create, or manage support tickets.</p> <p>When you assign a role, it is a best practice to set up a group and assign the group to a scope, such as the subscription, resource group, or individual resources. That way, when a person can be added and removed from the group to set of resources is not changed.</p> <p>Depending on how your enterprise is organizes, you might have groups similar to these:</p> <ul> <li>Global administrators</li> <li>Operations admistrators</li> <li>Security administrators</li> <li>Network administrators</li> <li>Developers</li> <li>Developer leads</li> <li>Production developers</li> </ul> <p>You can assign a user to a group and a group to a scope. Users have access to resources based on their group.</p> <p>For example, you might manage a group by setting up a Network Admin Group that has a set of users. And you might grant Network Administrator to a resource group that includes virtual networks.</p> <p>Now let\u2019s set up Log Analytics and Security Center, as you learn in our next post.</p>"},{"location":"azure/compliance/setup-subscription/#conclusion","title":"Conclusion","text":"<p>In this article, you learned how to:</p> <ol> <li>Create or add an Azure subscription</li> <li>Understand how user identities are related to Microsoft 365</li> <li>Set up your owners, including your emergency (break-glass) subscription owners</li> <li>Set up multi-factor authentication for all the subscription users</li> </ol>"},{"location":"azure/compliance/setup-subscription/#references","title":"References","text":"<ul> <li>Manage emergency access accounts in Azure AD</li> <li>Administrator role permissions in Azure Active Directory</li> <li>Plan an Azure Multi-Factor Authentication deployment</li> <li>Conditional Access: Require MFA for all users</li> <li>What is Azure Security Center?</li> </ul>"},{"location":"deploy/","title":"Overview","text":""},{"location":"deploy/#deploy-code-and-models","title":"Deploy code and models","text":"<p>In this section, learn how to deploy code and models into production environments that respond to changing compliance requirements and support best practices.</p>"},{"location":"deploy/#compliance-foundations","title":"Compliance foundations","text":""},{"location":"deploy/#set-up-infrastructure-for-compliance","title":"Set up infrastructure for compliance:","text":"<ul> <li>Governance</li> <li>Landing Zone</li> <li>Setup Subscription</li> <li>Setup Management Group</li> </ul>"},{"location":"deploy/#manage-compliance","title":"Manage compliance","text":"<ul> <li>Azure Policy</li> <li>Regulatory Compliance</li> <li>Security Center</li> </ul>"},{"location":"deploy/#day-2-operations","title":"Day 2 Operations","text":"<ul> <li>Log Analytics for Compliance</li> </ul>"},{"location":"deploy/#deploy-containers","title":"Deploy containers","text":"<ul> <li>Create local registry</li> <li>Deploy source code to image without a Dockerfile</li> <li>Deploy the application using a Docker image in a local registry</li> </ul>"},{"location":"deploy/#deploy-container-to-azure","title":"Deploy container to Azure","text":"<ul> <li>Create Azure container registry (CLI) [Azure]</li> <li>Deploy container to App Services [Azure]</li> <li>Setup AKS using ARM Templates</li> <li>Setup AKS using Terraform</li> </ul>"},{"location":"deploy/#deploy-container-to-openshift","title":"Deploy container to OpenShift","text":"<ul> <li>Deploy the application using a GitHub repo with sources and a Dockerfile to OpenShift</li> </ul>"},{"location":"deploy/#deploy-models","title":"Deploy models","text":"<ul> <li>Deploying Machine Learning Models [KD Nuggets]</li> </ul>"},{"location":"deploy/deploycontaineropenshift/","title":"Deploy the application using a GitHub repo with sources and a Dockerfile to OpenShift","text":"<p>This tutorial shows you how to deploy a Python application into an OpenShift cluster on the cloud. You can also use the methodology described here to deploy applications or microservices developed on other runtime environments. </p>"},{"location":"deploy/deploycontaineropenshift/#reference","title":"Reference","text":"<ul> <li>Deploy the application using a GitHub repo with sources and a Dockerfile</li> </ul>"},{"location":"deploy/deploycontainerwithoutDockerfile/","title":"Deploy source code to image without a Dockerfile","text":"<p>Source-to-Image (S2I) is a toolkit and workflow for building reproducible container images from source code. S2I produces ready-to-run images by injecting source code into a container image and letting the container prepare that source code for execution. By creating self-assembling builder images, you can version and control your build environments exactly like you use container images to version your runtime environments.</p>"},{"location":"deploy/deploycontainerwithoutDockerfile/#source-to-image-build-process-overview","title":"Source-to-image build process overview","text":"<p>Source-to-image (S2I) produces ready-to-run images by injecting source code into a container that prepares that source code to be run. It performs the following steps:</p> <ol> <li>Runs the <code>FROM &lt;builder image&gt;</code> command</li> <li>Copies the source code to a defined location in the builder image</li> <li>Runs the assemble script in the builder image</li> <li>Sets the run script in the builder image as the default command</li> </ol> <p>Buildah then creates the container image.</p>"},{"location":"deploy/deploycontainerwithoutDockerfile/#prerequisites","title":"Prerequisites","text":"<ul> <li>Podman installed</li> </ul>"},{"location":"deploy/deploycontainerwithoutDockerfile/#installation","title":"Installation","text":"<p>Download the latest release.</p> <p>Choose either the linux-386 or the linux-amd64 links for 32 and 64-bit, respectively.</p> <p>Unpack the downloaded tar with</p> <pre><code>tar -xvzf release.tar.gz.\n</code></pre> <p>You should now see an executable called s2i. Either add the location of s2i to your PATH environment variable, or move it to a pre-existing directory in your PATH. For example,</p> <pre><code>cp /path/to/s2i /usr/local/bin\n</code></pre> <p>will work with most setups.</p>"},{"location":"deploy/deploycontainerwithoutDockerfile/#to-test","title":"To test","text":"<p>To try it out, use the following command.</p> <pre><code>s2i build https://github.com/sclorg/django-ex centos/python-35-centos7 hello-python\npodman run -p 8080:8080 hello-python\n</code></pre> <p>Now browse to http://localhost:8080 to see the running application.</p>"},{"location":"deploy/deploycontainerwithoutDockerfile/#inside-openshift","title":"Inside OpenShift","text":"<p>S2I images are available for you to use directly from the OpenShift Container Platform web console by following procedure:</p> <ol> <li>Log in to the OpenShift Container Platform web console using your login credentials. The default view for the OpenShift Container Platform web console is the Administrator perspective. 2, Use the perspective switcher to switch to the Developer perspective.</li> <li>In the +Add view, use the Project drop-down list to select an existing project or create a new project. 4.Click All services in the Developer Catalog tile.</li> <li>Click Builder Images under Type to see the available S2I images.</li> </ol> <p>S2I images are also available though the Cluster Samples Operator.</p>"},{"location":"deploy/deploycontainerwithoutDockerfile/#for-more-information","title":"For more information","text":"<p>See:</p> <ul> <li>Creating images from source code with source-to-image</li> </ul>"},{"location":"deploy/deploycontainerwithoutDockerfile/#reference","title":"Reference","text":"<ul> <li>Source-to-Image (S2I)</li> <li>Red Hat documentation Source-to-image</li> </ul>"},{"location":"deploy/deploytolocalregistry/","title":"Deploy application to local registry on OpenShift Local","text":""},{"location":"deploy/deploytolocalregistry/#references","title":"References","text":"<ul> <li>Deploy the application using a Docker image in a local registry</li> </ul>"},{"location":"genai/chatwithdb/","title":"Chat with your data","text":"<p>In this article, learn how to interact with your own data using Large Language Models.</p>"},{"location":"genai/chatwithdb/#definitions","title":"Definitions","text":""},{"location":"genai/chatwithdb/#references","title":"References","text":"<ul> <li>Creating Custom ChatGPT with Your Own Dataset using OpenAI GPT-3.5 Model, LlamaIndex, and LangChain</li> </ul>"},{"location":"kubernetes/podman/","title":"Getting started with Podman","text":"<p>Podman is a container engine that's compatible with the OCI Containers specification. Podman is part of RedHat Linux, but can also be installed on other distributions, including Windows and MacOS.</p> <p>The biggest argument for using Podman over Docker is am eye toward security. Two of Podman's primary features will attract you. </p> <ul> <li>Podman does not run as a daemon, meaning it doesn't rely on a process with root privileges to run containers.</li> <li>Podman can run containers without root access. It means that you do not need superuser privileges to mange containers.</li> </ul>"},{"location":"kubernetes/podman/#podman-key-features","title":"Podman key features","text":"<p>Docker has been the go-to containerization tool for years, but Podman is emerging as a strong alternative. Podman offers a few advantages:</p> <ul> <li>Rootless Containers. Run containers without needing root privileges, enhancing security. Learn more about rootless containers.</li> <li>Systemd Integration. Better integration with Linux's init system, systemd. For Linux users, this is a significant benefit. Podman's compatibility with systemd offers better process management and orchestration. Learn more about systemd.</li> <li>Follows Open Standards. Fully compatible with Open Container Initiative (OCI) standards. Podman is OCI-compliant, which means it adheres to industry standards for container images, making it easier to switch between different container technologies. Learn more about OCI.</li> <li>Deploy to Kubernetes. Deploy pods from Podman Desktop to local or remote Kubernetes contexts using automatically-generated YAML config.</li> <li>Podman is compatible with Docker's command line interface. Meaning, moving from Docker to Podman will not require any major changes to your existing code. This also means that you can just substitute the docker command with podman and it just works.</li> <li>Podman is daemon-less. Docker's core runs as a daemon (dockerd). Meaning, it is always running in the background, managing the containers. Meanwhile, Podman is like your average program; once you perform an action (start/stop a container) using Podman, it exits.</li> </ul>"},{"location":"kubernetes/podman/#prerequisites","title":"Prerequisites","text":"<p>Since Podman uses WSL, you need a recent release of Windows 10 or Windows 11. On x64, WSL requires build 18362 or later, and 19041 or later is required for arm64 systems. Internally, WSL uses virtualization, so your system must support and have hardware virtualization enabled. If you are running Windows on a VM, you must have a VM that supports nested virtualization.</p> <p>Recommended:</p> <ul> <li>Windows Terminal</li> </ul>"},{"location":"kubernetes/podman/#install-on-windows","title":"Install on Windows","text":"<p>On Windows, each Podman machine is backed by a virtualized Windows Subsystem for Linux (WSLv2) distribution. </p> <p>To install Podman for Windows, download and install Podman from the official site.</p> <p>To check your installation:</p> <pre><code>podman version\n</code></pre> <p>For information on how to use Podman on a remote client, see Podman Remote clients for macOS and Windows.</p>"},{"location":"kubernetes/podman/#initialize-and-start-podman","title":"Initialize and start Podman","text":"<p>Before using Podman, you need to initialize and start a Podman machine:</p> <pre><code>podman machine init\npodman machine start\n</code></pre>"},{"location":"kubernetes/podman/#run-something","title":"Run something","text":"<pre><code>podman run ubi8-micro date\n</code></pre>"},{"location":"kubernetes/podman/#podman-has-pods","title":"Podman has Pods","text":"<p>Podman comes with unique features that Docker lacks entirely. In Podman, containers can form \"pods\" that operate together. It's similar to the Kubernetes Pod concept.</p> <p>To create a Pod, use the pod create command:</p> <pre><code>podman pod create --name my-pod\n</code></pre> <p>Add containers to Pods, by including the <code>--pod`` folg with</code>podman run`.</p> <pre><code>podman run --pod my-pod --name image-1 my-image:latest\n\npodman run --pod my-pod --name image-2 another-image:latest\n</code></pre> <p>Containers in the Pod can be managed in aggregate by using podman pod commands:</p> <pre><code>podman kill my-pod # Kill all containers\n\npodman restart my-pod # Restart all containers\n\npodman stop my-pod # Stop all containers\n</code></pre> <p>The Pod concept is powerful, as it lets you manage multiple containers in aggregate. You can create app containers, such as a frontend, a backend, and a database, add them to a Pod, and manage them in unison.</p> <p>The closest Docker gets to this is with Compose. Using <code>Compose</code> requires you to write a <code>docker-compose.yml</code> file and use the separate docker-compose binary. Podman lets you create Pods using one command without leaving the terminal.</p> <p>When you need to export a Pod's definition, Podman will produce a Kubernetes-compatible YAML manifest. You can take the manifest and apply it directly to a Kubernetes cluster. This narrows the gap between running a container in development and launching it onto production infrastructure.</p> <pre><code>podman generate kube\n</code></pre>"},{"location":"kubernetes/podman/#rootless-containers","title":"Rootless containers","text":"<p>Podman supports rootless containers. This helps you lock down your security by preventing containers from running as the host's root user. Docker now supports rootless mode as a daemon configuration option. </p> <p>Rootless containers are containers that can be created, run, and managed by users without admin rights. Rootless containers have several advantages:</p> <p>They add a new security layer; even if the container engine, runtime, or orchestrator is compromised, the attacker won't gain root privileges on the host. They allow multiple unprivileged users to run containers on the same machine (this is especially advantageous in high-performance computing environments).</p> <p>They allow for isolation inside of nested containers. To better understand these advantages, consider traditional resource management and scheduling systems. This type of system should be run by unprivileged users. From a security perspective, fewer privileges are better. With rootless containers, you can run a containerized process as any other process without needing to escalate any user's privileges. There is no daemon; Podman creates a child process.</p> <p>For more information, see Rootless Tutorial</p>"},{"location":"kubernetes/podman/#podman-buildah-and-skopeo","title":"Podman, Buildah, and Skopeo","text":"<p>Podman is a modular container engine, so it must work alongside tools like Buildah and Skopeo to build and move its containers. </p> <p>With Buildah, you can build containers either from scratch or by using an image as a starting point. Skopeo moves container images between different types of storage systems, allowing you to copy images between registries like docker.io, quay.io, and your internal registry or between different types of storage on your local system. </p>"},{"location":"kubernetes/podman/#next-steps","title":"Next steps","text":"<p>Get the Podman Basics Cheat Sheet</p>"},{"location":"kubernetes/podman/#references","title":"References","text":"<ul> <li>From Docker to Podman - VS Code DevContainers</li> <li>Making Visual Studio Code devcontainer work properly on rootless Podman</li> <li>Run Podman on Windows: How-to instructions</li> <li>What Is Podman and How Does It Differ from Docker?</li> <li>What is Podman?</li> </ul>"},{"location":"ocp/advancedclustermanagement/","title":"GitOps for Advanced Cluster Management (ACM)","text":"<p>One Touch Provisioning across Multi-Cloud makes the deployment and maintenance of a multi-cloud environment beginning with the hitting of one big red button (figuratively). This starts provisioning a platform that provides:</p> <ul> <li>Cluster and Virtual Machine Provisioning capabilities</li> <li>Governance and policy management</li> <li>Observability of Clusters and workload</li> <li>Deployment of applications, such as IBM Cloud Paks, all within a single command.</li> </ul> <p>The following illustration shows how the parts of the project work together.</p> <p></p> <p>This method/pattern is an opinionated implementation of the GitOps principles, using the latest and greatest tooling available.</p>"},{"location":"ocp/advancedclustermanagement/#business-value","title":"Business value","text":"<p>Codified, Repeatable and Auditable deployment of complex systems, such as Cloud Paks across multiple clusters.</p>"},{"location":"ocp/advancedclustermanagement/#red-hat-advanced-cluster-management-hub-and-spoke-clusters-concept","title":"Red Hat Advanced Cluster Management Hub and Spoke Clusters Concept","text":"<p>The project leverages two Open Source technologies to underpin the functionality within this pattern. </p> <ul> <li>ArgoCD (aka OpenShift Gitops) </li> <li>Open Cluster Management (aka Red Hat Advanced Cluster Management or RHACM).</li> </ul> <p>The following diagram shows the Hub and Spoke architecture.</p> <p></p>"},{"location":"ocp/advancedclustermanagement/#next-steps","title":"Next steps","text":"<p>See One Touch Provisioning across Multi-Cloud on GitHub.</p>"},{"location":"ocp/architecture/","title":"Red Hat OpenShift Architecture Workshop","text":"<p>IBM Academy of Technology Red Hat OpenShift solution design guidance provides step-by-step guidance to conduct an OpenShift Decision Workshop, including:</p> <ul> <li>Reference architectures</li> <li>Architecture decisions</li> <li>Capacity planning</li> <li>Reference architectures</li> </ul> <p>The following diagram shows a sample on premises deployment reference architecture.</p> <p></p> <p>The common reference architecture is a great place to start. But clients will want a deeper understanding of the deployment and its capabilities.</p>"},{"location":"ocp/architecture/#red-hat-openshift-deployment-architecture","title":"Red Hat Openshift deployment architecture","text":"<p>Here is a typical design pattern for Red Hat OpenShift solutions. In this pattern, the entire cluster (i.e., control plane, worker, infrastructure) exist in the same network zone.</p> <p>The following conceptual deployment architecture showing three types of workloads:</p> <ul> <li>containerized workloads from IBM Cloud Paks deployed on the Red Hat OpenShift cluster</li> <li>containerized workloads from customer application microservices</li> <li>the workload from non-containerized enterprise applications hosted on regular VMs.</li> </ul> <p>Deployment architecture</p> <p>The following operational model shows the physical view for a common deployment.</p> <p>Physical architecture</p>"},{"location":"ocp/architecture/#architecture-decision-point-guidance","title":"Architecture decision point guidance","text":"<p>Cloud Solution Architects need to make these critical decisions as part of every Red Hat\u00ae OpenShift\u00ae design.</p> <ul> <li>HA and DR</li> <li>Red Hat OpenShift Cluster Configuration</li> <li>Red Hat OpenShift Instance Size</li> <li>Oversubscription</li> </ul> <p>Architecture decision points can help guide you through the decisions in deploying Openshift. Decisions rationale, motives, and considerations are explained for each decision:</p> <p>AD-001 - Red Hat OpenShift Cluster Platform Node Type AD-002 - Hosting Platform AD-003 - Red Hat OpenShift Platform Selection AD-004 - Red Hat OpenShift Version AD-005 - Master Node Scheduling AD-006 - Cluster Workload Isolation AD-007 - Master Node High-Availability AD-008 - Application Availability AD-009 - Load Balancer Selection AD-010 - Multi-Region Red Hat OpenShift Cluster Deployment AD-011 - Container Registry AD-012 - Management Service Placement AD-013 - Worker Node OS AD-014 - Overcommitting Nodes AD-015 - Editing Kubelet Parameters AD-016 - Storage Technology for Red Hat OpenShift Cluster Platform Registry AD-017 - Storage Technology for Metrics &amp; Logging AD-018 - Persistent Storage Options for Applications AD-019 - Production and Non-Production Co-Location AD-020 - Number of Installer Nodes AD-021 - Ingress Traffic for Applications AD-022 - High availability of UAT Environment</p>"},{"location":"ocp/architecture/#reference-cluster-configuration","title":"Reference Cluster Configuration","text":"<p>When you install OpenShift, you may want to include Operators and funcationality, such as:</p> <ul> <li>OpenShift Container Storage</li> <li>Cluster Logging</li> <li>Git (using the Gitea operator)</li> <li>Jenkins</li> <li>OpenShift Pipelines</li> <li>OpenShift GitOps (for clusters at 4.7+)</li> <li>Quay</li> <li>Nexus Repository (Optional - Artifactory is another common choice)</li> <li>Red Hat Single Sign-on (Optional)</li> <li>Red Hat Service Mesh (Optional)</li> </ul>"},{"location":"ocp/aro/","title":"Create a Azure RedHat OpenShift (ARO) cluster","text":"<p>In this tutorial, you will install Red Hat OpenShift on Azure (called ARO). You will install the resource group, virtual network, and ARO. Then you will connect to the cluster using the <code>oc</code> commands.</p> <p>The following illustration shows the networking traffic routing of the default ARO installation.</p> <p></p>"},{"location":"ocp/aro/#features","title":"Features","text":"<p>Important distinctions of ARO:</p> <ul> <li>Red Hat, billed by Microsoft. </li> <li>Supported by Red Hat and Mircosoft.</li> <li>Prometheus comes pre-installed and configured for Azure Red Hat OpenShift 4.x clusters. </li> </ul> <p></p> <p>IMPORTANT:   - Check OpenShift version supported by ARO to be sure it is compatible with your use case. - Check with your ATL to see if customers can use their IBM ELA for this installation or whether it is fully billed through Microsoft ELA.</p>"},{"location":"ocp/aro/#set-up","title":"Set up","text":"<p>You will need:</p> <ul> <li>Azure CLI. </li> </ul> <p>On Linux you can use:</p> <pre><code>curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n</code></pre> <p>Or on Mac (or Linux or WSL2 with Brew) use:</p> <pre><code>brew update &amp;&amp; brew install azure-cli\n</code></pre> <p>You will need Azure CLI version 2.6.0 or greater. Check using <code>az --version</code></p> <ul> <li>jq. Use <code>sudo apt-get install -y jq</code>.  To check to see if it is installed use <code>which oc</code></li> <li>oc CLI (Use either the Cloud Native Toolkit) <code>curl -sL shell.cloudnativetoolkit.dev | bash -</code> Or use the following:</li> </ul> <p><pre><code>cd ~\nwget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/latest/openshift-client-linux.tar.gz\n\nmkdir openshift\ntar -zxvf openshift-client-linux.tar.gz -C openshift\necho 'export PATH=$PATH:~/openshift' &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc\n</code></pre> To check to see if it is installed use <code>which oc</code></p> <p>IMPORTANT: You will also need:</p> <ul> <li>To create a resource group and resources, you must have either permissions for that scope. For example, to create the OCP cluster, you will need permissions on the resource group or subscription containing it:<ul> <li>Contributor and User Access Administrator permissions, or </li> <li>Owner permissions, either directly on the virtual network</li> </ul> </li> <li>Sufficient Azure Active Directory permissions (either a member user of the tenant, or a guest user assigned with role Application administrator) for the tooling to create an application and service principal on your behalf for the cluster.}</li> <li> <p>Permission to request help requests so that you can:</p> <ul> <li>Increase the default number of cores to 40 vCPUs (for the minimum installation). An off-the-shelf Azure subscription limits the number of vCPUs to 10 (for your protection). Submit a help request in your Subscriptions | Usage + Quotas panel for the region you want to deploy to, as shown in the following illustration:</li> </ul> <p></p> </li> </ul>"},{"location":"ocp/aro/#set-environment-variables","title":"Set environment variables","text":"<pre><code>az login\naz account subscription list\n\n## select a subscription from the list and enter it on the next line\nSUBSCRIPTION_ID=\"dcafb2cf-5c3b-49d0-969f-82dd18c4e466\"\nLOCATION=\"north central us\"\nRESOURCEGROUP=\"aro-nc-aro-project-1\"\nCLUSTER=\"cluster\"\nVNET_NAME=\"vnet-nc-aro-project-1\"\n</code></pre>"},{"location":"ocp/aro/#create-aro","title":"Create ARO","text":"<p>Follow instructions from Tutorial: Create an Azure Red Hat OpenShift 4 cluster</p> <pre><code>az account set --subscription $SUBSCRIPTION_ID\naz vm list-usage -l $LOCATION \\\n  --query \"[?contains(name.value, 'standardDSv3Family')]\" \\\n  -o table\n</code></pre> <p>Responds with:</p> <pre><code>CurrentValue    Limit    LocalName\n--------------  -------  --------------------------\n0               10       Standard DSv3 Family vCPUs\n</code></pre> <p>In the preceding case, you will need to file an help incident to request to increase your quota to 40. You will need access to 40 vCPUs.</p> <pre><code># you will need to only do this once for your CLI \naz provider register -n Microsoft.RedHatOpenShift --wait\naz provider register -n Microsoft.Compute --wait\naz provider register -n Microsoft.Storage --wait\naz provider register -n Microsoft.Authorization --wait\n</code></pre>"},{"location":"ocp/aro/#get-pull-secret","title":"Get pull secret","text":"<ol> <li>Navigate to your Red Hat OpenShift cluster manager portal and log in.</li> <li>Click Download pull secret and download a pull secret to be used with your ARO cluster.</li> </ol>"},{"location":"ocp/aro/#configure-public-dns-zone-in-azure","title":"Configure public DNS zone in Azure","text":"<p>Configure a DNS zone and ensure you delegate it to registrar. You can use Azure App service Domain or external internet domain registrar like GoDaddy. This is a critical step as the OpenShift installer tries to connect to OpenShift cluster using the DNS names that are created dynamically. Installation will fail if the DNS hostname are not resolved automatically.</p> <p>IMPORTANT: By default, OpenShift uses self-signed certificates for all of the routes created on custom domains <code>*.apps.example.com</code>. </p> <p>You can use an App Service Doman (provided by Microsoft) for your testing purposes.</p> <p></p>"},{"location":"ocp/aro/#create-resource-group-and-virtual-network","title":"Create resource group and virtual network","text":"<pre><code>az group create \\\n  --name $RESOURCEGROUP \\\n  --location $LOCATION\n\n## virtual network\naz network vnet create \\\n  --resource-group $RESOURCEGROUP \\\n  --name $VNET_NAME \\\n  --address-prefixes 10.0.0.0/22\n\n# control plane node subnet\naz network vnet subnet create \\\n  --resource-group $RESOURCEGROUP \\\n  --vnet-name $VNET_NAME \\\n  --name master-subnet \\\n  --address-prefixes 10.0.0.0/23 \\\n  --service-endpoints Microsoft.ContainerRegistry\n\n  az network vnet subnet create \\\n  --resource-group $RESOURCEGROUP \\\n  --vnet-name aro-vnet \\\n  --name worker-subnet \\\n  --address-prefixes 10.0.2.0/23 \\\n  --service-endpoints Microsoft.ContainerRegistry\n\n## disable private endpoints on control plane subnet\naz network vnet subnet update \\\n  --name master-subnet \\\n  --resource-group $RESOURCEGROUP \\\n  --vnet-name $VNET_NAME \\\n  --disable-private-link-service-network-policies true\n</code></pre> <p>NOTE: When you see the following error.</p> <pre><code>Deployment failed. Correlation ID: 8d4dd186-abf4-48ad-b071-c7a66aa635c8. Resource quota of cores exceeded. Maximum allowed: 10, Current in use: 0, Additional requested: 36.\n</code></pre>"},{"location":"ocp/aro/#create-cluster","title":"Create cluster","text":"<p>You will need the Azure for Redhat Openshift provider installed for the following command to work. In the following case, I am using the smaller VMs for my worker nodes.</p> <pre><code>az aro create \\\n  --resource-group $RESOURCEGROUP \\\n  --name $CLUSTER \\\n  --vnet $VNET_NAME \\\n  --master-subnet master-subnet \\\n  --worker-subnet worker-subnet \\\n  --master-vm-size Standard_D8s_v3 \\\n  --worker-vm-size Standard_F4s_v2 \\\n  --pull-secret @/mnt/c/Users/6J1943897/Downloads/pull-secret.txt\n</code></pre> <p>For more information on the parameters available, see az aro create command line reference.</p>"},{"location":"ocp/aro/#connect-to-aro","title":"Connect to ARO","text":"<p>Follow the instructions in the second section: Tutorial: Connect to an Azure Red Hat OpenShift 4 cluster</p> <pre><code>az aro list-credentials \\\n  --name $CLUSTER \\\n  --resource-group $RESOURCEGROUP\n</code></pre> <p>Get the URL to the OpenShift console</p> <pre><code>az aro show \\\n    --name $CLUSTER \\\n    --resource-group $RESOURCEGROUP \\\n    --query \"consoleProfile.url\" -o tsv\n</code></pre>"},{"location":"ocp/aro/#connect-to-openshift-server","title":"Connect to OpenShift server","text":"<p>To connect to the OpenShift server, use:</p> <pre><code>APISERVER=$(az aro show -g $RESOURCEGROUP -n $CLUSTER --query apiserverProfile.url -o tsv)\n\n## Replace &lt;kubeadmin password&gt; with the password you just retrieved.\noc login $APISERVER -u kubeadmin -p &lt;kubeadmin password&gt;\n</code></pre>"},{"location":"ocp/aro/#delete-cluster","title":"Delete cluster","text":"<p>To delete the cluster, use:</p> <pre><code>az aro delete --resource-group $RESOURCEGROUP --name $CLUSTER\n</code></pre>"},{"location":"ocp/aro/#security-considerations","title":"Security considerations","text":"<ul> <li>Active Directory integration</li> <li>Control egress traffic for your Azure Red Hat OpenShift (ARO) cluster (preview)</li> </ul>"},{"location":"ocp/aro/#configure-storageclass-for-rwx","title":"Configure StorageClass for RWX","text":"<p>For use with Cloud Paks and customer applications, you will want to dynamically provision ReadWriteMany (RWX) storage, which provides that your storage volume can be mounted as read-write by many nodes. </p> <p>You can use either OpenShift Container Platform storage (OCS) or OpenShift Data Foundation (ODF) Operators or set Azure Files for your StorageClass. </p> <p>NOTE: OpenShift Container Storage (OCS) has been updated to OpenShift Data Foundation (ODF) starting with version OCP 4.9. For more information, see either:</p> <ul> <li>OpenShift Container Platform storage overview </li> <li>Deploying OpenShift Data Foundation on Azure Red Hat OpenShift.</li> </ul> <p>OR if you prefer, you can set up Azure Files as your StorageClass. See Create an Azure Files StorageClass on Azure Red Hat OpenShift 4.</p>"},{"location":"ocp/aro/#advanced-reference","title":"Advanced reference","text":"<ul> <li>ARO Resource provider source code</li> </ul>"},{"location":"ocp/aro/#next-steps","title":"Next steps","text":"<p>Learn more about how to incorporate ARO into an existing Azure architecture. See Control egress traffic for your Azure Red Hat OpenShift (ARO) cluster (preview) and how to incorporate Azure AD identities into OpenShift, see Configure Azure Active Directory authentication for an Azure Red Hat OpenShift 4 cluster (CLI)</p> <p>The following illustration shows how ARO can be configured inside a customer firewall.</p> <p></p>"},{"location":"ocp/azure/","title":"Introduction to Azure","text":"<p>In this tutorial you will learn how to install a sample resource in Azure. It introduces the 300-level concepts in Azure, such as how to deploy resources, how resources are governed, how access is controlled to resources.</p> <p>The purpose is to help the reader understand how ARM deploys and manages your resources.</p>"},{"location":"ocp/azure/#prerequisites","title":"Prerequisites","text":"<p>Assumes you have seen how to deploy resource groups and resources in Azure portal.  Understand basics of role based access control.</p> <p>You will need:</p> <ul> <li>Install Azure CLI</li> </ul> <pre><code>brew update &amp;&amp; brew install azure-cli\naz login\n</code></pre>"},{"location":"ocp/azure/#definitions","title":"Definitions","text":"<ul> <li>Resource A manageable item that is available through Azure. Virtual machines, storage accounts, web apps, databases, and virtual networks are examples of resources.</li> <li>Resource Group a group of resources. (in practice, you group resources as lifecycle by region.)</li> <li>Subcription A way to bill for Azure. </li> </ul>"},{"location":"ocp/azure/#demo-on-using-the-portal","title":"Demo on using the portal","text":"<ol> <li>Stand up a storage account using the portal. </li> <li>Demonstrate the ARM template and parameter template that is created.</li> <li>Demonstrate role based access control for the resource group.</li> </ol> <p>You can do the same thing using the CLI. </p>"},{"location":"ocp/azure/#deploy-an-azure-resource","title":"Deploy an Azure resource","text":"<p>Ways to deploy Azure resources:</p> <ul> <li>Azure Portal the way to get started to deploy your first resources in Azure.</li> <li>CLI or PowerShell interfaces.</li> <li>Templates:<ul> <li>ARM Template a description of resources as a repeatable JSON template.</li> <li>Bicep template is similar to ARM template but uses a DSL that translates your calls into ARM template. Bicep is less typing.</li> <li>Terraform templates (requires Terraform).</li> <li>SDK that makes REST commands to Azure.</li> </ul> </li> <li>REST Interface to ARM. The portal and cli talk to the REST Interface by making calls to the REST interface. Yes, you can run it from Postman.</li> </ul> <p>For more information, see What is Resource Manager?</p> <p></p> <p>Identities are managed through Azure Active Directory (AAD). The instance of identities to mange Azure (or Microsoft 365) is called a tenant. For IBM, you will see this domain as <code>ibm.onmicrosoft.com</code>. NOTE: you and the permissions you exercise must be added into the AAD domain. To log into Azure, you will probably use your IBM credentials. On a customer site, they may add you to their tenant and grant some permissions using your IBMid.</p>"},{"location":"ocp/azure/#deploy-storage-account-using-the-cli","title":"Deploy storage account using the CLI","text":"<p>Recommend doing this in the following steps:</p> <ol> <li>Start your shell or command line, then log in</li> <li>Create environment variables</li> <li>Create the resource group</li> <li>Create the resource(s)</li> </ol> <pre><code>az login\n\nRESOURCE_GROUP_NAME=\"rg-wus2-storageproject\"\nSTORAGE_ACCOUNT_NAME=\"stproject567343d6\"\nLOCATION=\"West US 2\"\n\naz group create \\\n    --name $RESOURCE_GROUP_NAME \\\n    --location $LOCATION\n\naz storage account create \\\n    --name $STORAGE_ACCOUNT_NAME \\\n    --resource-group $RESOURCE_GROUP_NAME \\\n    --location $LOCATION \\\n    --sku Standard_LRS \\\n    --encryption-services blob\n</code></pre> <p>The CLI is documented. For the storage account, see <code>az storage</code></p> <p>Not all resources can be named in a similar way. See Naming rules and restrictions for Azure resources. A storage account must be gloabally unique and be between 3 and 24 lowercase characters and numbers.</p> <p>IMPORTANT: Your resource group location should be the same as the location of your resources.</p>"},{"location":"ocp/azure/#deploy-storage-account-using-a-template","title":"Deploy storage account using a template","text":"<p>You can deploy a resource from a template file. The template can be local or in storage. The following example, uses the current environment variables and uses a sample storage template from GitHub, using inline parameters into the template. </p> <pre><code>az deployment group create \\\n  --name $RESOURCE_GROUP_NAME$(date +\"%d-%b-%Y\") \\\n  --resource-group  $RESOURCE_GROUP_NAME \\\n  --template-uri \"https://raw.githubusercontent.com/Azure/azure-quickstart-templates/master/quickstarts/microsoft.storage/storage-account-create/azuredeploy.json\" \\\n  --parameters storageAccountType=Standard_LRS\n</code></pre> <p>Typically, you will use a parameters file with values.</p>"},{"location":"ocp/azure/#subscription-management","title":"Subscription management","text":"<p>In enterprises, a subscription is managed by a higher organization called a Management Group. Your subscription ID is accessed as a GUID or by name. - Management Group A group of subscriptions or other management groups that provide governance.</p> <p>The following diagram shows an example organization using Management Groups to manage subscriptions.</p> <p></p> <p>Resources are managed through ARM, the Azure Resource Manager, that provides REST commands for Azure to actions on the resources.</p>"},{"location":"ocp/azure/#resource-provider","title":"Resource provider","text":"<p>Resource provider is a type of resource. </p> <p>The resource provider is <code>Microsoft.Storage</code> for the storageAccounts https://docs.microsoft.com/en-us/azure/templates/microsoft.storage/storageaccounts?tabs=json </p> <p>Not all resource providers are available in the default subscription. Typically when you want to use a resouce intensive (read as costly) or third party resources (non Microsoft), you will need to add the resource provider.</p> <p>To view resource providers in the portal or CLI, see Azure resource providers and types</p> <p>IMPORTANT: To maintain least privileges in your subscription, only register those resource providers that you're ready to use. The following code shows how to register the Azure Batch resource provider and retrieve information about the provider:</p> <pre><code>az provider register --namespace Microsoft.Batch\naz provider show --namespace Microsoft.Batch\n</code></pre>"},{"location":"ocp/azure/#high-level-of-role-based-access-control","title":"High level of role based access control","text":"<ul> <li>Security Principal. A security principal is an object that represents a user, group, service principal, or managed identity that is requesting access to Azure resources. </li> </ul> <p>The following diagram shows the combination of security principal:</p> <p></p>"},{"location":"ocp/azure/#role","title":"Role","text":"<p>Role definition is a collection of permissions. It's typically just called a role. A role definition lists the operations that can be performed, such as read, write, and delete.</p> <p>Typically you will see:</p> <ul> <li>Owner</li> <li>Contributor</li> <li>Reader</li> </ul> <p>These are actually aggregations of permissions for underlying resources.</p> <p>The following illustration shows the Contributor role definition for a virtual machine.</p> <p></p> <p>For an exercise, see Quickstart: Check access for a user to Azure resources.</p> <p>See Azure built in roles.</p> <p>(300 LEVEL NOTE: Roles are actually aggregation of permissions of particular RBAC calls made on the resource. For example, see Build in role: Classic Storage Account Contributor</p>"},{"location":"ocp/azure/#scope","title":"Scope","text":"<p>Scope is the set of resources that the access applies to. </p> <p>Specify a scope at four levels: management group, subscription, resource group, or resource. Scopes are structured in a parent-child relationship. </p> <p></p>"},{"location":"ocp/azure/#role-assignment","title":"Role Assignment","text":"<p>You combine the User (or Service prinicpal) with the Role and Scope to create a Role Assignment.</p> <p></p> <p>For more information, see What is Azure role-based access control (Azure RBAC)?.</p>"},{"location":"ocp/azure/#service-principal","title":"Service principal","text":"<p>When applications, hosted services, or automated tools needs to access or modify resources, you can create an identity for the app. This identity is known as a Service principal.</p> <p>There is no way to directly create a service principal using the Azure portal itself. You will need access to the Azure AD portal or Office 365 portal. When you register an application through the Azure portal, an application object and service principal are automatically created in your home directory or tenant. Instead, you need access to the Tenant.</p> <p>NOTE: As applications modernize, the service principal is being replaced with a managed identity. Managed identities provide an identity for applications to use when connecting to resources that support Azure Active Directory (Azure AD) authentication.  In this case, the identity is resource created by the application itself. For more information, see What are managed identities for Azure resources?.</p> <p>IMPORTANT: See Securing service principals.</p>"},{"location":"ocp/azure/#tenant","title":"Tenant","text":"<p>A tenant provides identity and access management (IAM) capabilities to applications and resources used by your organization. An identity is a directory object that can be authenticated and authorized for access to a resource. Identity objects exist for human identities such as students and teachers, and non-human identities like classroom and student devices, applications, and service principles.</p> <p>The Azure AD tenant is an identity security boundary that is under the control of your organization\u2019s IT department. Within this security boundary, administration of objects (such as user objects) and configuration of tenant-wide settings are controlled by your IT administrators.</p> <p></p> <p>Organizations can be connected in a tenant. In the following illustration, Woodgrove Bank allows external users as Users its Azure AD tenant by connecting the tenants.</p> <p></p> <p>In this scenario, the client could allow you to log into the client Azure subscription using your IBMid.</p> <p>The authentication types for connected organizations are:</p> <ul> <li>Azure AD</li> <li>Direct federation</li> <li>One-time passcode (domain)</li> </ul> <p>(Note. It is possible to require MFA on your Azure resources through your Azure AD, even if the connected company does not require MFA.)</p>"},{"location":"ocp/azure/#create-a-service-principal","title":"Create a service principal","text":"<p>Very few resources will need you to create a service principal. You will need a service principal when you want to give prermissions to your script to call into the Azure Resource Manager. In other words, when you want the script to act on its own behalf, rather than using your user credentials.</p> <p>IMPORTANT: You will need permissions to access Azure Active Directory tenant to create a Service Principal. This is not the same as subscription permissions. Only users with an administrator role in the tenant may register a service principal. If you created the subscription, you have access to the tenant.</p> <p>Service principals can be created through the AD portal. See Use the portal to create an Azure AD application and service principal that can access resources</p>"},{"location":"ocp/azure/#demo-on-providing-roles-to-a-resource","title":"Demo on providing roles to a resource","text":"<p>When you make a call to ARM (with the portal, CLI, Terraform), the REST command will determine if you have permissions. </p>"},{"location":"ocp/azure/#show-openshift-and-cloud-pak-resources","title":"Show OpenShift and Cloud Pak resources","text":"<p>For more information, see Deployment reference.</p> <p>You must add the <code>Microsoft.RedHatOpenShift</code> resource provider to access OpenShift template.</p>"},{"location":"ocp/azure/#best-practices","title":"Best Practices","text":"<ul> <li>Naming convention for resources and resource groups</li> </ul>"},{"location":"ocp/clustersecurity/","title":"Red Hat Advanced Cluster Security for Kubernetes (RHACS)","text":"<p>Red Hat Advanced Cluster Security (ACS) for Kubernetes is the pioneering Kubernetes-native security platform, equipping organizations to more securely build, deploy, and run cloud-native applications. The solution helps protect containerized Kubernetes workloads in all major clouds and hybrid platforms, including:</p> <ul> <li>Red Hat OpenShift</li> <li>Amazon Elastic Kubernetes Service (EKS)</li> <li>Microsoft Azure Kubernetes Service (AKS)</li> <li>Google Kubernetes Engine (GKE)</li> </ul>"},{"location":"ocp/clustersecurity/#use-cases","title":"Use cases","text":"<p>Key use cases:</p> <ul> <li>Vulnerability management. Implement risk-based vulnerability management that spans the full application life cycle.</li> <li>Compliance. Ensure your cloud native environment is compliant with industry standards and best practices such as CIS Benchmarks, NIST, PCI, and HIPAA.</li> <li>Configuration management.  Automate configuration best practices to help prevent security misconfigurations in the build pipeline and deployments.</li> <li>Threat detection and response.  Use behavioral analysis, rules, and allow-listing to understand runtime behavior and detect and respond to threats.</li> </ul>"},{"location":"ocp/clustersecurity/#devsecops","title":"DevSecOps","text":"<p>Red Hat Advanced Cluster Security integrates with DevOps and security tools to help you mitigate threats and enforce security policies that minimize operational risk to your applications within your Kubernetes environment.</p> <p>Red Hat Advanced Cluster Security reduces the time and effort needed to implement security by acting as a common source of truth, so you can streamline security analysis, investigation, and remediation.</p> <p>For more information, see What is DevSecOps?.</p>"},{"location":"ocp/clustersecurity/#features-and-benefits-of-red-hat-advanced-cluster-security-for-kubernetes","title":"Features and benefits of Red Hat Advanced Cluster Security for Kubernetes","text":"<p>Lower operational cost</p> <ul> <li>Guide development, operations, and security teams towards using a common language and source of truth\u2014driving down the operational costs of team silos.</li> <li>Use Kubernetes-native controls across the build, deploy, and runtime phases of the application for better visibility and management of vulnerabilities, policy and configuration violations, and application runtime behavior.</li> <li>Reduce the cost of addressing a security issue by catching and fixing it in the development stage.</li> </ul> <p>Reduce operational risk</p> <ul> <li>Align security and infrastructure to reduce application downtime using built-in Kubernetes capabilities, such as Kubernetes network policies for segmentation and admission controller for security policy enforcement.</li> <li>Mitigate threats using Kubernetes-native security controls to enforce security policies, minimizing potential impacts to your applications and infrastructure operations. For example, using controls to contain a successful breach by automatically instructing Kubernetes to scale suspicious pods to zero or kill then restart instances of breached applications.</li> </ul> <p>Increase developer productivity</p> <ul> <li>Take advantage of Kubernetes and existing continuous integration and continuous delivery (CI/CD) tooling to provide integrated security guardrails supporting developer velocity while still maintaining the desired security posture. </li> <li>Accelerate your organization\u2019s pace of innovation and provide developers actionable guidance by standardizing on Kubernetes as the common platform for declarative and continuous security across development, security, and operations.</li> </ul>"},{"location":"ocp/clustersecurity/#secure-the-software-supply-chain","title":"Secure the software supply chain","text":"<p>By integrating with your CI/CD pipelines and image registries, Red Hat Advanced Cluster Security provides continuous scanning and assurance. By shifting security left, vulnerable and misconfigured images can be remediated within the same developer environment with real-time feedback and alerts. Integration with Cosign/sigstore delivers security attestation for your assets, including image and deployment signing, for security validation and tamper detection.</p>"},{"location":"ocp/clustersecurity/#rhacs-operator","title":"RHACS Operator","text":"<p>A Kubernetes operator is a method of packaging, deploying, and managing a Kubernetes application.</p> <p>Using the RHACS operator, you can configure RHACS like an expert operator by taking advantage of the operational knowledge built into the operator and exposed as a custom resource. </p> <p>The Red Hat Advanced Cluster Security operator supports the following two custom resources:</p> <ul> <li>Central custom resource</li> <li>SecuredCluster custom resource</li> </ul>"},{"location":"ocp/clustersecurity/#central-custom-resource","title":"Central custom resource","text":"<p>The Central custom resource allows users to configure Central services. Central is the management control plane and user interface for RHACS. Central includes the following services:</p> <ul> <li>Central. Central is the RHACS application management interface and services.</li> <li>Scanner. Scanner is the StackRox vulnerability scanner, a Red Hat developed and certified scanner for the container images and associated databases.</li> </ul>"},{"location":"ocp/clustersecurity/#securedcluster-custom-resource","title":"SecuredCluster custom resource","text":"<p>The SecuredCluster custom resource allows users to configure SecuredCluster services. Secured Cluster Services manages the components of RHACS necessary to secure your OpenShift cluster. SecuredCluster includes the following services:</p> <p>-- Sensor. Sensor is the service responsible for analyzing and monitoring the cluster. - Collector. Collector analyzes and monitors container activity on Kubernetes nodes. - Admission Control. Admission Controller is the validating webhook designed to enforce and monitor events against the OpenShift/Kubernetes API server.</p>"},{"location":"ocp/clustersecurity/#configuration-options","title":"Configuration options","text":"<p>The operator\u2019s default configuration settings are set to a \u201cmonitor\u201d security policy, allowing users time to analyze their typical behavior, set policies and enforce the created rules when comfortable. As you become acquainted with RHACS, it will be essential to understand the configuration options and adjust the settings to fit your requirements.</p> <p>One of the main benefits of operators is the configuration capabilities that are given to the users. RHACS users can alter a significant amount of Central services and the SecuredCluster service settings. For common settings, see Useful Configuration Options.</p> <p>See Generating and applying an init bundle for RHACS on Red Hat OpenShift for a complete list.</p>"},{"location":"ocp/clustersecurity/#next-steps","title":"Next steps","text":"<p>See A layered approach to container and Kubernetes security whitepaper.</p>"},{"location":"ocp/clustersecurity/#reference","title":"Reference","text":"<ul> <li>Red Hat Advanced Cluster Security for Kubernetes</li> <li>Datasheet Red Hat Advanced Cluster Security for Kubernetes</li> <li>The Advanced Cluster Security Operator Is Here. What You Need to Know and How to Get Started</li> </ul>"},{"location":"ocp/devops/","title":"Use Cloud Native Toolkit DevOps on ROKS instance, share with team members","text":"<p>Cloud-Native Toolkit is a collection of open-source assets that provide an environment for developing cloud-native applications for deployment within Red Hat OpenShift and Kubernetes. </p>"},{"location":"ocp/devops/#environment-components","title":"Environment components","text":"<p>After installation, the environment consists of the following components and developer tools:</p> <ul> <li>A Red Hat OpenShift or IBM Cloud Kubernetes Service development cluster</li> <li>A collection of continuous delivery tools deployed into the cluster</li> <li>A set of backend services</li> </ul> <p>This diagram illustrates the environment:</p> <p></p> <p>In this tutorial, learn how to set up the Cloud-Native Toolkit in your own OpenShift cluster running on IBM Cloud in TechZone. NOIE: The Skytap and Fyre instances do not provide a way to share the OpenShift instance with your team.</p> <p>In this tutorial, you will:</p> <ol> <li>Create an cluster of IBM RedHat Openshift Kubernetes Service (ROKS) </li> <li>Install the Cloud Native Toolkit into your ROKS cluster to provide the tools needed for your CI/CI pipeline.</li> <li>Add Artifactory as your HELM and container image repository.</li> <li>Clone the sample repo of a React application into GitHub, then pull the sample from your GitHub to run locally. Run the app locally.</li> <li>Set up the pipeline.</li> <li>Check the pipeline is running.</li> <li>Add team members join in working with the cluster.</li> </ol> <p>You will want to use ROKS when you want to share your cluster with others on your team.</p>"},{"location":"ocp/devops/#roles","title":"Roles","text":"<p>The tutorial assumes you are an admin who deploys the cluster and developers who work within a project/namespace in the cluster.</p> <p>When you create the OpenShift cluter in TechZone, you will become the admin for the cluster.</p> <p></p> <p>Team members can join as demonstrated in the last section of the tutorial.</p>"},{"location":"ocp/devops/#prerequisites","title":"Prerequisites","text":"<p>You will need:</p> <ul> <li>IBMid</li> <li>Internal access to TechZone and understand how to create a reservation in TechZone.</li> <li>Git</li> <li> <p>Github account and know how to clone your repo locally.</p> </li> <li> <p>The GitHub account Personal Access Token.</p> </li> <li>IBM Cloud CLI.</li> <li>If on Mac OS or Linux then you need to have curl installed. This is usually available from the Linux package manager or installed as part of the base operating system.</li> <li>For all operating systems Node.js needs to be installed.</li> <li>How to log into IBM Cloud and retrieve your login token. See IBM Cloud Setup</li> </ul> <p>Check to see that you have <code>ibmcloud</code> installed in your command line.</p> <pre><code>which ibmcloud\nwhich oc\n</code></pre>"},{"location":"ocp/devops/#create-a-reservation-in-techzone","title":"Create a reservation in TechZone","text":"<p>In this step you will create a reservation in TechZone and add your team members.</p> <p>Begin by creating a reservation in TechZone. Go to TechZone, click Environments search in the s searchbox using <code>IBM RedHat Openshift Kubernetes Service (ROKS)</code>.</p> <p></p> <p>Click the computer icon on the line matching your requirementsm in this case, the Hybrid Cloud collection. </p> <p>Reserve your reservation with something similar to the following:</p> <p></p> <p>Select a Preferred Geography with some capacity available and NFS Size can be none.</p> <p>Click Submit.</p> <p>The IBM Cloud takes an hour or so to provision. </p> <p>Once provisioned, you can view your reservation by going to TechZone then use the My library along the top, then click My reservations menu item. The reservation will need to be in the Provisioned state to proceed.</p>"},{"location":"ocp/devops/#get-the-openshift-command-line-to-log-into-the-cluster","title":"Get the OpenShift command line to log into the cluster","text":"<p>A few hours after asking for your cluster, it should be provisioned. Go to TechZone main page, click My Library menu at the top, click My reservations. Click the panel with your reservation to see the overview of the reservation.</p> <p>Take note of the Cloud Account. This is the cloud account DTE uses to run your account. When you log into your cluster, you will use this identity to manage it.</p> <p></p> <p>Click the cluster URL to go directly to the cluster. </p> <p></p> <p>You will want to take note of the Cluster name that TechZone uses. If you have multiple clusters, you will want to know this name as the one created by DTE.</p> <p>Click the blue OpenShift web console button. That brings you to:</p> <p></p> <p>Click IAM#yourlogin.com and select Copy Login Command from the drop down menu.</p> <p>Click Display Token, which is the only thing on the page.</p> <p></p> <p>Copy the Log in with this token onto your clipboard.</p> <p>You are ready to log into OpenShift from the command line.</p>"},{"location":"ocp/devops/#log-into-the-openshift-from-the-command-line","title":"Log into the OpenShift from the command line","text":"<p>Log into the OpenShift from the command line. The following example shows using your key from the previous step.</p> <pre><code>oc login --token=sha256-XXXXXXXXXXXXX-iVs --server=https://c107-e.us-south.containers.cloud.ibm.com:32106 \n</code></pre> <p>IMPORTANT: Do not share your command line with other users, otherwise they will be executing commands in the cluster under your name and credentials.</p>"},{"location":"ocp/devops/#set-up-the-cluster-using-the-cloud-native-toolkit","title":"Set up the cluster using the Cloud Native Toolkit","text":"<p>Use the following command to set up your toolkit in the IBM Cloud cluster.</p> <pre><code>curl -sfL get.cloudnativetoolkit.dev | sh -\n</code></pre> <p>(If you want to set up a multi-user workshop, use <code>curl -sfL workshop.cloudnativetoolkit.dev | sh -</code> to set up your cluster. This will provision user names from <code>user01</code> to <code>user15</code> with a password of <code>password</code>, like you had in bootcamp). </p> <p>For more information, see Setup Workshop Environment. For more advanced set up, see Prepare an IBM Cloud account.</p> <p>You have completed setting up the Cloud Native Toolkit in your Open Stack cluster. </p> <p>Log into RedHat cluster, click on ninebox to see the list of tools that the toolkit installed.</p> <p></p>"},{"location":"ocp/devops/#add-artifactory","title":"Add Artifactory","text":"<p>Next, use the steps in Cloud Native Toolkit documentation to set up Artifactory. </p> <p>Artifactory functions as the single source of truth for all packages, container images and Helm charts, as they move across the entire DevOps pipeline. It is similar to the repository we expect you to find at customer sites.</p> <p>The installation seemlessly integrates Artifactory with OpenShift.</p> <p>In a later step, you will build the container image and push it into Artifactory within the pipeline.</p>"},{"location":"ocp/devops/#get-the-sample-code-for-your-react-project","title":"Get the sample code for your React project","text":"<p>Next, let's install sample source code to put into the pipeline. In this case, use the React UI Sample code.</p> <p>Log into RedHat cluster, click on ninebox, click Developer Dashboard.</p> <p>Next click Starter Kits, then click React UI Patterns panel. This will ask to connect a repo, with a prepopulated code for everything you need. Name the repo <code>react-demo</code>. </p> <pre><code># set MYREPO environment variable to your repo url\nMYREPO=\"https://yourgithubid/react-demo\"\n</code></pre> <p>You have now cloned the React sample in your GitHub repo.</p> <p>Next you will pull the repo to your local machine.</p>"},{"location":"ocp/devops/#clone-repo-to-your-development-computer","title":"Clone repo to your development computer","text":"<p>In this step, you clone the repo to your local development computer and run locally. The following script creastes a directory for your code and clones your repository.</p> <pre><code>MYCODE=$HOME/code\nmkdir $MYCODE\ncd $MYCODE\ngit clone $MYREPO\ncd react-demo\n</code></pre> <p>Change directory into the demo</p> <p>Set the version of Node. It must be version 12, because the template in React sample requires that version.</p> <pre><code>npm install -g n\nsudo n 12\nnode -v\n</code></pre> <p>Returns the version number for Node.js, such as:</p> <pre><code>v12.22.5\n</code></pre> <p>Remove the package-lock.json from root and from <code>client</code> directories.</p> <pre><code>rm -rf package-lock.json\nrm -rf client/package-lock.json\n</code></pre>"},{"location":"ocp/devops/#install-npm-dependencies","title":"Install NPM dependencies","text":"<pre><code>npm  install\ncd client\nnpm install\ncd ..\nnpm run\n</code></pre> <p>You can now check the status of the application.</p> <pre><code>npm start\n</code></pre> <p>Use the URL that is returned to see the project running in your browser.</p> <p>If this is running type <code>Ctrl-c</code> to continue build the pipeline.</p> <p>You have now confirmed that the application is running as expected.</p>"},{"location":"ocp/devops/#create-the-pipeline","title":"Create the pipeline","text":"<p>Check to be sure you are in the correct directory.</p> <pre><code>ls\n</code></pre> <p>Should respond with the files and folders including <code>client</code> directory. If not, change directory to the root of your project.</p> <p>To create the pipeline, you will take these steps:</p> <ol> <li>Create a new project</li> <li>Create the pipeline for the project in OpenShift</li> <li>Check the console to see the pipeline runs successfully</li> </ol> <p>Let's verify you are in the repo.</p>"},{"location":"ocp/devops/#create-a-project","title":"Create a project","text":"<p>Create a project named <code>reactui-sample</code> in OpenShift using:</p> <pre><code>PROJECT_NAME=reactui-sample\noc sync $PROJECT_NAME\n</code></pre> <p>The commands set up the project, also creates the namespace in Kubernetes. The command replies:</p> <pre><code>Setting up namespace reactui-sample\nChecking for existing project: reactui-sample\nCreating project: rreactui-sample\nCopying ConfigMaps\nCopying Secrets\nSetting current project to reactui-sample\n</code></pre> <p>NOTE: The project name must be lowercase or dashes or numbers, no capitals nor underscores.</p>"},{"location":"ocp/devops/#create-the-pipeline_1","title":"Create the pipeline","text":"<p>Create the pipeline, provide the GitHub authentication and select Tekton.</p> <pre><code>oc pipeline\n</code></pre> <p>Use the arrow key to select <code>Tekton</code>, and press <code>enter</code>.</p> <p>The command prompts you for your GitHub username and use your Personal Access Token. (The password option has been disabled in GitHub.)</p> <p>Use the arrow keys to select <code>ibm-nodejs</code> and press enter. </p> <p>Say <code>n</code> to scan-image and <code>y</code> for lint-dockerfile`**. The current build fails the image security scan due to its dependencies.</p>"},{"location":"ocp/devops/#log-into-the-console","title":"Log into the console","text":"<p>The previous command replies:</p> <p></p> <p>Select the URL after <code>View PipelineRun</code> and copy into your browser to see the pipeline run.</p> <p>Log into the console to see the pipeline running successfully.</p> <p>The console shows the pipeline working.</p> <p></p> <p>You have successfully created the pipeline and can share the project with other developers on your team who are granted permissions through TechZone.</p>"},{"location":"ocp/devops/#add-team-members","title":"Add team members","text":"<p>After your cluster is provisioned, you can  provide access to the cluster for your team members.  </p> <p>You will want to add team members who may want to view their executing code after the build process. To share the cluster with team members, first invite them from withing TechZone reservation and then the team members must accept the invitation. Once accepted, the cluster will one of their IBM Cloud clusters viewable within the IBM Cloud portal.</p>"},{"location":"ocp/devops/#invite-team-members-in-techzone-reservation","title":"Invite team members in TechZone reservation","text":"<p>Go to your reservation in TechZone, click on the <code>:</code> in the upper right of your reservation, the click Share.  </p> <p></p> <p>Fill in the dialog with the IBMid for each user, one at a time. The users should receive an invite by email to use cluster.</p> <p>NOTE: You will not have permission to add users from within the TechZone/DTS provided subscription. You will need to invite team members through TechZone.</p> <p>If for some reason your team member does not generate an email or notification for your user, contact <code>#dte-techzone-support</code> on Slack.</p>"},{"location":"ocp/devops/#team-members-accept-invitation","title":"Team members accept invitation","text":"<p>In a previous step, as admin you invited team members to join in your IBM Cloud account. As a team member, you must accept the invitation. The invitation is either:</p> <ul> <li>In your email from DTS. Open the email and click Join now link in the email.</li> <li> <p>In your notifications in IBM Cloud, use the following steps:</p> <p>Team members log into the IBM cloud portal using your IBM account. Your users log into the cluster from IBM Cloud Login using their own IBMId credentials. </p> <p>Click the Bell icon the upper menu. Select the notification for joining. Click on the Join now. link as shown in the following illustration.</p> </li> </ul> <p></p> <p>As administrator, you will need to provide the cluster name to your team members. But the team members should be able to see the cluster in their IBM Cloud. Navigate to the main IBM Cloud console page. </p> <p>Click Clusters under Resource Summary in the second set of panels from the top of the page.</p> <p>Click the your cluster to get to the cluster portal page.</p>"},{"location":"ocp/devops/#references","title":"References","text":"<ul> <li>Cloud Native Toolkit</li> <li>Cloud Native Garage Method Boot Camp</li> </ul>"},{"location":"ocp/devops/#authors","title":"Authors","text":"<ul> <li>Taeksu Kim</li> <li>Alex Wu</li> <li>Jack Sanders</li> <li>Bruce Kyle</li> </ul> <p>Aug 20, 2021</p>"},{"location":"ocp/elasticsearch/","title":"Install Elastic Cloud on Kubernetes (Elastic Kubernetes Operator) using the CLI","text":"<p>Elastic Cloud on Kubernetes (ECK) is an official open source operator designed especially for the Elastic Stack (ELK). It lets you automatically deployment, provisioning, management, and orchestration of Elasticsearch, Kibana, APM Server, Beats, Enterprise Search, Elastic Agent and Elastic Maps Server on Kubernetes. ECK provides features like monitoring clusters, automated upgrades, scheduled backups, and dynamic scalability of local storage.</p> <p>Following the instructions in the Red Hat OpenShift documentation Install cluster logging using the CLI</p> <p>In this section, you will:</p> <ul> <li>Create Namespaces for the Elasticsearch Operator and for the Cluster Logging Operator</li> <li>Install the OpenShift Elasticsearch Operator's<ul> <li>Operator Group</li> <li>Subscription</li> </ul> </li> <li>Install the Cluster Logging Operator's<ul> <li>Operator Group</li> <li>Subscription</li> <li>Instance</li> </ul> </li> <li>Verify the installation by listing the pods</li> </ul>"},{"location":"ocp/elasticsearch/#definitions","title":"Definitions","text":"<p>Operator Lifecycle Manager (OLM) helps users install, update, and manage the lifecycle of Kubernetes native applications (Operators) and their associated services running across their OpenShift Container Platform clusters. </p> <p>Operator Group provides multitenant configuration to OLM-installed Operators. An Operator group selects target namespaces in which to generate required RBAC access for its member Operators.</p> <p>Subscription keeps operators up to date by tracking changes to Catalogs. A subscription is optional.</p>"},{"location":"ocp/elasticsearch/#see-what-is-available-in-the-operatorhub-marketplace","title":"See what is available in the OperatorHub marketplace","text":"<p>Get list from marketplace</p> <pre><code>oc get packagemanifests -n openshift-marketplace\n</code></pre> <p>Returns a long list that includes <code>elasticsearch-operator</code>.</p> <ol> <li>Inspect the desired Operator</li> </ol> <pre><code>oc describe packagemanifests elasticsearch-operator -n openshift-marketplace\n</code></pre> <p>In the description, it recommends that this operator be installed in the <code>openshift-operators-redhat</code> namespace to properly support the Cluster Logging and Jaeger use cases.</p>"},{"location":"ocp/elasticsearch/#steps-to-deploy-elasticsearch-from-cli","title":"Steps to deploy Elasticsearch from CLI","text":""},{"location":"ocp/elasticsearch/#1-create-namespaces","title":"1. Create Namespaces","text":"<p>Create a namespace for the OpenShift Elasticsearch Operator:</p> <pre><code>cat &lt;&lt;EOF | oc create -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: openshift-operators-redhat \n  annotations:\n    openshift.io/node-selector: \"\"\n  labels:\n    openshift.io/cluster-monitoring: \"true\"\nEOF\n</code></pre> <p>Note: You must specify the <code>openshift-operators-redhat</code> namespace. </p> <p>Next, create a namespace for the Cluster Logging Operator:</p> <pre><code>cat &lt;&lt;EOF | oc create -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: openshift-logging\n  annotations:\n    openshift.io/node-selector: \"\"\n  labels:\n    openshift.io/cluster-monitoring: \"true\"\nEOF\n</code></pre>"},{"location":"ocp/elasticsearch/#2-install-the-openshift-elasticsearch-operator-by-creating","title":"2. Install the OpenShift Elasticsearch Operator by creating:","text":"<ul> <li>Operator Group object</li> <li>Subscription object</li> </ul> <p>An Operator group, defined by an <code>OperatorGroup</code> object, selects target namespaces in which to generate required RBAC access for all Operators in the same namespace as the Operator group. </p> <p>If the Operator you intend to install uses the <code>AllNamespaces</code>, then the openshift-operators namespace already has an appropriate Operator group in place.</p> <pre><code>cat &lt;&lt;EOF | oc create -f -\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: openshift-operators-redhat\n  namespace: openshift-operators-redhat \nspec: {}\nEOF\n</code></pre> <p>A Subscription object describes the namespace to the OpenShift Elasticsearch Operator.</p> <pre><code>cat &lt;&lt;EOF | oc create -f -\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: \"elasticsearch-operator\"\n  namespace: \"openshift-operators-redhat\" \nspec:\n  channel: \"4.6\" \n  installPlanApproval: \"Automatic\"\n  source: \"redhat-operators\" \n  sourceNamespace: \"openshift-marketplace\"\n  name: \"elasticsearch-operator\"\nEOF\n</code></pre> <p>You must specify the <code>openshift-operators-redhat</code> namespace. Specify <code>4.6</code> as the channel.</p> <p>Verify the Operator installation.</p> <pre><code>oc get csv --all-namespaces\n</code></pre> <p>There should be an OpenShift Elasticsearch Operator in each namespace. </p>"},{"location":"ocp/elasticsearch/#3-install-the-cluster-logging-operator","title":"3. Install the Cluster Logging Operator","text":"<p>Create a OperatorGroup for the Cluster Logging Operator and subscribe to the namespace using:</p> <p><pre><code>cat &lt;&lt;EOF | oc create -f -\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: cluster-logging\n  namespace: openshift-logging \nspec:\n  targetNamespaces:\n  - openshift-logging \n---\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: cluster-logging\n  namespace: openshift-logging \nspec:\n  channel: \"4.6\" \n  name: cluster-logging\n  source: redhat-operators \n  sourceNamespace: openshift-marketplace\nEOF\n</code></pre> The Cluster Logging Operator is installed to the openshift-logging namespace.</p> <p>Verify using:</p> <pre><code>oc get csv -n openshift-logging\n</code></pre>"},{"location":"ocp/elasticsearch/#4-create-a-cluster-logging-instance","title":"4. Create a Cluster Logging instance","text":"<p>Create an instance object for Cluster Logging Operator.</p> <pre><code>cat &lt;&lt;EOF | oc create -f -\napiVersion: \"logging.openshift.io/v1\"\nkind: \"ClusterLogging\"\nmetadata:\n  name: \"instance\" \n  namespace: \"openshift-logging\"\nspec:\n  managementState: \"Managed\"  \n  logStore:\n    type: \"elasticsearch\"  \n    retentionPolicy: \n      application:\n        maxAge: 1d\n      infra:\n        maxAge: 7d\n      audit:\n        maxAge: 7d\n    elasticsearch:\n      nodeCount: 3 \n      storage:\n        storageClassName: \"&lt;storage-class-name&gt;\" \n        size: 200G\n      resources: \n        requests:\n          memory: \"8Gi\"\n      proxy: \n        resources:\n          limits:\n            memory: 256Mi\n          requests:\n             memory: 256Mi\n      redundancyPolicy: \"SingleRedundancy\"\n  visualization:\n    type: \"kibana\"  \n    kibana:\n      replicas: 1\n  curation:\n    type: \"curator\"\n    curator:\n      schedule: \"30 3 * * *\" \n  collection:\n    logs:\n      type: \"fluentd\"  \n      fluentd: {}\nEOF\n</code></pre> <p>This creates the Cluster Logging components, the Elasticsearch custom resource and components, and the Kibana interface.</p>"},{"location":"ocp/elasticsearch/#5-verify-logging","title":"5. Verify logging","text":"<p>Verify using:</p> <pre><code>oc get pods -n openshift-logging\n</code></pre>"},{"location":"ocp/elasticsearch/#6-post-installation-tasks","title":"6. Post-installation tasks","text":"<p>If you plan to use Kibana, you must manually create your Kibana index patterns and visualizations to explore and visualize data in Kibana.</p> <p>If your cluster network provider enforces network isolation, allow network traffic between the projects that contain the OpenShift Logging operators.</p>"},{"location":"ocp/local/","title":"Red Hat OpenShift Local","text":"<p>Red Hat OpenShift Local brings a minimal OpenShift Container Platform 4 cluster and Podman container runtime to your local computer:</p> <ul> <li>Windows. Windows 10 Fall Creators Update (version 1709) or later. </li> <li>Mac. macOS 11 Big Sur or later</li> <li>Linux. The latest two Red Hat Enterprise Linux/CentOS 7, 8 and 9 minor releases and on the latest two stable Fedora releases.</li> </ul> <p>The OpenShift Container Platform cluster is ephemeral and is not intended for production use. Red Hat OpenShift Local does not have a supported upgrade path to newer OpenShift Container Platform versions.</p>"},{"location":"ocp/local/#hardware-requirements","title":"Hardware requirements","text":"<p>For OpenShift Container Platform</p> <ul> <li>4 physical CPU cores</li> <li>9 GB of free memory</li> <li>35 GB of storage space</li> </ul> <p>For the Podman container runtime</p> <ul> <li>2 physical CPU cores</li> <li>2 GB of free memory</li> <li>35 GB of storage space</li> </ul> <p>This may not be enough resources for demo sizings of some Cloud Paks.</p>"},{"location":"ocp/local/#next-steps","title":"Next steps","text":"<p>Review Getting Started Guide for steps to install.</p>"},{"location":"ocp/odf/","title":"Install OpenShift Data Foundation","text":"<p>Red Hat OpenShift Data Foundation (ODF) \u2014 previously Red Hat OpenShift Container Storage (OCS) \u2014 is software-defined storage for containers.</p> <p>Red Hat OpenShift Data Foundation is a persistent storage solution for the OpenShift Container Platform that supports file, block, and object storage on-premises or in hybrid clouds. ODF is fully integrated with OpenShift Container Platform for deployment, management, and monitoring.</p> <p>Use OpenShift Data Foundation for OpenShift versions starting with version 4.9.</p> <p>OpenShift Container Platform has been verified to work in conjunction with localstorage devices and OpenShift Container Storage (OCS) on AWS EC2, VMware, Azure and Bare Metal hosts.</p>"},{"location":"ocp/odf/#openshift-data-foundation-services","title":"OpenShift Data Foundation services","text":"<p>Red Hat OpenShift Data Foundation services are primarily made available to applications by way of storage classes that represent the following components:</p> <ul> <li>Block storage devices, catering primarily to database workloads. Prime examples include Red Hat OpenShift Container Platform logging and monitoring, and PostgreSQL.</li> <li>Shared and distributed file system, catering primarily to software development, messaging, and data aggregation workloads. Examples include Jenkins build sources and artifacts, Wordpress uploaded content, Red Hat OpenShift Container Platform registry, and messaging using JBoss AMQ.</li> <li>Multicloud object storage, featuring a lightweight S3 API endpoint that can abstract the storage and retrieval of data from multiple cloud object stores.</li> <li>On premises object storage, featuring a robust S3 API endpoint that scales to tens of petabytes and billions of objects, primarily targeting data intensive applications. Examples include the storage and access of row, columnar, and semi-structured data with applications like Spark, Presto, Red Hat AMQ Streams (Kafka), and even machine learning frameworks like TensorFlow and Pytorch.</li> </ul> <p>The following diagram shows the Red Hat OpenShift Data Foundation architecture:</p> <p></p> <p>For more information, see Red Hat OpenShift Data Foundation architecture.</p> <p>Red Hat OpenShift Data Foundation supports deployment into Red Hat OpenShift Container Platform clusters deployed on:</p> <ul> <li>Installer Provisioned Infrastructure </li> <li>User Provisioned Infrastructure</li> </ul> <p>For details about these two approaches, see OpenShift Container Platform - Installation process. </p>"},{"location":"ocp/odf/#odf-operators","title":"ODF Operators","text":"<p>Red Hat OpenShift Data Foundation is comprised of three Operator Lifecycle Manager (OLM) operator bundles, deploying four operators which codify administrative tasks and custom resources so that task and resource characteristics can be automated:</p> <ul> <li> <p>OpenShift Data Foundation     <code>odf-operator</code></p> </li> <li> <p>OpenShift Container Storage     <code>ocs-operator</code> <code>rook-ceph-operator</code></p> </li> <li> <p>Multicloud Object Gateway     <code>mcg-operator</code></p> </li> </ul> <p>Administrators define the desired end state of the cluster, and the OpenShift Data Foundation operators ensure the cluster is either in that state or approaching that state, with minimal administrator intervention.</p>"},{"location":"ocp/odf/#lab","title":"Lab","text":"<p>For a lab on how to deploy OCS and ODF, see Installation and Configuration.</p>"},{"location":"ocp/odf/#references","title":"References","text":"<p>See</p> <ul> <li>Installing OpenShift Container Storage from Dave Wakeman</li> <li>Red Hat documentation: ODF architecture</li> <li>OpenShift Container Storage for an quick high level view of how OCS and ODF works.</li> </ul>"},{"location":"ocp/odf/#learn-more","title":"Learn more","text":"<ul> <li>Storage patterns for Kubernetes ebook.</li> </ul>"},{"location":"ocp/operators/","title":"Getting started with Operators in OpenShift","text":"<p>An Operator extends Kubernetes to automate the management of the entire life cycle of a particular application. Operators serve as a packaging mechanism for distributing applications on Kubernetes, and they monitor, maintain, recover, and upgrade the software they deploy. </p> <p>For context:</p> <ul> <li>Manifests work great to manage stateless applications.</li> <li>Operators make it easy to manage complex stateful applications on top of Kubernetes. </li> </ul>"},{"location":"ocp/operators/#what-are-operators","title":"What are Operators?","text":"<p>Operators are Kubernetes applications. There are lots of ways to extend Kubernetes. In this tutorial, you will learn about Operators and custom resources.</p> <p>Operators make it easy to manage complex stateful applications on top of Kubernetes.</p> <p>Operators allow you to write code to automate a task, beyond the basic automation features provided in Kubernetes. For teams following a DevOps or site reliability engineering (SRE) approach, operators were developed to put SRE practices into Kubernetes. </p> <p>IBM Cloud Paks are collections of Operators that you install. Links at the end of the document gets you started in deploying Cloud Pak for Integration using Operators.</p>"},{"location":"ocp/operators/#definitions","title":"Definitions","text":"<p>All Operators use the controller pattern, but not all controllers are Operators. </p> <p>It's only an Operator if it's got: </p> <p>controller pattern + API extension + single-app focus.</p> <p>A Kubernetes operator is an application-specific controller that extends the functionality of the Kubernetes API to create, configure, and manage instances of complex applications on behalf of a Kubernetes user.</p> <p>It builds upon the basic Kubernetes resource and controller concepts, but includes domain or application-specific knowledge to automate the entire life cycle of the software it manages. </p> <p>A custom resource is the API extension mechanism in Kubernetes. A custom resource definition (CRD) defines a CR and lists out all of the configuration available to users of the operator. </p> <p>Operator is a customized controller implemented with a CRD. It follows the same pattern as built-in controllers (i.e. watch, diff, action).</p>"},{"location":"ocp/operators/#how-does-a-kubernetes-operator-work","title":"How Does a Kubernetes Operator Work?","text":"<p>Operators use controllers that monitor Kubernetes objects. These controllers are slightly different from regular Kubernetes controllers, because they track custom objects, known as custom resource definitions (CRDs). A CRD is an extension of the Kubernetes API that provides a place to store and retrieve structured data (the desired application state). </p> <p>Operators track cluster events related to specific types of custom resources. These custom resources can track three types of events:</p> <ul> <li>Add</li> <li>Update</li> <li>Delete</li> </ul> <p>When the operator receives the information, it takes action to bring the Kubernetes cluster or external system to the desired state as part of the custom controller scaling cycle.</p> <p>To summarize a Kubernetes operator\u2019s workflow:</p> <ol> <li>User makes changes to a CRD</li> <li>The operator tracks the CRD and identifies change events</li> <li>The operator reconciles the CRD state with the desired state</li> <li>The operator adjusts cluster state to the desired state</li> </ol> <p>The following illustration shows the steps Kubernetes uses in the Operator's workflow.</p> <p></p>"},{"location":"ocp/operators/#create-a-crd","title":"Create a CRD","text":"<p>The manifest below shows an example CRD <code>crd.yaml</code>:</p> <p><pre><code>apiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: appconfigs.stable.example.com\nspec:\n  group: stable.example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n  scope: Namespaced\n  names:\n    plural: appconfigs\n    singular: appconfig\n    kind: AppConfig\n    shortNames:\n    - ac\n</code></pre> Run <code>kubectl create -f crd.yaml</code> to create the CRD.</p> <p>To use the CRD, create a manifest using the kind we created with the CRD. For example <code>my-kind.yaml</code>:</p> <pre><code>apiVersion: \"stable.example.com/v1\"\nkind: AppConfig\nmetadata:\n  name: demo-appconfig\nspec:\n  uri: \"some uri\"\n  Command: \"some command\"\n  image: my-image\n</code></pre> <p>To use the AppConfig, run <code>kubectl create -f my-kind.yaml</code>.</p> <p>When you want to create your own custom resource resource definition, you will want to start with [Operator SDK].</p>"},{"location":"ocp/operators/#operator-framework","title":"Operator Framework","text":"<p>The Operator Framework includes:</p> <ul> <li>Operator SDK: Enables developers to build operators based on their expertise without requiring knowledge of Kubernetes API complexities.</li> <li>Operator Lifecycle Management: Oversees installation, updates, and management of the lifecycle of all of the operators running across a Kubernetes cluster.</li> <li>Operator Metering: Enables usage reporting for operators that provide specialized services.</li> </ul>"},{"location":"ocp/operators/#operatorio","title":"Operator.io","text":"<p>OperatorHub.io is a new home for the Kubernetes community to share Operators. </p> <p></p>"},{"location":"ocp/operators/#redhat-operatorhub","title":"RedHat OperatorHub","text":"<p>Red Hat has its own Operator Hub within OpenShift that are groups into categories:</p> <ul> <li>Red Hat Operators, supported by Red Hat</li> <li>Certified Operators, from ISVs</li> <li>Red Hat Marketplace, can be purchsed</li> <li>Community Operators with no official support</li> <li>Custom Operators, roll your own</li> </ul> <p>The following illustration shows how to find the OpoeratorHub in the OpenShift portal.</p> <p></p>"},{"location":"ocp/operators/#operator-lifecycle-manager","title":"Operator lifecycle manager","text":"<p>Operators on OperatorHub are packaged to run on Operator Lifecycle Manager (OLM).</p> <p>The level of sophistication of the management logic encapsulated within an Operator can vary. This logic is also in general highly dependent on the type of the service represented by the Operator.</p> <p></p>"},{"location":"ocp/operators/#prerequisites","title":"Prerequisites","text":"<p>You will need either:</p> <ul> <li>RedHat OpenShift on IBM Cloud (ROKS) cluster. You can use one from TechZone.</li> <li>Homebrew (yes it works on WSL2)</li> <li><code>oc</code> installed locally.</li> <li>Must be cluster administrator or allow non-cluster administrators to install Operators.</li> <li>Operator SDK CLI. Use <code>brew install operator-sdk</code></li> <li>[Optional] Tree using <code>brew install tree</code> </li> </ul> <p>Kubernetes 1.17-1.21 OpenShift 3.11, 4.3-4.7</p> <p>Log in to OpenShift.</p>"},{"location":"ocp/operators/#install-the-jenkins-operator-using-the-ui","title":"Install the Jenkins Operator using the UI","text":"<p>Jenkins Operator is a Kubernetes native operator which fully manages Jenkins on Openshift. It was built with immutability and declarative configuration as code in mind. It is meant to replace the Jenkins on Openshift Template as the primary source of deploying Jenkins on Openshift while also easing the Operational work necessary to manage the Jenkins Instance(s) on an Openshift Cluster.</p> <p>Out of the box it provides:</p> <ul> <li>Integration with Kubernetes</li> <li>Pipelines as code</li> <li>Extensibility via groovy scripts or configuration as code plugin</li> <li>Security and hardening</li> <li>Backup and restore</li> </ul> <p>After you log in, use:</p> <pre><code>oc new-project test-jenkins\n</code></pre> <p>Go to OperatorHub and select Jenkins Operator. Fill in using the <code>test-jenkins</code> namespace.</p> <p>Click Installed Operators.  Click Pods.</p> <p>To get the route to Jenkins</p> <pre><code>oc get svc\n</code></pre> <p>In the UI, in RedHat OpenShift Container Platform UI under Administrator, view the Deployments, Pods, Secrets (filter on <code>jenkins</code>).</p> <p>Follow the example docs here</p> <p>You now have Jenkins installed as an Operator with the Automatic strategy using the Operator Lifecycle Manager (OLM) to automatically update the Operator when a new version is available.</p>"},{"location":"ocp/operators/#getting-started-to-build-a-custom-operator-with-helm","title":"Getting started to build a custom Operator with HELM","text":"<p>You can build your own custom operator. You would begin with the Operator SDK. </p> <p><code>cd</code> into your directory and then use the following:</p> <pre><code>mkdir nginx-operator\ncd nginx-operator\noperator-sdk init --domain example.com --plugins helm\n</code></pre> <p>The code initializes a new project including vendor/ directory and Go package directories.</p> <p>Writes the following files:</p> <ul> <li>Boilerplate license file</li> <li>PROJECT file with the domain and repo</li> <li>Makefile to build the project</li> <li>go.mod with project dependencies</li> <li>Kustomization.yaml for customizating manifests</li> <li>Patch file for customizing image for manager manifests</li> <li>Patch file for enabling prometheus metrics</li> <li>main.go to run</li> </ul> <p>Create a simple nginx API using Helm\u2019s built-in chart boilerplate (from helm create):</p> <pre><code>operator-sdk create api --group demo --version v1alpha1 --kind Nginx\n</code></pre> <p>Use <code>tree</code> to see what it created</p> <pre><code>tree\n</code></pre> <p>Open the crd base file for your new example:</p> <pre><code>nano config/crd/bases/demo.example.com_nginxes.yaml\n</code></pre> <p>For more information, see Quickstart for Helm-based Operators</p>"},{"location":"ocp/operators/#next-step-to-create-your-custom-operator","title":"Next step to create your custom Operator","text":"<p>See Operator Reconciliation: \"You can't always get what you want... or can you?\". See Operator Framework tutorial in IBM documents.</p>"},{"location":"ocp/operators/#news","title":"News","text":"<p>Cassandra to Operator</p>"},{"location":"ocp/operators/#next-steps","title":"Next steps","text":"<p>Try Install, Configure and Expose MongoDB on IBM Cloud Pak for Data (ICP4D)</p> <p>Try Express installation of Cloud Pak for Integration</p>"},{"location":"ocp/operators/#references","title":"References","text":"<ul> <li>What are Operators? in OpenShift documentation</li> <li>What is a Kubernetes operator? in RedHat documentation</li> <li>CNCF Operator White Paper</li> <li>O\u2019Reilly ebook Kubernetes Operators: Automating the Container Orchestration Platform</li> <li>Google Cloud about best practices for building Operators</li> <li>Operator Framework</li> <li>How to install TraderLite Operator</li> <li>From Aqua Kubernetes Operators: How they Work and 6 Operators to Try</li> <li>Operator APIs for OpenShift</li> </ul>"},{"location":"ocp/postgresql/","title":"Deploy Cloud Native PostgreSQL","text":"<p>In this tutorial, you will deploy Cloud Native PostgreSQL directly using the Operator manifest using <code>kubectl</code>. </p> <p>NOTE: This Operator will not show up in Installed Operators in OpenShift control panel. It needs the OperatorGroup.</p> <p>The operator can be installed like any other resource in Kubernetes, through a YAML manifest applied via kubectl.</p> <p>You can install the latest operator manifest as follows:</p> <pre><code>kubectl apply -f \\\n  https://get.enterprisedb.io/cnp/postgresql-operator-1.8.0.yaml\n</code></pre> <p>Once you have run the kubectl command, Cloud Native PostgreSQL will be installed in your Kubernetes cluster.</p> <p>You can verify that with:</p> <pre><code>kubectl get deploy -n postgresql-operator-system postgresql-operator-controller-manager\n</code></pre> <p>For more information, see EDB docs Cloud Native PostgreSQL</p>"},{"location":"ocp/security/","title":"Security guidance","text":""},{"location":"ocp/security/#kubernetes","title":"Kubernetes","text":"<p>See CISA and NSA Release Kubernetes Hardening Guidance</p>"},{"location":"ocp/security/#openshift","title":"OpenShift","text":"<p>See Red Hat OpenShift security guide ebook.</p>"},{"location":"ocp/security/#updated","title":"Updated","text":"<p>Sep 2022</p>"},{"location":"ocp/stocktrader/","title":"Set up IBM Stock Trader demo","text":"<p>The IBMStockTrader application demonstrates how to build a cloud-native application out of a set of containerized microservices (each in their own repo under this org) that will run in Kubernetes. </p> <p>The IBM Stock Trader application is a simple stock trading sample where you can create various stock portfolios and add shares of stock to each for a commission. It keeps track of each porfolio's total value and its loyalty level which affects the commission charged per transaction. It sends notifications of changes in loyalty level. It also lets you submit feedback on the application which can result in earning free (zero commission) trades, based on the tone of the feedback.</p> <p>The following diagram shows how the microservices fit together, and what external services (databases, messaging products, API/function services, etc.) they utilize.</p> <p></p> <p>Note that only the services with a solid border are mandatory - the rest are all optional, only installed when you want to enable additional bells and whistles.</p>"},{"location":"ocp/stocktrader/#install-using-helm","title":"Install using HELM","text":"<p>See General preparation for installation</p>"},{"location":"ocp/stocktrader/#prerequisites","title":"Prerequisites","text":"<p>The prerequisites for deploying the HELM operator and the operator in the next section are explained here:</p> <ul> <li>Install and configure DB2</li> <li>Install and configure MQ</li> <li>Install and configure ODM</li> <li>Install and configure Redis</li> <li>Install and configure Tone Analyzer</li> <li>Install stock API with API Connect</li> </ul>"},{"location":"ocp/stocktrader/#install-stocktrader-helm-chart","title":"Install stocktrader Helm chart","text":"<p>See Install stocktrader Helm chart</p>"},{"location":"ocp/stocktrader/#install-using-an-operator","title":"Install using an operator","text":"<p>The IBM Stock Trader Operator is a simple stock trading sample where you can create various stock portfolios and add shares of stock to each for a commission.</p> <p>It keeps track of each porfolio's total value and its loyalty level which affects the commission charged per transaction. It is a derivation of IBM Stock Trader sample.</p>"},{"location":"ocp/stocktrader/#capabilities","title":"Capabilities","text":"<p>This operator is intended to install all of the microservices from the IBM Stock Trader sample, and configure them to talk to services they require.</p> <p>The operator can be deployed on IBM Cloud and other hyperscalers.</p>"},{"location":"ocp/stocktrader/#prerequisites_1","title":"Prerequisites","text":"<p>You will need an instance of DB2, and API Connect to run. Additional features may require additional features in Cloud Pak for Integration.  </p> <p>To build it from scratch, you will need the Operator SDK.</p>"},{"location":"ocp/stocktrader/#build","title":"Build","text":"<p>You can build the operator yourself or you can use the one developed in DockerHub.</p> <p>Deploy the operator, and its CRD that you will need to do a bit of hand editing as shown in the documentation.</p> <p>Once you install the operator, you can use the OpenShift UI to edit the features of Stock Trader you want to use.</p>"},{"location":"ocp/stocktrader/#references","title":"References","text":"<ul> <li>Outdated but useful: Using an umbrella helm chart to deploy the composite IBM Stock Trader sample</li> </ul>"},{"location":"ocp/value/","title":"Red Hat Openshift business value","text":"<p>Containers is the way to deploy applications into production into data centers. Containers offers several key advantages over deploying applications into virtual machines. From a developer point of view, containers are the answer, because:</p> <ul> <li>Containers are simple</li> <li>Containers are lightweight</li> <li>With containers, I can avoid the \"works on my machine\" effect</li> <li>There is a large community about containers</li> <li>Containers are extensible</li> <li>Containers are Cloud-ready</li> </ul>"},{"location":"ocp/value/#data-center-efficiency","title":"Data center efficiency","text":"<p>In the data center -- the same way server virtualization capitalized physical machines, containers enable capitalization of virtual machines (VMs), resulting in a significant reduction in overhead and improved efficiency of the underlying hardware.</p> <p>The following diagram shows a comparison of memory efficiency for production workloads:</p> <p></p> <p>For more information, see Legacy VM to Container Sizing for Red Hat OpenShift</p>"},{"location":"ocp/value/#how-do-you-orchestrate-lots-of-containers","title":"How do you orchestrate lots of containers?","text":"<p>But how do you orchestrate containers at scale -- in your data center?</p> <p>Container orchestration software automates containers and lets developers configure them.</p> <p>Several solutions to how to automate the number, sizing, and networking for container, but over time Kubernetes won.</p> <p>Yet --</p>"},{"location":"ocp/value/#kubernetes-is-complex","title":"Kubernetes is complex","text":"<p>Let's start with the CNCF Cloud Native Interactive Landscape. And doing it right means selecting and implementing many of the technologies shown in the diagram. </p> <p></p> <p>Kubernetes done right is hard.</p> Install Deploy Harden Operate - Templating- Validation- OS Setup - Identity, security access- App monitoring, alerts- Storage, persistence- Egress, ingress, integration - Host container images- Build/Deploy methodology - Platform monitoring, alerts- Metering, chargeback- Platform security hardening- Image hardening- Security certifications- Network policy- Disaster recovery-Resource segmentation - OS upgrade, patch- Platform upgrade, patch- Image upgrade, patch- App updrade, patch- Security patches- Continuous security scanning- Multli envrionment rollout- Enterprise container registry- Cluster and app elasticity- Montior, alert, remediate- Log aggregation"},{"location":"ocp/value/#openshift-is-built-on-top-of-kubernetes","title":"OpenShift is built on top of Kubernetes","text":"<p>OpenShift provides value added features, selected open source Cloud Native solution and ties them together as a supported platform.</p> <p></p> <p>OpenShift IS Kubernetes, 100% Certified by the CNCF. Certified Kubernetes is at the core of OpenShift. Users of <code>kubectl</code> love its power, once they are done with the learning curve. Users transitioning from an existing Kubernetes Cluster to OpenShift frequently point out how much they love redirecting their kubeconfig to an OpenShift cluster and have all of their existing scripts work perfectly. </p> <p>For more information, see A Guide to Enterprise Kubernetes with OpenShift</p> <p>OpenShift adds values to Kubernetes by selecting key technologies that make the orchestration of containers more secure, scalable, and friendly to developers.</p>"},{"location":"ocp/value/#openshift-offers-managed-and-self-managed-options-across-multiple-clouds","title":"OpenShift offers managed and self-managed options across multiple clouds","text":"<p>Red Hat offers both managed services and self-managed options, giving you the flexibility to choose where and how to deploy Red Hat OpenShift to meet your needs\u2014supported by the same foundation of Red Hat Enterprise Linux\u00ae and core Kubernetes.</p> <p></p> <p>For a comparision of managed and self-managed options, see Red Hat OpenShift: Container technology for hybrid cloud.</p>"},{"location":"ocp/value/#openshift-features","title":"OpenShift features","text":"<p>OpenShift is a cloud-based container orchestration platform that runs on Linux and includes multiple additional features and access controls, which some businesses may deem more important.</p> <p>The platform is designed to support better scaling and efficiency for cloud-based development on IBM, Amazon and other enterprise cloud services.</p> <p>Key features of OpenShift are:</p> <ul> <li>CI/CD pipeline definitions are standardized for easier integration and scaling</li> <li>Includes default container automation tools</li> <li>Offers the Kubernetes' command line interface (CLI), \u201ckubect1\u201d in addition to \u201coc,\u201d OpenShift's CLI</li> <li>OpenShift has become an integral building-block to application development because of its ease of compatibility with most cloud platforms</li> <li>Enables easy migration of container applications to the cloud</li> <li>Supports Kubernetes features and the Kubernetes platform, but with greater security features</li> <li>Builds upon line stability and user access with comprehensive route and access controllers</li> <li>Security features align with compliance regulations</li> <li>Technical support for production environments</li> </ul>"},{"location":"ocp/value/#key-value-adds-by-openshift","title":"Key value adds by OpenShift","text":"<p>OpenShift provides an opinionated set of technology that are add-ons to Kubernetes -- things you need to do to support Kubernetes that are not available in the box. OpenShift provides a matched set (versioned to work together) of key technologies to make Kubernetes safe and scalable in production environments.</p> <p>Some of the features have been adopted by the Kubernetes itself or are part of CNCF projects. </p> <p>Here are key features that OpenShift provides out-of-the-box that provide for production-ready Kubernetes:</p> <ul> <li>Routes that enabling external access to services, but is easier to implement than Ingress object. Routes also work with the built in load balancer or you can bring your own, such as NGINX, NGINX Plus, or F5 BIG-IP. </li> <li> <p>Project provides additional annotations to the Kubernetes namesapce. Projects have:</p> <ul> <li>Objects. Pods, services, replication controllers, etc.</li> <li>Policies. Rules for which users can or cannot perform actions on objects.</li> <li>Constraints. Quotas for each kind of object that can be limited.</li> <li>Service accounts. Service accounts act automatically with designated access to objects in the project.</li> </ul> </li> <li> <p>Operators and an operator library. Operators automate the creation, configuration, and management of instances of Kubernetes-native applications. Operators provide automation at every level of the stack\u2014from managing the parts that make up the platform all the way to applications that are provided as a managed service. Cloud Paks are delivered as Operators.</p> </li> <li>Service mesh based on the Istio that provides a uniform way to connect, manage, and observe microservices-based applications. Connect services securely by default with transparent TLS encryption. Enforce a \"zero trust\" or \"need to know\" network security model with fine-grained traffic policies based on application identities. </li> <li> <p>CoreOS control plane as the base operating system that is immutable. </p> <ul> <li>RHCOS provides controlled immutability: Many parts of the system can be modified only through specific tools and processes.</li> <li>OpenShift nodes are remotely managed by the cluster itself. The administrator does not need to deploy upgrades or patches to the node: The nodes are updated by the cluster itself.</li> <li>The operating system starts from a generic disk image which is customized on the first boot via a process named Ignition.</li> <li>RHCOS can be customized with uses JSON formatted files to define the customization information.</li> </ul> </li> <li> <p>Containers can not get root access. But you can debug and investigate pod issues without it.</p> </li> <li>CI/CD pipeline that is relatively easy to deploy.</li> <li>Secure the build process to nsures that the product of the build process is exactly what is deployed in production.</li> <li>Use Tekton Chains for OpenShift Pipelines supply chain security to manage the supply chain security of the tasks and pipelines.</li> <li>Configure identity providers such as LDAP, request header, GitHub, GitLab, OpenID Connect.</li> <li>Storage. OpenShift Container Platform uses a pre-provisioned storage framework called persistent volumes (PV) to allow cluster administrators to provision persistent storage. Use Dynamic Provisioning to create storage volumes on-demand, eliminating the need for cluster administrators to pre-provision storage.</li> <li>Network policy for you to restrict traffic to pods in your cluster. </li> <li>Monitoring stack based on Prometheus and its wider ecosystem. The monitoring stack includes the following: Default platform monitoring components and Components for monitoring user-defined projects.</li> </ul>"},{"location":"ocp/value/#managed-services","title":"Managed Services","text":"<p>Deploy Kubernetes more easily with Red Hat OpenShift managed services Optimized to improve developer productivity and promote innovation, Red Hat\u00ae OpenShift\u00ae is an enterprise-ready Kubernetes container platform with full-stack automated operations for managing hybrid cloud, multicloud, and edge deployments. Four Red Hat OpenShift managed cloud services are available, so you can choose the option that best fits your organization\u2019s needs. </p> Managed service Runs on... Managed and supported by... Billed through... Red Hat OpenShift Dedicated AWS or Google Cloud Red Hat Red Hat for Red Hat OpenShift use and AWS or Google Cloud for cloud use Red Hat OpenShift Service on AWS AWS Red Hat and AWS AWS Azure Red Hat OpenShift Microsoft Azure Red Hat and Microsoft Microsoft Red Hat OpenShift on IBM Cloud IBM Cloud Red Hat (support) and IBM (support and management) IBM <p>Each service offers more than just access to managed software and technologies. They provide complete, full-stack environments with all necessary services, simple self-service use options, and expert 24x7 support via stringent service level agreements (SLAs).</p> <p>For more information, see Achieve more with Red Hat OpenShift managed services</p>"},{"location":"ocp/value/#next-steps","title":"Next steps","text":"<p>See:</p> <ul> <li>Sales Play guide Red Hat Ansible and OpenShift</li> </ul>"},{"location":"ocp/value/#references","title":"References","text":"<ul> <li>White paper: Business value of Red Hat OpenShift. This white paper shares the survey findings, which include 636% ROI over five-years and the ability to ship almost 3 times more new features. The whitepaper details how the organizations achieved annual value worth an average of $10.59 million.</li> <li>Red Hat OpenShift Solution Design Guidance: Deliverables, OpenShift Value Proposition, and OpenShift 4.4 Key Features.pdf</li> <li>Red Hat OpenShift Solution Design Guidance: OpenShift Value Proposition</li> <li>Ebook: Red Hat OpenShift security guide</li> </ul>"},{"location":"ocp/value/#contributors","title":"Contributors","text":"<ul> <li>Lorraine Conway</li> <li>IBM Cloud Education</li> <li>Red Hat OpenShift documentation</li> </ul>"},{"location":"ocp/value/#updated","title":"Updated","text":"<p>Aug 2022</p>"},{"location":"ocp/openshiftlocal/install/","title":"Install OpenShift Local on Windows","text":"<p>Red Hat OpenShift Local brings a minimal OpenShift Container Platform 4 cluster and Podman container runtime to your local computer. These runtimes provide minimal environments for development and testing purposes. Red Hat OpenShift Local is mainly targeted at running on developers' desktops. </p> <ol> <li>Go to cloud.redhat.com/openshift/install</li> <li>Sign in with your Red Hat username and password.</li> <li>Select Run on Laptop.</li> <li>Select your Operating System and select Download Code-Ready Containers. This will download a file such as crc-windows-amd64.zip.</li> <li>Select Download pull secret. This will download a file such as pull-secret.txt.</li> </ol> <p></p> <p>Extract (unzip) <code>crc-windows-amd64.zip</code>. This will create a new folder, such as <code>crc-windows-&lt;version&gt;-amd64</code>. </p> <p>Run the installer application in the folder. Follow the prompts of the installer.</p> <p>Run the following commands to set up CRC and then start.</p> <pre><code>cd \\\ncrc setup --log-level debug\nStart-ScheduledTask -TaskName crcDaemon\ncrc start\n</code></pre> <p>NOTE: <code>Start-ScheduledTask -TaskName crcDaemon</code> fixes and issue where <code>crc setup</code> replies after several messages with <code>DEBU expected crcDaemon task to be in 'Running' but got 'Ready '</code></p> <p>Follow the instructions at the end of <code>crc start</code>:</p> <pre><code>Started the OpenShift cluster.\n\nThe server is accessible via web console at:\n  https://console-openshift-console.apps-crc.testing\n\nLog in as administrator:\n  Username: kubeadmin\n  Password: PD6Ls-CpBG5-Y6ImF-6LyVY\n\nLog in as user:\n  Username: developer\n  Password: developer\n\nUse the 'oc' command line interface:\n  PS&gt; &amp; crc oc-env | Invoke-Expression\n  PS&gt; oc login -u developer https://api.crc.testing:6443\n</code></pre>"},{"location":"ocp/openshiftlocal/install/#set-environment-variables","title":"Set environment variables","text":"<p>Open PowerShell. Run</p> <pre><code>cd \\\ncrc oc-env\n</code></pre> <p>Then run each of the command shown.</p> <pre><code>$Env.blah blah\n</code></pre>"},{"location":"ocp/openshiftlocal/install/#references","title":"References","text":"<ul> <li>OpenShift - Install OpenShift CodeReady Containers on Windows</li> <li>Setting up Red Hat OpenShift Local</li> </ul>"},{"location":"ocp/openshiftlocal/uninstall/","title":"Uninstall or Update OpenShift Local / Code Ready Containers (CRC) on Windows","text":"<p>You will need to uninstall OpenShift Local when you want to:</p> <ul> <li>Use Podman</li> <li>Update to a new version</li> <li>Remove to reset machine to known state</li> </ul> <p>This assumes you have installed OpenShift CodeReady Containers on Windows. </p> <p>Close/Exit the CRC in the tool tray</p> <p>To remove, use </p> <pre><code>crc.exe status\ncrc.exe stop\ncrc cleanup --log-level debug\ncrc.exe delete # C:\\Program` Files\\Red` Hat` OpenShift` Local\\crc.exe delete\n</code></pre> <p>Then use Apps &amp; features to uninstall Red Hat OpenShift Local.</p> <p></p> <p>You can then delete your user's hidden <code>.crc</code> directory, <code>C:\\Users\\john.doe\\.crc</code>.</p>"},{"location":"ocp/openshiftlocal/uninstall/#update","title":"Update","text":"<p>To update, reinstall. See  Install OpenShift Local.</p>"},{"location":"ocp/openshiftlocal/uninstall/#references","title":"References","text":"<ul> <li>OpenShift - Uninstall OpenShift CodeReady Containers on Windows</li> <li>Upgrading Red Hat OpenShift Local</li> </ul>"},{"location":"ocp/rhel/installrhel/","title":"Install Red Hat Enterprise Linux on Windows","text":"<p>You will need:</p> <ul> <li>Hyper-V enabled.</li> <li>A Red Hat developer account. You will need this to register the system and attach it to your subscription. Completing these steps are required for your system to download software from Red Hat.</li> <li>The RHEL Binary <code>.iso</code> file. When you register and download through developers.redhat.com, a subscription will be automatically added to your account.</li> <li> <p>A Windows system that:</p> <ul> <li>can run Hyper-V, a 64-bit x86 machine with with hardware virtualization assistance (Intel VT-X or AMD-V) and Second Level Address Translation (SLAT).</li> <li>has Hyper-V installed and the Hyper-V platform is enabled. For more information see Install Hyper-V on Windows 10 or consult the documentation for your version of Microsoft Windows. </li> <li>is equipped with has at least 8 GB of RAM and has at least 28 GB of available disk space for the VM and the .iso file.</li> </ul> </li> </ul> <p>For instructions on how to install Red Hat Enterprise Linux on Windows, see RHEL 8 Hyper-V Quick Install.</p> <p>The following diagram shows the settings for RHEL in Hyper-V.</p> <p></p> <p>Some minor differences from the blog post in RHEL 9:</p> <ul> <li>2 CPU is set up after you complete the New Virtual Machine Wizard. Set it in the Hyper-V when you set the Security feature.</li> <li>Set users at Begin Installation.</li> <li>Not sure how you eject installation media.</li> <li>To register, use your Red Hat id, not your email address associated with your Red Hat id.</li> <li>To run OpenShift Local, you will need 16 gigs of RAM.</li> </ul>"},{"location":"ocp/rhel/installrhel/#run-nested-virtualization","title":"Run nested virtualization","text":"<p>From powershell, run</p> <pre><code>Set-VMProcessor -VMName &lt;VMName&gt; -ExposeVirtualizationExtensions $true\n</code></pre> <p>For more information, see Configure Nested Virtualization.</p>"},{"location":"ocp/rhel/installrhel/#references","title":"References","text":"<p>RHEL 8 Hyper-V Quick Install</p>"},{"location":"supplychain/demandrisk/","title":"Demand risk","text":"<p>To demonstrate the importance of inventory optimization for any business, we will focus on one of the main demand risk use cases: understock and overstock. We will start by defining the business problem and the two main use cases: understock and overstock, describe the challenges and business drivers organizations face. Next, we will provide an action guide, provide an overview of the solution, show a schematic of the two use cases, understock and overstock, and conclude with the technology used in the solution.</p> <p>For a comprehensive inventory solution overview, see Inventory Optimization.</p>"},{"location":"supplychain/demandrisk/#business-problem","title":"Business problem","text":"<p>Inventory optimization is a critical element of any retail organisation's fulfilment process. It represents a balancing act between two key viewpoints in the fulfilment process.</p> <p>The first viewpoint is demand. The business needs to understand their current demand for products, goods and services. There are two aspects of demand, the \u201ccurrent\u201d demand and the \u201cfuture\u201d demand. Current demand asks the question, \u201chow many unsatisfied orders or requests do we have in the system at this point in time?\u201d Future demand asks the more difficult question, \u201chow many orders or requests do we expect to have at some point in the future?\u201d This future point could be measured in minutes, hours, days, weeks, months and even years. The further into the future, the harder it becomes to predict the demand.</p> <p>The second viewpoint is inventory. The business needs to have a clear understanding of all the inventory held within their fulfilment system. An inventory management provides oversight of current inventory and inventory changes due to purchases. Performs analysis on sold products, keeps consolidated stock data, and handles stock allocation. Makes decision on reallocation of goods between multiple warehouses. Inventory is stock or inventory available for sale to customers. As with demand, inventory has two aspects, \u201ccurrent\u201d and \u201cfuture\u201d inventory. Current inventory asks two questions, \u201chow many items of a particular product do I have available at this point in time?\u201d and \u201cwhere are those items located?\u201d Future inventory asks the question, \u201chow much inventory is required at some point in the future\u201d. As with demand, this future point could range from minutes to years and becomes increasingly harder to predict the further into the future. </p> <p>Inventory optimization is making sure the current and future demand is accurately balanced against current and future inventory. Getting the balance correct leads to a successful and profitable retail business. Getting the balance wrong leads to failure and in the worst case, eventual collapse of the business.</p> <p>To demonstrate the importance of inventory optimization for any retail business, this document will focus on main use cases of demand and inventory.</p>"},{"location":"supplychain/demandrisk/#use-cases","title":"Use cases","text":"<p>Two main issues are represented by demand risk.</p> <p>Understock refers to not holding sufficient inventory to meet current demand. This includes not having enough inventory today but also, not having enough inventory in the very near future that could be used to meet the demand. The end result is disappointed customers who order but don\u2019t get fulfillment or can\u2019t order due to lack of product. This \u201cstock out\u201d position often represents between 4-8% of total sales lost, but can also be a failed opportunity to satisfy customer in other ways, through up-sell and cross-sell. KPIs that can help avoid an under stock or stock-out position. include inventory turn over rate, days on hand and lead time (how long it takes to get more inventory from a supplier).</p> <p>Overstock refers to holding more stock than required to meet current and future demand. This results in additional costs to store then dispose of overstocked items via discounts, selling at a loss or destruction. Whilst understock is measured in customer satisfaction and loss of future opportunity, overstock has direct impact on the bottom-line costs and profitability of the business. KPIS relevant to overstock include, holding costs, dead stock (in-stock items failing to sell) and inventory turnover rates.</p>"},{"location":"supplychain/demandrisk/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <ul> <li>Maintaining inventory control of high value items to minimize loss and associated cost.</li> <li>When stores receive inventory from warehouse or direct from suppliers, how to manage direct ship is a real challenge for each store as it has to be managed at the store level.</li> <li>Forecasting inventory levels intelligently to meet customer demand.</li> <li>Efficiently handle overstocking and understocking events </li> </ul> <p>Drivers</p> <ul> <li>Inventory turnover - if improve by 2 or 3 times then will drive bigger profit</li> <li>Managing capacity - across the enterpriese and with suppliers</li> <li>Enhanced customer experience with inventory matching customer demnd</li> <li>Handling overstock and understock events</li> </ul>"},{"location":"supplychain/demandrisk/#responses","title":"Responses","text":"Business Problem Solution Unusual events, such as the global pandemic, war or other international incident, port issues, and waterway obstructions illustrate the need for enterprises to build resilient supply chains. Respond with intelligence, speed, and confidence to reduce the impact of disruptions, turning these events into opportunities to outperform and outcompete. Manual processes, limited capabilities of inventory management tools, and global operations pose a challenge for enterprises to manage and act on inventory and mitigate disruptions to meet actual demand. Monitor and manage network inventory availability and respond to disruptions such as out-of-stock and overstock with alerts and recommended actions. The lack of pertinent product information and poor data flow across partners lead to inefficient inventory management, waste, and lost sales. Gain detailed visibility into inventory characteristics at each location."},{"location":"supplychain/demandrisk/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Improve inventory demand and forecasting</li> <li>Automated processes updating inventory in closer to real time</li> <li>Efficient, consistent decision making of overstock and understock</li> </ul>"},{"location":"supplychain/demandrisk/#solution-overview","title":"Solution overview","text":"<p>This solution in the following diagram reflects steps in the Action Guide:</p> <ul> <li>Create a world-class sensing and risk-monitoring operation. </li> <li>Accelerate automation in extended workflows</li> <li>Amp up AI to make workflows smarter</li> <li>Include sustainability commitments in decision making </li> <li>Modernize for modern infrastructures, scale hybrid cloud platforms</li> </ul> <p></p> <p>The solution uses the following technologies, which can be grouped into three main categories as shown in the following diagram:</p> <ul> <li>Core application systems. Often customer-provided technologies, such as order management, facilities management. These systems can be stand-alone applications, on premises and cloud services, databases. </li> <li>Foundational infrastructure. The Red Hat/IBM solution is built on Red Hat OpenShift. Data is routed through API management. Events are routed through Business Automation tools such as Business Automation Workshop. </li> <li>Inventory Optimization platform</li> </ul>"},{"location":"supplychain/demandrisk/#understock","title":"Understock","text":"<p>The following diagram shows the schematic for the understock use case.</p> <p></p> <p>Understock workflow steps:</p> <ol> <li>Inventory Analysis detects low stock levels and predicts inventory will become unavailable sooner than originally expected. <li>Inventory Control Tower alerted to the understock position. <li>Inventory Control Tower collects current inventory positions from stores, in-transit, warehouses plus future inventory positions <li>Inventory Control Tower collects future demand requirements from Demand Intelligence. <li>Colleague alerted and asked to take remediation action. <li>Colleague triggers Business Automation to remediate stock levels using a combination of options, including: <ol><li>Ordering more stock <li>Adjusting stock positions within existing Supply Chain <li>Managing inventory held at existing stores or by moving existing inventory <li>Managing inventory held at existing warehouses or by moving existing inventory"},{"location":"supplychain/demandrisk/#overstock","title":"Overstock","text":"<p>The following diagram shows the schematic for the overstock use case.</p> <p></p> <p>Overstock workflow steps:</p> <ol> <li>Inventory Analysis detects high stock levels and predicts inventory will not be sold as quickly as expected.</li> <li>Inventory Control Tower alerted to the overstock position.</li> <li>Inventory Control Tower collects current inventory positions from stores, in-transit, warehouses plus future inventory positions.</li> <li>Inventory Control Tower collects future demand requirements from Demand Intelligence.</li> <li>Colleague alerted and asked to take remediation action.</li> <li>Colleague triggers Business Automation to remediate stock levels using a combination of options, including:</li> <ol><li>Product discounting and promotions.</li> <li>Adjust stock positions within stores by moving stock between stores or warehouses to accelerate sales in conjunction with promotions.</li> <li>Reduce future inventory requirements to slow down or stop replenishment.</li> <li>Offering stock to partners for liquidation, destruction, donation, sale via alternative channels or to food-waste partners.</li> </ol> </ol>"},{"location":"supplychain/demandrisk/#supply-assurance-platform-details","title":"Supply Assurance Platform details","text":"<p>With Inventory Control Tower, you:</p> <ul> <li> <p>View. End to end supply chain coverage</p> <ul> <li>Visibility across siloed data sources</li> <li>External data Track &amp; trace</li> </ul> </li> <li> <p>Detect. Work \u2013 queues of prioritized issues</p> <ul> <li>KPIs based on business rules and alerts</li> <li>Analytics using AI and machine learning</li> </ul> </li> <li> <p>Guide. Determine best approach for the situation</p> <ul> <li>Defined best practice solutions</li> <li>Context and recommendations</li> </ul> </li> <li> <p>Act. Quick, efficient and uniform problem resolution</p> <ul> <li>Intelligent workflows with guidance</li> <li>Automation to back-end systems</li> </ul> </li> </ul> <p>Respond faster to changes, enable efficient collaboration and decision support, and drive operational automation with Control Tower.</p> Use Case The Problem The Solution The Benefits and Implications Reduce out of stock (OOS) or approaching out of stock (AOOS)conditions Out of stock situations lead to lost revenue and decreased brand / retailer loyalty. SCIS Control Tower monitors inventory levels at all locations in a client's network and creates items in the work queue when revenue is at risk. When drilling down on the item, users can see where they have available inventory and receive recommendations about how much inventory can and should be transferred to the OOS / AOOS locations. Action can be taken directly from the Control Tower user interface. OOS situations are efficiently managed and AOOS are avoided with minimal human intervention. Manage industrial and manufacturing critical supplies Out of stock situations lead to line outages, manufacturing delays, and lost revenue. SCIS Control Tower monitors inventory to requested demand and creates items in the work queue when delivery is at risk. When drilling down on the item, users can see parts by SKU and location to see which supply is at risk take action to minimize impact. Minimize production and parts impact due to OOS / AOOS situations. Increase throughput and minimize customer delays. Minimize expedited and remediation costs. <p>Inventory is managed by exception. Manage and predict inventory exceptions such as: low inventory, stockouts, slow moving and aging inventory. Optimize inventory transfers to mitigate these circumstances.</p>"},{"location":"supplychain/demandrisk/#action-guide","title":"Action Guide","text":"<p>From a high-level perspective, there are several main steps your organization can take to drive innovation and move toward a digital supply chain:</p> <ul> <li>Automation</li> <li>Sustainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automation Create a world-class sensing and risk-monitoring operation Integrate data from multiple systems to get enterprise-wide view of changes in inventory demand. Monitor and analyze near real-time data Automation Accelerate automation in extended workflows As an example, in the Reduce out of stock (OOS) or approaching out of stock (AOOS) conditions, a SCIS Control Tower monitors inventory levels at all locations in a client's network and creates items in the work queue when revenue is at risk. Automation Amp up AI to make workflows smarter When users are inspecting inventory items by drilling down on the item, users see where they have available inventory and receive recommendations about how much inventory can and should be transferred to the OOS / AOOS locations. These recommendations are based on adding automation and AI to make workflows smarter. Sustainability Include sustainability commitments in decision making Integrate sustainability metrics in overstock and understock decision making. Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point."},{"location":"supplychain/demandrisk/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p> <p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security.</p> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.</p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"supplychain/demandrisk/#integeration-platform","title":"Integeration Platform","text":"<p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</p> <p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing.</p>"},{"location":"supplychain/demandrisk/#supply-assurance-platform","title":"Supply Assurance Platform","text":"<p>IBM Supply Chain Control Tower provides actionable visibility to orchestrate your end-to-end supply chain network, identify and understand the impact of external events to predict disruptions, and take actions based on recommendations to mitigate the upstream and downstream effects.</p> <p>IBM Sterling Intelligent Promising provides shoppers with greater certainty, choice and transparency across their buying journey. It includes:</p> <ul> <li>IBM Sterling Fulfillment Optimizer with Watson to determine the best location from which to fulfill an order, based on business rules, cost factors, and current inventory levels and placement</li> <li>Sterling Inventory Visibility to processes inventory supply and demand activity to provide accurate and real-time global visibility across selling channels.</li> </ul> <p>IBM Planning Analytics with Watson streamlines and integrates financial and operational planning across the enterprise.</p>"},{"location":"supplychain/demandrisk/#similar-use-cases","title":"Similar use cases","text":"<p>See:</p> <ul> <li>Inventory management</li> <li>Loss and waste management</li> <li>Product timeliness</li> <li>Intelligent order</li> <li>Returns</li> <li>Disaster readiness</li> </ul> <p>For a comprehensive supply chain overview, see Supply Chain Optimization.</p>"},{"location":"supplychain/demandrisk/#downloads","title":"Downloads","text":"<p>View and download all of the Inventory Optimization diagrams shown in previous sections in our open source tooling site.</p> <ul> <li>PowerPoint Solution Overview: Open Solution Overview</li> <li>PowerPoint Reference Architecture: Open Workflow Diagrams</li> <li>DrawIO: Open Schematic Diagrams</li> </ul>"},{"location":"supplychain/demandrisk/#contributors","title":"Contributors","text":"<ul> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Mike Lee, Principal Integration Technical Specialist, IBM</li> <li>James Stewart, Principle Account Technical Leader, IBM</li> <li>Bruce Kyle, Sr Solution Architect, IBM Client Engineering</li> <li>Mahesh Dodani, Principal Industry Engineer, IBM Technology</li> <li>Thalia Hooker, Senior Principal Specialist Solution Architect, Red Hat</li> <li>Jeric Saez, Senior Solution Architect, IBM</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> </ul>"},{"location":"supplychain/disasterreadiness/","title":"Disaster readiness","text":"<p>Companies are facing natural disasters and extreme weather conditions, including those impacted by:</p> <ul> <li>Climate change, such as hurricanes, typhoons, floods, earthquakes, wildfires, droughts</li> <li>Acts of terrorism or violence, including active shooter situations; energy shortages</li> <li>Public health issues, including pandemics and quarantines </li> </ul> <p>Each can negatively affect your supply chain operations and financial performance. </p> <p>In this article, we discuss how your organization can prepare for business disruption, including</p> <ul> <li>Preparations when a weather event is anticipated.</li> <li>How systems can work together to mitigate the business disruption when stores become disconnected from corporate data centers.</li> </ul>"},{"location":"supplychain/disasterreadiness/#use-case","title":"Use case","text":"<p>Events could result in physical damage to our properties, limitations on store operating hours, less frequent visits by members to physical locations, the temporary closure of warehouses, depots, manufacturing or home office facilities, the temporary lack of an adequate work force, disruptions to our IT systems, the temporary or long-term disruption in the supply of products from some local or overseas suppliers, the temporary disruption in the transport of goods to or from overseas, delays in the delivery of goods to our warehouses or depots, and the temporary reduction in the availability of products in our warehouses.</p> <p>In 2020, there were 22 separate billion-dollar weather and climate disasters in the US alone and you can understand why modeling the risk from weather is so critical to your operations. </p>"},{"location":"supplychain/disasterreadiness/#background","title":"Background","text":"<p>Business need to anticipate and respond to supply chain challenges due to unanticipated disasters.</p> <p>According to the Business Continuity Institute, 72% of suppliers who dealt with a breakdown in their supply chain lacked the full, real-time visibility needed to come up with a fast and simple solution.</p>"},{"location":"supplychain/disasterreadiness/#business-problem","title":"Business problem","text":"<p>A central feature is time to recover (TTR), the time it would take for a particular node \u2014 a supplier facility, a distribution center, or a transportation hub \u2014 to be restored to full functionality after a disruption.</p> <p>The second concept is risk exposure, an assessment of exposure to risk based on a model that applies suppliers' TTR to estimate the performance impact, including operational (lost production) and financial (lost revenue and profit).</p>"},{"location":"supplychain/disasterreadiness/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <p>Harvard Business Review article identified the following objectives for companies facing disaster situtations.</p> <ul> <li>Identify exposure to risk associated with parts and suppliers.</li> <li>Prioritize and allocate resources effectively.</li> <li>Invest in mitigation strategies such as booking logistics capacity.</li> </ul> <p>Drivers</p> <ul> <li>Monitor for disruptive environmental conditions such as severe weather, wildfires, flooding, air quality, and carbon emissions and receive alerts when detected.</li> <li>Predict potential impacts of climate change and weather across the business using climate risk analytics.</li> <li>Reconfigure supply chain Some critical but vulnerable suppliers will go out of business.</li> <li>Reposition inventory in anticipation of certain regions or facilities facing disaster.</li> <li>Route deliveries around impacted areas.</li> <li>Continuous operation when store are disconnected from corporate data center.</li> </ul>"},{"location":"supplychain/disasterreadiness/#responses","title":"Responses","text":"Business Problem Solution Unusual events, such as the global pandemic, war or other international incident, port issues, and waterway obstructions illustrate the need for enterprises to build resilient supply chains. Respond with intelligence, speed, and confidence to reduce the impact of disruptions, turning these events into opportunities to outperform and outcompete. Manual processes, limited capabilities of inventory management tools, and global operations pose a challenge for enterprises to manage and act on inventory and mitigate disruptions to meet actual demand. Monitor and manage network inventory availability and respond to disruptions such as out-of-stock with alerts and recommended actions. The lack of pertinent product information and poor data flow across partners lead to inefficient inventory management, waste, and lost sales. Gain detailed visibility into inventory characteristics at each location. Point of sales may not be connected to corporate office during a disaster Data can be queued and then sent to corporate at a later time"},{"location":"supplychain/disasterreadiness/#solution-overview","title":"Solution overview","text":"<p>The solution shown in Figure 1 uses components that can be grouped into three main categories as shown in the following diagram:</p> <ul> <li>Core application systems. Often customer-provided technologies, such as order management or facilities management. These systems can be standalone applications, on-premises and cloud services, and databases.</li> <li>Foundational infrastructure. The Red Hat/IBM solution is built on Red Hat OpenShift with data routed through API management and events routed through business automation tools such as Business Automation Workshop.</li> <li>Inventory Optimization platform consisting of a Supply Assurance Control Panel, Fulfillment Optimization, and Inventory Analysis &amp; AI.</li> </ul> <p></p> <p>Figure 1. Overall view of demand risk solution.</p>"},{"location":"supplychain/disasterreadiness/#logical-diagrams","title":"Logical diagrams","text":"<p>Figure 2. The personas and technologies that provide a platform for some of the biggest potential breakthroughs in the supply chain.</p>"},{"location":"supplychain/disasterreadiness/#architecture","title":"Architecture","text":"<p>The figures in this section show the interaction of customer systems with supply chain optimization platform systems in the context of a retail scenario with branch stores.</p>"},{"location":"supplychain/disasterreadiness/#weather-disruption-preparation","title":"Weather disruption preparation","text":"<p>Figure 3. Schematic diagram of weather disruption preparation use case.</p> <p>Weather disruption preparation workflow steps:</p> <ol> <li>External data feed, such as IBM Environmental Intelligence Suite, anticipated disruptive environmental condition. <li>Demand Intelligence system is alerted to the potential disruption. <li>Inventory analysis anticipates potential low stock levels and predicts extraordinary demand levels <li>Current inventory levels data collected <li>Control Tower collects current inventory positions from stores, in-transit, warehouses plus future inventory positions <li>Control Tower alerts colleage with a set of work queues to mitigate the disrutpion Colleague takes remediation action by selecting actions provided by Control Tower. <li>Colleague triggers Business Automation to remediate stock levels using a combination of options, including: <ol> <li>Ordering more stock in nearby and affected area. <li>Adjusting stock positions within existing Supply Chain. <li>Plan transport around affected area. <li>Coordinate with suppliers and vendors to position inventory"},{"location":"supplychain/disasterreadiness/#disconnected-store","title":"Disconnected store","text":"<p>The following scenario shows how data can be transmitted from a store to the data center as part of an overall solution to setting up and maintaining the compute facilities in a store or branch office .</p> <p></p> <p>Figure 4. Schematic diagram of disaster response with a disconnected store use case.</p> <p>Disconnected store workflow steps:</p> <ol> <li>Point of sale devices send information to in-store server that collects transactions. <li>Once connection is restored, store server queues events and plays them back as events.  <li>The transaction events are read, business automation workflows are triggered to update corporate systems. <li>Data is updated through business automation to: <ol> <li>Adjust stock positions data for the affected stores. <li>Update replentishment system. <li>Upste store operations data. <li>Consolidate data for to update work queue in Supply Chain Control Tower."},{"location":"supplychain/disasterreadiness/#action-guide","title":"Action Guide","text":"<p>From a high-level perspective, the Action Guide represents a future state for organizations considering a comprehensive commitment. The idea is to outline a set steps that can be prioritized to reach that future state by adding new functionality to your existing systems.</p> <ul> <li>Automation</li> <li>Sustainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automation <ul><li>Monitor for disruptive environmental conditions such as severe weather, wildfires, flooding, air quality, and carbon emissions<li>Predict potential impacts of climate change and weather across the business using climate risk analytics <ul><li>Gain insights into potential operational disruptions and prioritize mitigation and response efforts<li>Measure and report on environmental initiatives, while reducing the burden of this reporting on procurement and operations teams Automation Accelerate automation in extended workflows Prepare for severe weather-related shipping and inventory disruptions, or factor environmental risks into future warehouse locations Automation Amp up AI to make workflows smarter When users are inspecting inventory items by drilling down on the item, users see where they have available inventory and receive recommendations about how much inventory can and should be transferred. These recommendations are based on adding automation and AI to make workflows smarter. Automation Respond to disconnected store proactively Use available data to take actions to support disconnected store. Sustainability Include sustainability commitments in decision making Integrate sustainability metrics in disaster planning and response decision making. Sustainability Combine your proprietary and third-party geospatial information with weather data <ul><li>Predict energy demand and reduce waste so that you don\u2019t buy more power than you need.<li>Use weather information to predict and avoid disruptions and plot the most efficient routes. Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point. Modernization Modernize application deployment and operations practices <ul><li>Include DevOps best practices to deploy, monitor, and maintain applications<li>Use automation across the enterprise to maintain and update local store operations infrastructure Modernization Manage disconnected operations Computing capabilities and data can be mirrored in stores to maintain local data needed to support store operations, such as product catalogs, and transactions, to provide basic services<ul><li>Provide for data redundancy and high availability in local store operations<li>Provide a way to update transactions once connection is restored. <p>For specific steps on this approach, see The Action Guide details in Own Your Transformation survey of 1500 CSCOs across 24 industries.</p>"},{"location":"supplychain/disasterreadiness/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p>"},{"location":"supplychain/disasterreadiness/#core-platform","title":"Core platform","text":"<p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with:</p> <ul> <li>Red Hat Advanced Cluster Management</li> <li>Red Hat Advanced Cluster Security</li> </ul> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.</p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"supplychain/disasterreadiness/#integration-services","title":"Integration services","text":"<p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing. Rules processing, intelligent decison making, and regulation compliance using automation gives business flexible, auditable, policy-based workflows across the enterprise.</p> <p>IBM Data Fabric empowers your teams and works across the ecosystem by connecyting data from disparate data sources in multicloud envrionments. In particular, Watson Knowledge Catalog provides you users with a catalog tool for intelligent, self-service discovery of data, models. Watson Query provides data consumers with a universal query engine that executes distributed and virtualized queries across databases, data warehouses, data lakes, and streaming data without additional manual changes, data movement or replication. </p> <p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</p> <p>IBM MQ Advanced protects your growing volumes of data with superior security, including true end-to-end encryption. With IBM\u00ae MQ Advanced middleware, you can more easily implement reliable business continuity mechanisms to stay up and running. Share data across environments with ease; you can deploy certified containers for Red Hat\u00ae OpenShift\u00ae with one click.</p>"},{"location":"supplychain/disasterreadiness/#supply-assurance-platform","title":"Supply Assurance Platform","text":"<p>IBM Supply Chain Control Tower provides actionable visibility to orchestrate your end-to-end supply chain network, identify and understand the impact of external events to predict disruptions, and take actions based on recommendations to mitigate the upstream and downstream effects.</p> <p>IBM Sterling Intelligent Promising provides shoppers with greater certainty, choice and transparency across their buying journey. It includes:</p> <ul> <li>IBM Sterling Fulfillment Optimizer with Watson to determine the best location from which to fulfill an order, based on business rules, cost factors, and current inventory levels and placement</li> <li>Sterling Inventory Visibility to processes inventory supply and demand activity to provide accurate and real-time global visibility across selling channels.</li> </ul> <p>IBM Planning Analytics with Watson streamlines and integrates financial and operational planning across the enterprise.</p> <p>Envizi simplifies the capture, consolidation, management, analysis and reporting of your environmental, social and governance (ESG) data.</p> <p>IBM Environmental Intelligence Suite provides climate and weather insights to anticipate disruptive environmental conditions, proactively manage risk and build more sustainable operations.</p>"},{"location":"supplychain/disasterreadiness/#references","title":"References","text":"<ul> <li>Supply chain planning and analytics</li> <li>MIT Solan Managmenet Review Three Scenarios to Guide Your Global Supply Chain Recovery</li> <li>Harvard Business Review Managing Unpredictable Supply-Chain Disruptions</li> <li>IBM The missing link: Why visibility is essential to creating a resilient supply chain</li> <li>IBM How to create more sustainable operations \u2013 one asset at a time</li> </ul>"},{"location":"supplychain/disasterreadiness/#next-steps","title":"Next steps","text":"<p>See:</p> <ul> <li>Loss and waste management (coming soon)</li> <li>Product timeliness (coming soon)</li> <li>Perfect order (coming soon)</li> <li>Intelligent order (coming soon)</li> <li>Returns (coming soon)</li> <li>Disaster readiness (coming soon)</li> </ul> <p>For a comprehensive supply chain overview, see Supply Chain Optimization.</p>"},{"location":"supplychain/disasterreadiness/#contributors","title":"Contributors","text":"<ul> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Anthony Giles, Business Automation Technical Specialist, IBM</li> <li>Eric Singsaas, Account Technical Lead, IBM Technology</li> <li>Bruce Kyle, Sr Solution Architect, IBM Client Engineering</li> <li>Mahesh Dodani, Principal Industry Engineer, IBM Technology</li> <li>Mike Lee, Principal Integration Technical Specialist, IBM</li> <li>Thalia Hooker, Senior Principal Specialist Solution Architect, Red Hat</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> </ul>"},{"location":"supplychain/disasterreadiness/#download-diagrams","title":"Download diagrams","text":"<p>View and download all of the Inventory Optimization diagrams shown in previous sections in our open source tooling site.</p> <ul> <li>PowerPoint Solution Overview: Open Solution Overview</li> <li>PowerPoint Reference Architecture: Open Workflow Diagrams</li> <li>DrawIO: Open Schematic Diagrams</li> </ul>"},{"location":"supplychain/intelligentorder/","title":"Intelligent order","text":"<p>Last mile delivery, also known as last mile logistics, is the transportation of goods from a distribution hub to the final delivery destination \u2014 the door of the customer. The goal of last mile delivery logistics is to deliver the packages as affordably, quickly and accurately as possible.\u00a0</p> <p>The last mile describes the difficulty of getting goods (especially large and bulky ones) from a transportation hub to their final destination because it might include installation and configuration while\u00a0providing an excellent experience at the same time.</p> <p>Intelligent order is a process that uses inventory management systems and AI to provide effective last mile delivery.</p> <p>For businesses this means:</p> <ul> <li>Decrease waste</li> <li>Order optimization</li> <li>Reduced cost</li> </ul> <p>Consumers benefit with:</p> <ul> <li>Delivery promises fulfilled</li> <li>Proof of delivery</li> </ul> <p>In this article, we explore two scenarios. One that optimizes last mile delivery and one that includes both your customers and third party logistics (3PL) providers.</p>"},{"location":"supplychain/intelligentorder/#business-problem","title":"Business problem","text":"<p>If the last mile promise to delivery is incorrect and a product arrives late, there\u2019s a good chance that the customer will shop with a competitor next time.</p> <p>If last mile tracking information is incorrect, a product arrives late, is damaged, or is never delivered, there\u2019s a good chance that the customer will shop with a competitor next time.</p> <p>Customers want:</p> <ul> <li>Fast delivery</li> <li>Delivery tracking</li> <li>Security</li> <li>Convenience</li> <li>Specialization</li> <li>Selection delivery based on cost </li> </ul> <p>Business need to consider:</p> <ul> <li>Fuel costs</li> <li>Idling and downtime</li> <li>Warehousing</li> <li>Failed deliveries</li> <li>Returns</li> <li>Sustainability commitments</li> </ul> <p>Key metrics:</p> <ul> <li>47.6% of shoppers will shop elsewhere if they cannot see inventory availability before they buy</li> <li>49% of shoppers would cancel their cart due to mismatch between expectations for delivery date and actual delivery date</li> <li>58% of shoppers report checking inventory availability in the last 6 months</li> </ul> <p>Deloitte describes key sector trends in Last mile delivery landscape in the grocery sector:</p> <ul> <li>Shifting tactics \u2013 evolving channel strategies and business models</li> <li>Need convenience delivered \u2013 customer\u2019s new mantra</li> <li>Future of Last Mile increasingly a story of better software and connectivity</li> </ul>"},{"location":"supplychain/intelligentorder/#use-cases","title":"Use cases","text":"Business scenario Challenge Consumers are purchasing more products online Online purchases of goods\u00a0and\u00a0direct to consumer deliveries are increasing dramatically.\u00a0 This has driven huge increases in the volume\u00a0 and importance of\u00a0last mile delivery and installation services. Consumers are placing more emphasis on after sales services Consumers are demanding more control over delivery times and processes and want more value-added services. Manufacturers and suppliers are not meeting expectations, especially with last mile services. New service providers are entering the market; reshaping the landscape New entrants to 3PL (third party logistics) services such as Amazon and Uber are disrupting traditional service models.\u00a0 Same day delivery is now an expectation, and this increases infrastructure and transportation services cost.\u00a0Improved supply and demand forecasting is key to delivering last mile\u00a0services. Logistics costs are\u00a0increasing, while product prices are eroding Manufacturers are forced to react quickly to changing client demands while managing rising\u00a0costs. Costs for warehousing, transportation and labor are increasing and this increases pressure on perfect order performance for both profitability and client loyalty."},{"location":"supplychain/intelligentorder/#proof-of-delivery","title":"Proof of Delivery","text":"<p>Proof of delivery, or POD, is an acknowledgment that an order successfully arrived at its intended destination and is used to show a service was completed, which is why proof of delivery is also called proof of completion. Proof of delivery comes in two forms: paper and electronic. Benefits of POD are captured in the table.</p> Benefits for customers Benefits for businesses Customers in control Promise of delivery provides a better customer experience Proof of Delivery makes it less likely orders will be stolen Improves order accuracy and saves money on unnecessary refunds or replacements Lets customers know their service is complete POD automation accelerates back-office operations"},{"location":"supplychain/intelligentorder/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <ul> <li>Consumer demand for delivery as promised</li> <li>Consumers wanting control of delivery and return processes</li> <li>Logistics costs in providing last mile delivery</li> <li>Logistics ecosystem complexity</li> </ul> <p>Drivers</p> <ul> <li>Increasing deliveries direct to consumer</li> <li>Need for delivery price-point optimization and improvement</li> <li>Customer satisfaction to drive repeat business</li> </ul>"},{"location":"supplychain/intelligentorder/#responses","title":"Responses","text":"<p>By definition, last mile delivery is relevant for businesses that deliver products directly to their consumers. </p> Business Problem Solution Maximize inventory productivity Use real-time inventory visibility to confidently expose inventory and maximize conversions, gaining granular control over inventory actions, such as safety stock setting based on configurable business rules. Improve inventory turns by applying additional context like channel, fulfillment type and labor availability when making available-to promise decisions. Make and manage order promises Improve conversion rates by confidently delivering order and delivery promises across every step of the shopping journey, including the product list page, product detail page, cart, and checkout. Automate the review of inventory, capacity and costs to make informed promises, and harness powerful AI during fulfillment to simplify complex scenarios like orders with third-party services, and support a wide range of fulfillment options. Optimize omnichannel profitability Set operating performance objectives and KPIs using real cost drivers (like distance, labor, capacity, and carrier costs) and profit drivers markdown, stockout), so you can confidently make the best fulfillment decisions for your business objectives. By optimizing across thousands of fulfillment permutations in milliseconds, retailers can ensure balance between profitability and the best customer experience."},{"location":"supplychain/intelligentorder/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Decrease waste. Decrease order fragmentation and waste with decrease in packages per order.</li> <li>Order optimization. In-stock improvement through holiday season using Order Optimizer.</li> <li>Reduce costs. Reduced costs and emissions by housing software on the cloud more efficiently than on-premises</li> <li>Minimize disruption. Gain visibility to minimize disruption to business despite supply and demand shocks</li> <li>Returns optimization. Returns refurbished to create a revenue stream by re-circulation, refurbishment, repairs thereby reducing disposal and recycling costs.</li> </ul>"},{"location":"supplychain/intelligentorder/#solution-overview","title":"Solution overview","text":"<p>This solution focuses on Automation and Modernization in our Action Guide as shown in the following diagram:</p> <ul> <li>Create a world-class sensing and risk-monitoring operation. </li> <li>Accelerate automation in extended workflows</li> <li>Amp up AI to make workflows smarter</li> <li>Modernize for modern infrastructures, scale hybrid cloud platforms</li> </ul> <p></p> <p>The solution uses the following technologies, which can be grouped into three main categories as shown in the following diagram:</p> <ul> <li>Core application systems. Often customer-provided technologies, such as order management, facilities management. These systems can be stand-alone applications, on premises and cloud services, databases. </li> <li>Foundational infrastructure. The Red Hat/IBM solution is built on Red Hat OpenShift. Data is routed through API management. Events are routed through Business Automation tools such as Business Automation Workshop. </li> <li>Fulfillment optimization</li> <li>Intelligent promising</li> </ul>"},{"location":"supplychain/intelligentorder/#intelligent-order_1","title":"Intelligent order","text":"<p>The following diagram shows the schematic for the intelligent order use case.</p> <p></p> <p>Intelligent order steps:</p> <ol> <li>Customer places/track/confirm order thru omni channel\u00a0 <li>Inventory fulfilment, and delivery tracking information is quicky obtained from supply assurance Platform <li>Access underlying backend system via API Management <li>Integration services <li>Check with the Store Operations System if inventory available in store <li>If store cannot fulfill order, check with Warehouse Management System where inventory is available <li>Update Transport/Logistics System to schedule delivery <li>Cross check with the Fulfilment System to schedule and track the order <li>Provide real-time tracking of the order and upon delivery provide POD (electronic or paper)"},{"location":"supplychain/intelligentorder/#delivery-optimization","title":"Delivery optimization","text":"<p>The following diagram shows the schematic for the delivery optimization use case to improve your supply chain.</p> <p></p> <p>Delivery optimization workflow steps:</p> <ol> <li>Customer chooses items to buy online using the business app. <li>Determine sustainability posture by determining ESG indicator values. <li>Before providing cost and delivery options, provide customer sustainability options \u2013 equivalent greener items, later delivery day, pickup option, etc. If customer opts into sustainability option order is tagged so Supply Assurance Platform can honor that request. <li>Inventory fulfilment system updates inventory data. <li>Delivery Optimization system plays a key role in sustainability play. It determines whether to contact 3PL or if in-house Route Optimization can fulfill the requirements. <li>If 3PL is the only option, contact the sustainability approved 3PL company to fulfill the order. 3PL company takes over the delivery flow from here. If business has the means to fulfill the order continue with next Step 6a. <ul> <li>Access underlying backend system via API Management  <li>Alert the Order Fulfilment System (OFS) this special order via system APIs. <li>OFS notifies the Warehouse Management System to package and get it ready for delivery. <li>The Transport/Logistics System is alerted to schedule delivery.  <ol> <li>Track the order, notify customer and provide real-time tracking. <li>Delivery department maintains the sustainability posture and upon final delivery provides POD (electronic or paper) to customer.  <li>All sub-systems are updated via the data fabric that helps maintain a consistent view."},{"location":"supplychain/intelligentorder/#action-guide","title":"Action Guide","text":"<p>From a high-level perspective, there are several main steps your organization can take to drive innovation and move toward a digital supply chain:</p> <ul> <li>Automation</li> <li>Sustainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automation Create a world-class sensing and risk-monitoring operation Delivering greater certainty, choice and transparency to shoppers to enhance shopping experiences, improve digital and in-store conversion, and increase omnichannel revenue. Automation Accelerate automation in extended workflows Combine inventory and capacity visibility with sophisticated fulfillment decisioning to maximize inventory productivity, make reliable and accurate order promises, and optimize fulfillment decisions at scale. Automation Provide visibility across multiple systems Build a global view of real-time inventory, including available-to-promise (ATP) inventory, capacity to process orders at different locations, and transportation availability. Automation Amp up AI to make workflows smarter Optimize fulfillment execution and inventory levels to improve cost-to-serve and balance operations with industry-leading machine learning technology. Use AI with learned sell-through patterns to understand potential stockouts, demand shifts, markdowns, shipping costs, labor costs, and capacity so that retailers can make sourcing decisions that balance costs and service Sustainability Deliver on corporate commitment to sustainability Include sustainability KPIs in the selection of delivery methods Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point."},{"location":"supplychain/intelligentorder/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p>"},{"location":"supplychain/intelligentorder/#core-solution","title":"Core solution","text":"<p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security.</p> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.</p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"supplychain/intelligentorder/#integration-services","title":"Integration services","text":"<p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</p> <p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing. </p> <p>IBM Data Fabric empowers your teams and works across the ecosystem by connecyting data from disparate data sources in multicloud envrionments. In particular, Watson Knowledge Catalog provides you users with a catalog tool for intelligent, self-service discovery of data, models. Watson Query provides data consumers with a universal query engine that executes distributed and virtualized queries across databases, data warehouses, data lakes, and streaming data without additional manual changes, data movement or replication. </p>"},{"location":"supplychain/intelligentorder/#supply-assurance-platform","title":"Supply Assurance Platform","text":"<p>IBM Sterling Intelligent Promising provides shoppers with greater certainty, choice and transparency across their buying journey. It includes:</p> <ul> <li>IBM Sterling Fulfillment Optimizer with Watson to determine the best location from which to fulfill an order, based on business rules, cost factors, and current inventory levels and placement</li> <li>Sterling Inventory Visibility to processes inventory supply and demand activity to provide accurate and real-time global visibility across selling channels.</li> </ul>"},{"location":"supplychain/intelligentorder/#similar-use-cases","title":"Similar use cases","text":"<p>See:</p> <ul> <li>Inventory management</li> <li>Demand risk</li> <li>Loss and waste management</li> <li>Product timeliness</li> <li>Returns</li> <li>Disaster readiness</li> </ul> <p>For a comprehensive supply chain overview, see Supply Chain Optimization.</p>"},{"location":"supplychain/intelligentorder/#downloads","title":"Downloads","text":"<p>View and download all of the Inventory Optimization diagrams shown in previous sections in our open source tooling site.</p> <ul> <li>PowerPoint Solution Overview: Open Solution Overview</li> <li>PowerPoint Reference Architecture: Open Workflow Diagrams</li> <li>DrawIO: Open Schematic Diagrams</li> </ul>"},{"location":"supplychain/intelligentorder/#references","title":"References","text":"<ul> <li>Last mile delivery landscape in the grocery sector</li> <li>IDC Blog: Enabling the Retail Workforce for Omnichannel Fulfillment</li> <li>COVID-19 Survey: Accurate Delivery Dates Are Key To Win Consumer Confidence</li> <li>Lauren Freedman, The Shopper Speaks: 3 secrets why curbside will not die post-COVID-19</li> </ul>"},{"location":"supplychain/intelligentorder/#contributors","title":"Contributors","text":"<ul> <li>Rajeev Shrivastava, Account Technical Lead, IBM</li> <li>Ashok Iyengar, Executive Cloud Architect, IBM</li> <li>Karl Cama, Chief Architect, Red Hat</li> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Bruce Kyle, Solutions Architect, IBM Client Engineering</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> </ul>"},{"location":"supplychain/lossmanagement/","title":"Loss and waste management","text":"<p>A key focus when dealing with loss and waste management in relation to inventory optimization is how to handle unplanned or unforseen situations causing an inventory item be considered damaged or spoiled. If the situation or problem is rectified withing a well defined time window, there is a possibility of salvaging the product. In some cases once the incident occurs, there is no possibility of salvage and the product is considered damaged. Typically these events are external factors forced upon the business and cannot always be planned or predicted. </p> <p>To demonstrate the importance of inventory optimization for any business, we will focus on two main use cases of an unanticipated exception:</p> <ul> <li>Environment Exceptions such as power outages or temperature changes that creates potential spoilage </li> <li>Product contamination or recall such as foreign objects or bacteria occuring earlier in the processing or supply chain</li> </ul> <p>We will start by defining the business problem and the two main use cases: Environmental Exceptions and Product contamination or recall, describe the challenges and business drivers organizations face. Next, we will provide an action guide, provide an overview of the solution, show a schematic of the two use cases: Environmental Exceptions and Product contamination or recall, and conclude with the technology used in the solution.</p> <p>For a comprehensive inventory solution overview, see Inventory Optimization.</p>"},{"location":"supplychain/lossmanagement/#business-problem","title":"Business problem","text":"<p>Loss and waste management is principally focused on ensuring food and food related products remain fit for consumption at the time they are sold to the end consumer. However, external factors outside the control of the business can cause food items to be marked as spoiled or damaged. The problem faced by the business is ensuring the overall loss and wastage is minimised.</p> <p>To prevent spoilage, food products must typically be stored and transported at temperatures within well defined ranges. For example: - Frozen food must be kept below a specific temperature (0\u00b0F) at all times. If the temperature rises and food defrosts, it cannot be refrozen. - Chilled food must be kept within a temperature range (34\u00b0F - 39\u00b0F). The temperature can sometimes go above the higher limit for a short space of time before returning to the correct temperature, without becoming spoiled. If the high temperature is exceeded for a specified duration it must be considered spoiled. - Shelf stable food does not require chilling, but needs to be stored withing a temperature range to ensure the product shelf life is maintained. A shelf stable food product's shelf life can be 6 weeks to 5 years, depending on the product.</p> <p>To keep food at the correct temperature, refrigeration and chilling during transport and storage are the primary options. Power outages will happen. The ability to deal with the situation within well defined time periods is critical to ensuring the loss and wastage is minimised. If the problem can be rectified quickly and easily, there is every chance the product can remain in perfect conditiona and be sold to a consumer. Failure to act quickly will result in spoilage and loss of the product.</p> <p>Between farm and end consumer, food products generally go through multiple stages which vary depending on the end product. For convenience foods, there can be several manufuacturing steps, for fresh produce packaging and transport are the main stages. At any stage in the process, there is a possibility of contamination through foreign objects or bacteria. Both of which can trigger a recall of the food products. With any kind of contamination, a fast and efficient recall process to prevent the products beng purchased by consumers is vital.</p> <p>All retailers handing and selling food products need processes to reduce food waste through external factors and ensure recall procedures in the event of contamination are fast, efficient and minimise any risk to public health.</p>"},{"location":"supplychain/lossmanagement/#use-cases","title":"Use cases","text":"<p>The main use cases represented by loss management are:</p> <ul> <li>Environmental Exceptions \u2010 an environmental issue or problem places the food product at risk. If the issue is not dealt with quickly and efficiently the product will be spoiled making unavailable for sale to a consumer. Typically this would be a failure of refrigeration equipment or a loss of power to refrigeration euipment. If the temperature can be kept within a specific range, the product will remain suitable for sale to a consumer. If the temperature moves beyond the specified range for a set period of time, the product will be considered spoiled and no longer suitable for sale to a consumer.</li> <li>Product contamination or recall \u2010 a contamination or recall requires the food product to be removed immediately from the inventory and supply chain. The contamination issue or recall notice can come from external parties such as suppliers or food standards agencies, or through the retailer identifying an issue. Once the issue has been identified, the reatiler must immediately remove all affected products from their own inventory and supply chain, then destroy or pass the product to a third party. The issue can be caused by foreign objects or the detection of selected bateria in the product.</li> </ul>"},{"location":"supplychain/lossmanagement/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <ul> <li>Protect public health: Businesses that sell consumable products are expected to protect the public health by implementing specific processes to address food safety. Maintaining equipment that refrigerates or freezes food or pharmaceutical products for storage until purchase or combined into a recipe and responding to and documenting actions taken to a food safety recall notification by either the supplier or regulatory body are key challenges.</li> <li>Regulatory compliance: Business that sell consumable products are expected to comply with local, state, and federal regulatory requirements for food and pharmaceutical products. Automated documentation of compliance is preferred over manual, error-prone documentation.</li> </ul> <p>Drivers</p> <ul> <li>Industry leading inventory management system, incorporating sensors that can mitigate risks before they create an exposure.</li> <li>Integrating loss and waste management into the core supply chain applications, leveraging technical investments in modern infrastructures and edge devices.</li> <li>Protect Reputation</li> <li>Avoid Penalties and fines</li> </ul>"},{"location":"supplychain/lossmanagement/#responses","title":"Responses","text":"<p>All businesses that require a supply chain have unanticipated issues. </p> Business problem Solution Manual processes, limited capabilities of inventory management tools, and global operations pose a challenge for enterprises to manage and act on inventory and mitigate disruptions to meet actual demand. Monitor and manage network inventory availability and anticipate actions due to unanticipated exception with alerts and recommended actions Business process for handling unexpected issues may not be consistent across the enterprise Gain detailed visibility into inventory characteristics at each location and provide transparency to inventory. Enable actionable inventory shifts across the enterprise Visibility into actions needed and alternatives to anticipate and respond to inventory in an unanticipated event. Provide actionable tasks, work orders, visibility for workers and supply chain partners to remove recalled items. Proactively replace items in response to demand."},{"location":"supplychain/lossmanagement/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Respond to unexpected events quickly</li> <li>Automated processes provide up-to-date transparently into inventory </li> <li>Risks mitigated</li> </ul>"},{"location":"supplychain/lossmanagement/#solution-overview","title":"Solution overview","text":"<p>This solution focuses on Automation and Modernization in our Action Guide as shown in the following diagram:</p> <ul> <li>Create a world-class sensing and risk-monitoring operation. </li> <li>Accelerate automation in extended workflows</li> <li>Amp up AI to make workflows smarter</li> <li>Modernize for modern infrastructures, scale hybrid cloud platforms</li> </ul> <p></p> <p>The solution uses the following technologies, which can be grouped into three main categories as shown in the following diagram:</p> <ul> <li>Core application systems. Often customer-provided technologies, such as order management, facilities management. These systems can be stand-alone applications, on premises and cloud services, databases. </li> <li>Foundational infrastructure. The Red Hat/IBM solution is built on RedHat OpenShift. Data is routed through API management. Events are routed through Business Automation tools such as Business Automation Workshop. </li> <li>Inventory Optimization platform</li> </ul>"},{"location":"supplychain/lossmanagement/#solution-principles","title":"Solution principles","text":"<p>True end-to-end visibility. Remove data silos and create a unified view across supply chain data with a standard data platform. Personalized dashboards and insights provide a 360-degreee view of KPIs and significant events.</p> <p>Manage by exception. Detect, display, and prioritize work tasks in real time. This allows clients to sense and react to issues quickly while managing risks and disruptions in a supply chain proactively.</p> <p>Intelligent workflows. Actionable workflows can be customized to meet unique requirements and process steps required to automate actions within source transactional systems. Make informed decisions with a supply chain virtual assistant that provides responses to issues based on a client\u2019s supply chain data using natural language search.</p>"},{"location":"supplychain/lossmanagement/#environment-exception","title":"Environment Exception","text":"<p>The following diagram shows the schematic for the understock use case.</p> <p></p> <p>Food Loss - Environmental Exception steps:</p> <ol> <li>Environmental event detected (e.g. Temperature out of range or loss of power) <li>Notification sent to Supply Risk Management via API Management service <li>Inventory Control Tower notified of risk <li>Inventory Control Tower triggers process to manage issue <li>Inventory Controller notified and action determined <li>Update process with Inventory Controller decision <li>Remediation   <ol> <li>Inventory analysis notified to determine remediation     <li>Facilities personel notified to take remediation action     <li>Update Inventory Control Tower"},{"location":"supplychain/lossmanagement/#contaminationrecall","title":"Contamination/Recall","text":"<p>The following diagram shows the schematic for the contamination recall use case.</p> <p>Food Loss - Contamination recall steps:</p> <p>)</p> <ol><li>External notification of food safety event <li>Notification sent to Supply Intelligence &amp; Inventory Analysis via API Management service <li>Determine scope   <ol> <li>Determine if supply affected     <li>Determine which locations received affected product    <li>Notify Inventory Control Tower <li>Inventory Control Tower processes event data and starts remedediation action <li>Colleague remediates inventory and counts, then removes product from inventory <li>Remediation   <ol> <li>Inventory updated     <li>Apply financial\u00a0reimbursement. Generate new order"},{"location":"supplychain/lossmanagement/#use-case-and-benefits","title":"Use case and benefits","text":"Use Case The Problem The Solution The Benefits and Implications Automated processes Manual input and follow up Business automation provides a systematic way to notifications, documentation of notifications, and creation of work orders. Actions follow a consistent business process and can be easily updated as needs change Damaged or potential issues products Facilities issues can immediately impact product liability, lead to lost revenue and decreased brand / retailer loyalty. Control Tower monitors inventory connections to multiple core application systems foster visibility,  create items in the work queue when revenue is at risk. When drilling down on the item, users can see where they have available inventory and receive recommendations about how much inventory can and should be ordered for replacement based on demand. Action can be taken directly from the Control Tower user interface. Product situations are efficiently managed and OOS are avoided with minimal human intervention. API Management Separation of systems, control and monitoring of access, providing consistent user authentication and security between platforms API Manages the access and permissions required for data between systems. Improved security, monitoring of frequency of access between systems Supply intelligence, inventory analysis Avoid discarding items not included in recall, contamination Provide alternative products that can be substituted. Determine alternative locations or steps to stage product Supply intelligence and inventory analytics provides record of product details, visibility into substitute products, - visibility of item locations, suggested remediation steps Supplies can be immediately removed from sale, substitute product offered, steps to ship unaffected products as needed based on actual demand Colleage and partner engagement Quick sharing data between enterprise silos and to partners who can provide solutions Visibility into recall issues"},{"location":"supplychain/lossmanagement/#action-guide","title":"Action Guide","text":"<p>From a high-level perspective, there are several main steps your organization can take to drive innovation and move toward a digital supply chain:</p> <ul> <li>Automation</li> <li>Systainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automation Create a world-class sensing and risk-monitoring operation Leveraging IOT/Edge devices, implement the ability to detect abnormal variations in temperature, power, water, machinery, and transportation to quickly react and correct. Automation Accelerate automation in extended workflows Business automation provides a systematic way to notifications, documentation of notifications, and creation of work orders. Automation Amp up AI to make workflows smarter For Damaged or potential issues products, Control Tower monitors inventory connections to multiple core application systems foster visibility, create items in the work queue when revenue is at risk. When drilling down on the item, users can see where they have available inventory and receive recommendations about how much inventory can and should be ordered for replacement based on demand. Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point."},{"location":"supplychain/lossmanagement/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p>"},{"location":"supplychain/lossmanagement/#core-systems","title":"Core systems","text":"<p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security.</p> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.  </p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"supplychain/lossmanagement/#integration-services","title":"Integration services","text":"<p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing.</p>"},{"location":"supplychain/lossmanagement/#supply-assurance-platform","title":"Supply assurance platform","text":"<p>IBM Supply Chain Control Tower provides actionable visibility to orchestrate your end-to-end supply chain network, identify and understand the impact of external events to predict disruptions, and take actions based on recommendations to mitigate the upstream and downstream effects.</p> <p>IBM Sterling Intelligent Promising provides shoppers with greater certainty, choice and transparency across their buying journey. It includes:</p> <ul> <li>IBM Sterling Fulfillment Optimizer with Watson to determine the best location from which to fulfill an order, based on business rules, cost factors, and current inventory levels and placement</li> <li>Sterling Inventory Visibility to processes inventory supply and demand activity to provide accurate and real-time global visibility across selling channels.</li> </ul> <p>IBM Planning Analytics with Watson streamlines and integrates financial and operational planning across the enterprise.</p>"},{"location":"supplychain/lossmanagement/#similar-use-cases","title":"Similar use cases","text":"<p>See:</p> <ul> <li>Inventory management</li> <li>Demand risk</li> <li>Product timeliness</li> <li>Intelligent order</li> <li>Returns</li> <li>Disaster readiness</li> </ul> <p>For a comprehensive supply chain overview, see Supply Chain Optimization.</p>"},{"location":"supplychain/lossmanagement/#downloads","title":"Downloads","text":"<p>View and download all of the Inventory Optimization diagrams shown in previous sections in our open source tooling site.</p> <ul> <li>PowerPoint Solution Overview: Open Solution Overview</li> <li>PowerPoint Reference Architecture: Open Workflow Diagrams</li> <li>DrawIO: Open Schematic Diagrams</li> </ul>"},{"location":"supplychain/lossmanagement/#contributors","title":"Contributors","text":"<ul> <li>Mike Lee, IBM</li> <li>Iain Boyle, Red Hat</li> <li>Bruce Kyle, IBM</li> <li>Mahesh Dodani, IBM</li> <li>Thalia Hooker, Red Hat</li> <li>Jeric Saez, IBM</li> <li>Lee Carbonell, IBM</li> <li>James Stewart, IBM</li> <li>Lee Carbonell, IBM</li> </ul>"},{"location":"supplychain/perfectorder/","title":"Perfect order","text":"<p>Managing inventory efficiently is critical to any business that sells physical goods, is responsible for maintenance, repair and operations (MRO) supplies. </p> <p>Inventory management encompasses procedures and processes that directly or indirectly affect the bottom line \u2010 ordering, receiving, storing, tracking and accounting for all of the goods a business sells. It is a key element of supply chain management.</p> <p>In this scenario, you will explore the implications of a perfect and imperfect order in which inventory:</p> <ul> <li>Can meet customer expecation.</li> <li>May not have all items available.</li> </ul>"},{"location":"supplychain/perfectorder/#business-problem","title":"Business problem","text":"<ul> <li>Inventory visibility. Multichannel order fulfillment operations typically have inventory spread across many places throughout the supply chain. Inventory visibility is knowing what inventory you have and where it\u2019s located. Businesses need an accurate view of inventory to guarantee fulfillment of customer orders, reduce shipment turnaround times, and minimize stockouts, oversells and markdowns.</li> <li>Orders may not match available inventory. Inventory may not be on hand or in a different location than expected. Multiple and partial shipments may be able to solve many customer requests.</li> </ul>"},{"location":"supplychain/perfectorder/#business-solution","title":"Business solution","text":"<p>The goal of inventory management is to have the right products in the right place at the right time. This requires inventory visibility \u2014 knowing when to order, how much to order and where to store stock. The basic steps of inventory management include:</p> <ul> <li>Purchasing inventory. Ready-to-sell goods are purchased and delivered to the warehouse or directly to the point of sale.</li> <li>Storing inventory. Inventory is stored until needed. Goods or materials are transferred across your fulfillment network until ready for shipment.</li> <li>Profiting from inventory. The amount of product for sale is controlled. Finished goods are pulled to fulfill orders. Products are shipped to customers.</li> </ul>"},{"location":"supplychain/perfectorder/#use-cases","title":"Use cases","text":"<p>More and more businesses are looking at having sustainable supply chains because customers are demanding it. </p> Use case Problem statement Solution Stock on hand Inventory visibility What goods are in stock, how much do you have and where are the items? All that information is required to fill orders. An Inventory management system helps provide that information and more. Forecasting for fulfillment / Demand forecasting Forecasting Forecasting is predicting how much inventory you\u2019ll need on hand to meet upcoming demand. Goal of forecasting is to have just enough inventory on hand to cover predicted sales for a prescribed period of time. Optimal inventory Monitor Orders &amp; Inventory  By minimizing lost sales, misplaced stock and excess ordering, inventory management boosts your profits and can even reduce your taxes. Optimal inventory level is the ideal quantity of product that one should have in a fulfillment center(s) at any given time. By optimizing inventory levels, you reduce the risk of common inventory issues, from high storage costs to out-of-stock items. Warehouse location Inside the Warehouse refers to the specific spot, such as a shelf or a bin, where a product is located within the four walls of a warehouse. It impacts the efficiency and timeliness of picking products. The products most often picked is placed in the most efficient spot. As products change and demand for products changes, the inventory layout can be updated to ensure continued efficiency. Warehouse location Physical Location of the Warehouse Choosing the site for a warehouse is based on many factors: <ul><li>Rent Rates &amp; Taxes<li>Workforce Availability, Labor Skills &amp; Costs<li>Access Roads, Highways &amp; Traffic Flow<li>Proximity to Airport, Railway Stations &amp; Ports<li>Proximity to Markets &amp; Local Environment<li>Building Availability &amp; Utility Costs<li>Building intelligent"},{"location":"supplychain/perfectorder/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <ul> <li>Inventory visibility. Multichannel order fulfillment operations typically have inventory spread across many places throughout the supply chain. Inventory visibility is knowing what inventory you have and where it\u2019s located. Businesses need an accurate view of inventory to guarantee fulfillment of customer orders, reduce shipment turnaround times, and minimize stockouts, oversells and markdowns.</li> <li>Orders may not match available inventory. Inventory may not be on hand or in a different location than expected. Multiple and partial shipments may be able to solve many customer needs.</li> <li>Spreadsheets, hand-counted stock levels and manual order placement have largely been replaced by advanced inventory tracking software. An inventory management system can simplify the process of ordering, storing and using inventory by automating end-to-end production, business management, demand forecasting and accounting.</li> </ul> <p>Drivers</p> <ul> <li>Supply chain operators will use technologies that provide significant insights into how supply chain performance can be improved. They\u2019ll anticipate anomalies in logistics costs and performance before they occur and have insights into where automation can deliver significant scale advantages</li> <li>Eliminate \u201cout of stock\u201d occurrences</li> <li>Optimizes inventory costs by balancing inventory to operational demand</li> <li>Improve stock allocation to improve space utilization</li> <li>Reduce dormant/non-moving inventory</li> <li>Have an accurate assessment of critical spares and the money invested in those parts</li> <li>Deliver transparency and visibility to critical and non-critical spares inventory for the organization</li> <li>Reduce time spent looking for and ordering parts</li> <li>Accelerate disposal of obsolete materials</li> </ul>"},{"location":"supplychain/perfectorder/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Decrease waste. Decrease order fragmentation and waste with decrease in packages per order.</li> <li>Order optimization. In-stock improvement through holiday season using Order Optimizer.</li> <li>Reduce costs. Reduced costs and emissions by housing software on the cloud more efficiently than on-premises</li> <li>Minimize disruption. Gain visibility to minimize disruption to business despite supply and demand shocks</li> <li>Returns optimization. Returns refurbished to create a revenue stream by re-circulation, refurbishment, repairs thereby reducing disposal and recycling costs.</li> </ul>"},{"location":"supplychain/perfectorder/#solution-overview","title":"Solution overview","text":"<p>This solution focuses on Automation and Modernization in our Action Guide as shown in the following diagram:</p> <ul> <li>Increase inventory visibility</li> <li>Monitor the criticality of inventory items to the organization </li> <li>Accelerate automation in extended workflows</li> <li>Amp up AI to make workflows smarter</li> <li>Modernize for modern infrastructures, scale hybrid cloud platforms</li> </ul> <p></p>"},{"location":"supplychain/perfectorder/#solution-principles","title":"Solution principles","text":"<p>True end-to-end visibility. Remove data silos and create a unified view across supply chain data with a standard data platform. Personalized dashboards and insights provide a 360-degreee view of KPIs and significant events.</p> <p>Manage by exception. Detect, display, and prioritize work tasks in real time. This allows clients to sense and react to issues quickly while managing risks and disruptions in a supply chain proactively.</p> <p>Intelligent workflows. Actionable workflows can be customized to meet unique requirements and process steps required to automate actions within source transactional systems. Make informed decisions with a supply chain virtual assistant that provides responses to issues based on a client\u2019s supply chain data using natural language search.</p>"},{"location":"supplychain/perfectorder/#perfect-order_1","title":"Perfect order","text":"<p>The following diagram shows the inventory management scenario for an perfect order with responses for cases where the order is not perfect.</p> <p></p> <ol> <li>Customer places/track/confirm order thru omni channel\u00a0 <li>Inventory fulfilment, and delivery tracking information is quicky obtained from supply assurance Platform <li>Access underlying backend system via API Management <li>Notify various backend systems via the Integration Services <li>Check with the Inventory Management System if inventory is available then lock the items and update the inventory. <li>Use Store Operations System to determine if store can fulfill order. Store can only fulfill partial order. Notify customer about partial order fill. If acceptable, package order and get it ready for delivery to customer. Go to Step 7. If partial order is not acceptable, get it ready to be sent to warehouse. Go to Step 8. <li>Notify Transport/Logistics System to schedule delivery.  <li>Use Warehouse Management System to find which warehouse can fulfill remaining order items. Send alert to warehouse to combine partial orders and package itmes and get it ready for delivery. Notify customer and update/sync related systems. <li>Update Transport/Logistics System to schedule delivery. <li>Cross check with the Fulfilment System to schedule and track the order and notify customer. <li>Provide real-time tracking of the order to customer and upon delivery provide POD (electronic or paper) to customer."},{"location":"supplychain/perfectorder/#action-guide","title":"Action guide","text":"<p>From a high-level perspective, there are several main steps your organization can take to drive innovation and move toward a digital supply chain:</p> <ul> <li>Automation</li> <li>Sustainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automation Monitor the criticality of inventory items to the organization Some are critical to operations or employee safety. Others are \u201cnice to have\u201d but not urgent. Select a methodology and a solution that routinely audit those parameters. For example, any time a new transaction is created in the Enterprise Resource Planning (ERP), the solution needs to apply that transaction against the material to determine if the criticality is still correct. Automation Accelerate automation in extended workflows Automate the response using workflows that are consistant with criticality of the inventory items. Automation Provide visibility into the inventory system When users are inspecting inventory items, users can see where they have available inventory and receive recommendations about how much inventory can and should be transferred to the OOS / AOOS locations. These recommendations are based on adding automation and AI to make workflows smarter. Automation Automate the maintenance of scores to each individual item Eliminate data manual entry for criticality with a systems wide approach. Sustainability Includes sustainability metrics into decision making Surface sustainability information as part of the inventory management processes. Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point."},{"location":"supplychain/perfectorder/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p>"},{"location":"supplychain/perfectorder/#core-systems","title":"Core systems","text":"<p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security.</p> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.</p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"supplychain/perfectorder/#integration-services","title":"Integration services","text":"<p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</p> <p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing.</p>"},{"location":"supplychain/perfectorder/#supply-assurance-platform","title":"Supply assurance platform","text":"<p>IBM Supply Chain Control Tower provides actionable visibility to orchestrate your end-to-end supply chain network, identify and understand the impact of external events to predict disruptions, and take actions based on recommendations to mitigate the upstream and downstream effects.</p> <p>IBM Sterling Intelligent Promising provides shoppers with greater certainty, choice and transparency across their buying journey. It includes:</p> <ul> <li>IBM Sterling Fulfillment Optimizer with Watson to determine the best location from which to fulfill an order, based on business rules, cost factors, and current inventory levels and placement</li> <li>Sterling Inventory Visibility to processes inventory supply and demand activity to provide accurate and real-time global visibility across selling channels.</li> </ul> <p>IBM Planning Analytics with Watson streamlines and integrates financial and operational planning across the enterprise.</p> <p>IBM Maximo MRO Inventory Optimization can help you optimize your maintenance, repair and operations (MRO) inventory by providing an accurate, detailed picture of performance.</p>"},{"location":"supplychain/perfectorder/#similar-use-cases","title":"Similar use cases","text":"<p>See:</p> <ul> <li>Demand risk</li> <li>Loss and waste management</li> <li>Product timeliness</li> <li>Intelligent order</li> <li>Returns</li> <li>Disaster readiness</li> </ul> <p>For a comprehensive supply chain overview, see Supply Chain Optimization.</p>"},{"location":"supplychain/perfectorder/#downloads","title":"Downloads","text":"<p>View and download all of the Inventory Optimization diagrams shown in previous sections in our open source tooling site.</p> <ul> <li>PowerPoint Solution Overview: Open Solution Overview</li> <li>PowerPoint Reference Architecture: Open Workflow Diagrams</li> <li>DrawIO: Open Schematic Diagrams</li> </ul>"},{"location":"supplychain/perfectorder/#reference","title":"Reference","text":"<ul> <li>IBM documentation What is inventory management?</li> <li>Blog Understanding \u201cSpares Criticality\u201d in your MRO inventory</li> </ul>"},{"location":"supplychain/perfectorder/#contributors","title":"Contributors","text":"<ul> <li>Rajeev Shrivastava, Account Technical Lead, IBM</li> <li>Ashok Iyengar, Executive Cloud Architect, IBM</li> <li>Karl Cama, Chief Architect, Red Hat</li> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Bruce Kyle, Solutions Architect, IBM Client Engineering</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> </ul>"},{"location":"supplychain/returns/","title":"Returns","text":"<p>Evolving customer expectations have forced retailers to figure out how to better manage the ripple effects that the current global crisis has created. This includes the realm of returns. </p>"},{"location":"supplychain/returns/#use-cases","title":"Use cases","text":"<ul> <li>Manage return policies and manage customer expectations.</li> <li>Minimizing return losses.</li> <li>Categorize return items for recycle, dispose, or return for sales.</li> <li>Determine fraudulent returns.</li> <li>Identify and track returns items with hazardous materials.</li> </ul>"},{"location":"supplychain/returns/#background","title":"Background","text":"<p>Let\u2019s look at some of the trends that 2020 has seen so far:</p> <ul> <li>Extended return windows: Many retailers have instituted extended return periods that go beyond the average 30-day window for customers.</li> <li>Increased returns: With physical stores closed, ecommerce is on the rise. But with this shift comes increased returns, as customers can\u2019t be certain what they\u2019re buying online will be what the right size and fit.</li> <li>Not accepting returns: In the face of health concerns around coronavirus lingering on surfaces for extended periods of time, some retailers have stopped accepting product returns at all.</li> </ul>"},{"location":"supplychain/returns/#business-problem","title":"Business problem","text":"<p>Meet consumer demands by modernizing IT for the retail and consumer packaged goods (CPG) industry.</p>"},{"location":"supplychain/returns/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <p>Making returns easy for consumers is a way to create a loyal customer.</p> <p>Minimizing returns losses by:</p> <ul> <li>Sell the right item. One major step that eliminates a possible return is to make sure you sell the right item to the right customer. </li> <li>Save the sale. The \u2018save the sale\u2019 method is key for businesses who are looking to compensate for returns through a loyalty incentive. </li> <li>Smart returns. Smart returns will require a connected network of inventory visibility, as well as predicted demand.</li> <li>Predict returns. Data plays a vital role when it comes to predicting returns. Specifically, data that tracks the reason for a return: whether it's done directly (item was damaged, didn\u2019t fit, wasn\u2019t as pictured, etc.) or learned through predictive analytics.</li> <li>Buy online, pick up (or return) in-store. Retailers can give shoppers greater flexibility by allowing them to purchase online and then pick up in-store. While they\u2019re there to pick up, they can test or try products they\u2019ve ordered and make an exchange via curbside processing if needed.</li> <li>Virtual or personal shopping. Whether it\u2019s a virtual showcase of new items for your most loyal customers, or enabling sales associates to set aside items for shoppers based on their past purchase history, these tech-enabled shopping solutions should help lower the risk of returns.</li> <li>Process returns quickly and efficiently, reducing the amount of time and employee hours spent.</li> </ul> <p>Identify fraudulent returns:</p> <ul> <li>Returning stolen merchandise</li> <li>Receipt fraud</li> <li>Employee fraud</li> <li>Price switching</li> <li>Price arbitrage</li> <li>Switch fraud</li> <li>Bricking</li> <li>Cross-retail return</li> <li>Open-box fraud</li> <li>Wardrobing</li> </ul> <p>Drivers</p> <ul> <li>Increase customer loyalty with appropriate returns policies</li> <li>Minimizing returns losses</li> <li>The return process is an opportunity to upsell</li> <li>Identify fraudulent returns</li> <li>Provide sustainable transport of returned items</li> <li>Demonstrate proper disposal of hazardous material returns</li> </ul>"},{"location":"supplychain/returns/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Determine return policy suitable by item</li> <li>Setting and meeting customer expectations on returned items</li> <li>Enhance loyalty of customers</li> <li>Minimize losses by having fewer returns</li> <li>Identify items for resale, open box, refurbish, resale, donation to charity</li> <li>Hazardous materials handled properly</li> </ul>"},{"location":"supplychain/returns/#solution-overview","title":"Solution overview","text":"<p>The solution shown in Figure 1 uses components that can be grouped into three main categories as shown in the following diagram:</p> <ul> <li>Core application systems. Often customer-provided technologies, such as order management or facilities management. These systems can be standalone applications, on-premises and cloud services, and databases.</li> <li>Foundational infrastructure. The Red Hat/IBM solution is built on Red Hat OpenShift with data routed through API management and events routed through business automation tools such as Business Automation Workshop.</li> <li>Supply Chain Optimization platform consisting of a Supply Assurance Control Panel, Fulfillment Optimization, and Inventory Analysis &amp; AI.</li> </ul> <p></p> <p>Figure 1. Overall view of demand risk solution.</p>"},{"location":"supplychain/returns/#logical-diagrams","title":"Logical diagrams","text":"<p>Figure 2. The personas and technologies that provide a platform for some of the biggest potential breakthroughs in the supply chain.</p>"},{"location":"supplychain/returns/#architectures","title":"Architectures","text":"<p>The figures in this section show the interaction of customer systems with supply chain optimization platform systems in the context of a retail scenario with branch stores.</p>"},{"location":"supplychain/returns/#returns-management","title":"Returns management","text":"<p>Figure 3. Schematic diagram of returns management use case.</p> <p>Returns workflow steps:</p> <ol> <li>Customer initiates return, provides reason at a kiosk, store, or mobile application <li>Set customer expectation of refund, store credit, or upsell <li>Business automation orchestrates, return item process <ol> <li>Item is picked up and returns to a warehouse or store location <li>The item is returned to a location <li>Disposition system determines whether item is for disposal, resell as open box, referbish based on business rules.  <li>Review return for fraudulent return patterns <li>Track hazardous material disposal"},{"location":"supplychain/returns/#action-guide","title":"Action Guide","text":"<p>From a high-level perspective, the Action Guide represents a future state for organizations considering a comprehensive commitment. The idea is to outline a set steps that can be prioritized to reach that future state by adding new functionality to your existing systems.</p> <ul> <li>Automation</li> <li>Sustainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automation Create a world-class sensing and risk-monitoring operation Integrate data from multiple systems to get enterprise-wide view of returns of items in inventory demand. Monitor and analyze returns in real-time. Automation Accelerate automation in extended workflows Automate returns policy for consistency across the enterprise and customize based on item. Automation Amp up AI to make workflows smarter <ul><li>Use AI to identify product deficiencies that lead to return patterns, generate upsell recommendations, identify fraud patterns, and recommend sustainable transport options for return items.<li>Use AI to determine likely returned items<li>Use AI to identify fraudulent returns Sustainability Include sustainability commitments in decision making Integrate sustainability metrics in returns decision making, especially return items transport or disposal. Sustainability Track disposal of hazardous returns Report on how hazardous returns are handled. Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point. Modernization Modernize application deployment and operations practices Include DevOps best practices to deploy, monitor, and maintain applications <p>For specific steps on this approach, see The Action Guide details in Own Your Transformation survey of 1500 CSCOs across 24 industries.</p>"},{"location":"supplychain/returns/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p>"},{"location":"supplychain/returns/#core-systems","title":"Core systems","text":"<p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security.</p> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.</p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"supplychain/returns/#integration-services","title":"Integration services","text":"<p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</p> <p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing. Rules processing, intelligent decison making, and regulation compliance using automation gives business flexible, auditable, policy-based workflows across the enterprise.</p>"},{"location":"supplychain/returns/#supply-assurance-platform","title":"Supply assurance platform","text":"<p>IBM Sterling Fulfillment Optimizer with Watson to determine the best location from which to fulfill an order, based on business rules, cost factors, and current inventory levels and placement.</p> <p>IBM Planning Analytics with Watson streamlines and integrates financial and operational planning across the enterprise.</p>"},{"location":"supplychain/returns/#references","title":"References","text":"<ul> <li>IBM The future of retail returns</li> <li>Digital Commerce 360 How returns can be a retail \u2018superpower\u2019</li> <li>Digital Commerce 360 Loop processes 60,000 returns a day during 2022 holiday season</li> </ul>"},{"location":"supplychain/returns/#next-steps","title":"Next steps","text":"<p>See:</p> <ul> <li>Loss and waste management (coming soon)</li> <li>Product timeliness (coming soon)</li> <li>Perfect order (coming soon)</li> <li>Intelligent order (coming soon)</li> <li>Returns (coming soon)</li> </ul> <p>For a comprehensive supply chain overview, see Supply Chain Optimization.</p>"},{"location":"supplychain/returns/#contributors","title":"Contributors","text":"<ul> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Bruce Kyle, Sr Solution Architect, IBM Client Engineering</li> <li>Ramesh Yerramsetti, Customer Success Architect, IBM Technology</li> <li>Mahesh Dodani, Principal Industry Engineer, IBM Technology</li> <li>Thalia Hooker, Senior Principal Specialist Solution Architect, Red Hat</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> <li>Eric Singsaas, Account Technical Lead, IBM Technology</li> <li>Mike Lee, Principal Integration Technical Specialist, IBM</li> </ul>"},{"location":"supplychain/returns/#download-diagrams","title":"Download diagrams","text":"<p>View and download all of the Inventory Optimization diagrams shown in previous sections in our open source tooling site.</p> <ul> <li>PowerPoint Solution Overview: Open Solution Overview</li> <li>PowerPoint Reference Architecture: Open Workflow Diagrams</li> <li>DrawIO: Open Schematic Diagrams</li> </ul>"},{"location":"supplychain/supplychain/","title":"Supply Chain Optimization Overview","text":"<p>Retailers, manufacturers and organizations are exploring ways to better able to understand and act on changes in the market as they occur \u2013 to balance protecting margins, utilizing store and warehouse capacity and meeting delivery expectations. These sourcing decisions can dramatically increase profits, especially during peak periods.</p> <p>In addition, organizations are exploring how to provide a more sustainable footprint. Organizations are looking to redefine an enterprise-wide approach to sustainability.</p> <p>In this overview, we will discuss the business challenges, business value, and business outcomes and then provide automation and modernization actionable steps organizations can take to drive innovation and move toward a digital supply chain. These are based on The Action Guide in Own your transformation survey of 1500 CSCOs across 24 industries. Actionable steps will be developed through the lens of use cases on how the main risk factors can be transformed: </p> <p>To further describe the business problems and possible solutions a number of scenarios will be introduced. - Perfect order - Intellient Order - Demand risk - Loss and waste management - Product timeliness - Sustainable supply chain</p> <p>We will then give an overview of the solution, reference architecture, logical diagram, and how these capabilities are realized by technology capabilities.</p>"},{"location":"supplychain/supplychain/#supply-chain-concepts","title":"Supply chain concepts","text":"<p>Fulfilment is the process of exceeding customer expectations when the customer receives their requested products, good or services. The items must be made available in a suitable timeframe, at the correct location and in an acceptable condition.</p> <p>Fulfilment optimization takes the fulfillment process a step further by using information and knowledge about the supply chain, inventory and stock positions to ensure any promises made to the customer are met or exceeded.</p> <p>A key element of the retail fulfilment process is knowing the inventory position. This is the amount of stock available for sale to a customer, it\u2019s location and the time it takes to make it available to the customer. </p> <p>Inventory optimization is a collection of best practices for ensuring the retail organisation maintains complete and accurate stock levels whilst balancing customer demand against current and future stock levels.</p> <p>Sustainability in business refers to a company's strategy to reduce negative environmental impact resulting from their operations in a particular market. An organization\u2019s sustainability practices are typically analyzed against environmental, social, and governance (ESG) metrics.</p> <p>The business goal is to balance the long-term imperative to protect the planet with the immediate need to preserve the bottom line. </p>"},{"location":"supplychain/supplychain/#business-problem","title":"Business problem","text":"<p>Chief Supply Chain Officers (CSCOs) face issues related to supply chain disruptions, technology infrastructure, sustainability, and market shifts as their greatest challenges. Yet when addressed with an open mindset, challenges create opportunities within the enterprise\u2014and visibility. </p> <p>Harvard Business Review article, Three steps to prepare your supply chain for the next crisis. reports:</p> <p>Companies that are well prepared and as a result prosper in a crisis can expect to recover more quickly than their competitors. In a review of corporate performance during the past four U.S. downturns (since 1985), Boston Consulting Group (BCG) found that 14% of companies increased\u00a0their sales and their profit margin.</p> <p>Investors are starting to reward companies that build for the future by becoming more innovative and more resilient. In June 2020, during the depths of the Covid-19 pandemic, BCG surveyed major institutional investors and found that nine out of ten believed it was \"important for healthy companies to prioritize the building of business capabilities \u2014 even if it means lowering earnings-per-share guidance or delivering below consensus.\"</p> <p>The McKinsey report How COVID-19 is reshaping supply chains explains that companies found it was easier to increase inventories than implement their preferred strategy of nearshoring or regionalization.  In addition, \"The proactive monitoring of supplier risks was the primary focus \u2026, yet significant blind spots remain in most companies\u2019 supply-chain risk-management setups. Just under half of the companies in our survey say they understand the location of their tier-one suppliers and the key risks those suppliers face.\"</p> <p>Enhanced customer experiences, improved profitability, and more predictive forecasts are high on CSCOs' priority lists according to a report by IBM Institute for Business Value, Own your transformation: Data-led innovation for the modern supply chain. </p>"},{"location":"supplychain/supplychain/#business-value","title":"Business value","text":"<p>Inventory optimization is making sure the current and future demand is accurately balanced against current and future inventory across the enterprise. Getting the balance correct leads to a successful and profitable retail business. Getting the balance wrong leads to failure and in the worst case, eventual collapse of the business.</p> <p>Specifically, Harvard Business Review article recommends:</p> <p>CEOs need to invest in risk intelligence and strategic foresight, creating a team of procurement super-forecasters equipped with the latest artificial-intelligence (AI)-powered sensing technology.</p>"},{"location":"supplychain/supplychain/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <ul> <li>Addressing supply chain disruptions, technology infrastructure, sustainability and market shifts.</li> <li>Using an open mindset to address challenges thereby create opportunities.</li> </ul> <p>Drivers</p> <ul> <li>Resilient inventory management systems handle unexpected events and disruption to ensure business success</li> <li>Right product at the right time, matching customer expectations</li> <li>Enhanced customer experience with inventory matching customer demand</li> <li>Increased margins and improved net zero benefits due to reduce food waste and spoilage</li> <li>Perfect product delivery delighting customers</li> </ul>"},{"location":"supplychain/supplychain/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Business responds to unexpected changes in supply chain</li> <li>Increased customer satisfaction by meeting customer expectation for goods and delivery</li> <li>Profit, revenue increases due to reduce waste and spoilage</li> <li>Meets sustainability commitments reflected in brand awareness</li> </ul>"},{"location":"supplychain/supplychain/#impacts-on-kpis","title":"Impacts on KPIs","text":"<p>According to the IBM IBV study, innovators track well ahead of their peers when it comes to AI-enabled workflows for risk management and to handle other predictions. And they expect continued development of these workflows and other capabilities over the next three years. Right now, Innovator CSCOs report developing digitized workflows and leveraging AI automation a full 95% more than their peers.</p> <p>Innovators also stand out by leveraging data with AI and advanced analytics in demand management. With demand volatility and associated supplier, operations, and logistics disruption at all-time highs, CSCOs are applying AI and machine learning to the critical and strategic continuous planning elements of demand management and forecasting. A full 90% of Innovators use AI and advanced analytics in demand management and predictive forecasting, 18% more than their peers (76%).</p> <p>Directly influence the following KPIs:</p> <ul> <li>Improve lost sales from stock out 4-8% </li> <li>Solution cost savings 10% </li> <li>Increase stock turnover</li> <li>Reduce days on hand</li> <li>Manage lead time (may be leading indicator of stock overage)</li> <li>Reduce holding cost</li> <li>Manage product at risk/perishability/age</li> <li>Improve gross margin return on investment</li> <li>Reduce return rate</li> <li>Black swan events. Manage unusual events regarding weather, natural disaster, supply chain interruption.</li> <li>Improve customer satisfaction</li> </ul>"},{"location":"supplychain/supplychain/#solution-overview","title":"Solution overview","text":"<p>This solution combines existing systems with state-of-the-art offerings to:</p> <ul> <li>Create a world-class sensing and risk-monitoring operation. </li> <li>Accelerate automation in extended workflows</li> <li>Amp up AI to make workflows smarter</li> <li>Modernize for modern infrastructures, scale hybrid cloud platforms</li> </ul> <p>The following diagram provides a high-level overview how systems work together for the desired outcomes.</p> <p></p>"},{"location":"supplychain/supplychain/#scenarios","title":"Scenarios","text":"<p>To demonstrate the importance of inventory optimization for any business, several articles outline main use cases on how the main risk factors can be transformed:</p> <ul> <li>Perfect order</li> <li>Intellient Order</li> <li>Demand risk</li> <li>Loss and waste management</li> <li>Product timeliness</li> <li>Returns</li> <li>Disaster readiness</li> </ul> <p>The following section explore more details on each of the scenarios.</p>"},{"location":"supplychain/supplychain/#perfect-order","title":"Perfect order","text":"<p>Inventory may not be on hand or in a different location than expected. Multiple and partial shipments may be able to solve many customer requests.</p> <p>The business can respond to the imperfect order and increase customer responsiveness with:</p> <ul> <li>Intelligent promising</li> <li>Optmization user expectations with improved demand forecasting </li> <li>Automated responses</li> </ul> <p>For more information and solution details on this scenario, see Perfect order</p>"},{"location":"supplychain/supplychain/#intelligent-order","title":"Intelligent order","text":"<p>If the last mile delivery promise is incorrect and a product arrives late, there\u2019s a good chance that the customer will shop with a competitor next time.</p> <p>If last mile tracking information is incorrect, a product arrives late, is damaged, or is never delivered, there\u2019s a good chance that the customer will shop with a competitor next time.</p> <p>For more information about the solution details on this scenario, see Intelligent order.</p>"},{"location":"supplychain/supplychain/#demand-risk","title":"Demand risk","text":"<p>Understock - not holding sufficient inventory to meet current demand. This includes not having enough inventory today but also, not having enough inventory in the very near future that could be used to meet the demand.</p> <p>Overstock - holding more stock than required to meet current and future demand. This results in additional costs to store then dispose of overstocked items via discounts, selling at a loss or destruction. </p> <p>For more information about the solution details on this scenario, see Demand risk.</p>"},{"location":"supplychain/supplychain/#loss-and-waste-management","title":"Loss and waste management","text":"<p>Loss and waste management requires you to take decisive action in cases of:</p> <ul> <li>Shelf life. Identify and timely replace items as shelf life expires. NOTE: This is covered in Product timeliness</li> <li>Environmental exceptions. Food expirations, power interruption or other disaster affecting the product salability.</li> <li>Contamination or recall. Quickly identify, remove contaminated and recalled items from sale or items that have a contaminated component. Proactively provide safe alternative or replacement when safe.</li> </ul> <p>For more information about the solution details on this scenario, see Loss and waste management.</p>"},{"location":"supplychain/supplychain/#product-timeliness","title":"Product timeliness","text":"<p>Product timeliness is having goods and products in the right place at the right time, packed correctly and in line with customer expectations. This risk is particular to seasonal goods, fast fashion, drugs, cosmetics, grocery and food supply industry. KPIs relevant to product timeliness include dead stock and inventory days on hand.</p> <ul> <li>Shelf life defines a set of actions to be taken with products that expire on specific dates and must be removed from use or sale. For example, food can expire and need to be removed from shelves. </li> <li>Timeliness defines a set of actions that can be taken when products expire, but that may be still have some benefit. For example, seasonality of clothing can be warehoused until the following year.</li> </ul> <p>For more information about the solution details on this scenario, see Product timeliness.</p>"},{"location":"supplychain/supplychain/#sustainable-supply-chain","title":"Sustainable supply chain","text":"<p>Sustainability provides an opportunity to differentiate your business, yet the business must balance the long-term imperative to protect the planet with the immediate need to preserve the bottom line. On a rapidly warming planet, companies across sectors have transformed  business models to forge a sustainable future \u2010 one that protects people, planet, and profits. In the race to reduce emissions, consumption, and waste, everything is on the table. Supply chains are being recalibrated. Source materials are evolving. Travel requests are carefully scrutinized.</p>"},{"location":"supplychain/supplychain/#portfolio-architecture","title":"Portfolio architecture","text":"<p>The following diagram provides an overall portfolio architecture for the solution.</p> <p></p> <p>The portfolio architecture shows the major systems and how they relate to each other. The following sections describe the system features in more details.</p>"},{"location":"supplychain/supplychain/#logical-diagram","title":"Logical diagram","text":"<p>The following logical diagram describes a set of personas and technologies that provide a platform for some of the biggest potential for ideation and breakthroughs with supply chain.</p> <p></p> <p>The logical diagram is explained as solution tiers and personas in the following sections.</p>"},{"location":"supplychain/supplychain/#solution-tiers","title":"Solution tiers","text":"<p>The technologies can be grouped into main categories:</p> <ul> <li>User applications. Applications where supply chain activities are reported and used by customers, colleagues, suppliers, and logistics. In particular, the inventory controller interacts with the Inventory Optimization Platform, described in a following section.</li> <li> <p>Core application systems. Often customer-provided technologies, such as order management, facilities management. These include services, data, and systems currently used within the organization, such as:</p> <ul> <li>Point of Sale systems</li> <li>Store Operations Systems</li> <li>External Data Feeds</li> <li>Planning and Replentishment system</li> <li>Warehouse management</li> <li>Order management</li> <li>Supply chain system</li> <li>Transport system</li> <li>Catalog Management system</li> <li>Facility Asset Management system</li> </ul> </li> <li> <p>Inegration Services. Manages the events and data between systems, and includes:</p> <ul> <li>Integration services</li> <li>Data fabric</li> <li>Business automation</li> <li>DevOps</li> <li>API Management</li> </ul> </li> <li> <p>Supply Assurance platform including systems supporting:</p> <ul> <li>Supply assurance control tower</li> <li>Fulfillment optimization</li> <li>Inventory analysis and AI</li> <li>Supply intelligence</li> <li>Supply risk management</li> <li>Demand intelligence</li> <li>Sustainability</li> </ul> </li> <li> <p>Omni Channels</p> <ul> <li>Web applications for users</li> <li>Point of sale applications in stores</li> <li>Mobility and device applications</li> <li>Social media</li> <li>Email</li> <li>Parter applications</li> </ul> </li> </ul>"},{"location":"supplychain/supplychain/#personas","title":"Personas","text":"<p>The following stakeholders within the organization.</p> <ul> <li>Executive sponsors. Global Supply Chain VP/Officer, VP Global Supply Chain Operations, IT Innovation, eComm Inventory Controller, Merchandize Logistics Manager, Store Operations VP/Lead</li> <li>Influencers. Inventory Control Specialists, Inventory management director, Supply chain professional, store inventory manager, fulfillment manager, inventory analyst, financial control officer/controller, Demand forecaster analyst, Inventory planning</li> <li>Operations. Warehouse managers, logistics managers</li> </ul> <p>The following represent users of the system outside of the organization.</p> <ul> <li>Customers</li> <li>Business partners</li> <li>Vendors</li> </ul>"},{"location":"supplychain/supplychain/#the-technology-capabilities","title":"The technology capabilities","text":"<p>This section provides the logical components of the solution. The solution architecture provides for:</p> <ul> <li>Independent deployable, scaled business services</li> <li>Composable, stateless modular services </li> <li>API first approach to building any service</li> <li>Inter communication between services asynchronous</li> <li>Business functions, API access controls managed independently</li> <li>Can be deployed across multiple data centers for HA capable of tenant level failover</li> </ul>"},{"location":"supplychain/supplychain/#foundational-technologies-hybrid-cloud-platform","title":"Foundational technologies \u2010 Hybrid Cloud Platform","text":"<p>The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point. Key client value areas include:</p> <ul> <li>Platform for Digital Transformation and Modernization</li> <li>Accelerated application development for faster go-to-market and innovation</li> <li>Engineering excellence through an enterprise grade open source container platform</li> <li>Self-service transformation with DevOps/SRE and automated operations</li> </ul> <p>Specifically, the recommended platform includes:</p> <ul> <li>Red Hat Enterprise Linux is the world\u2019s leading enterprise Linux platform. It\u2019s an open-source operating system (OS). It\u2019s the foundation from which you can scale existing apps\u2014and roll out emerging technologies\u2014across bare-metal, virtual, container, and all types of cloud environments.</li> <li> <p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with </p> <ul> <li>Red Hat Advanced Cluster Management </li> <li>Red Hat Advanced Cluster Security</li> </ul> </li> <li> <p>Business Automation Workflow unites information, processes and users to help you automate digital workflows on premises or on cloud. Create workflows that increase productivity, improve collaboration between teams, and gain new insight to resolve cases and drive better business outcomes.</p> </li> <li>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.  </li> <li>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</li> <li>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</li> <li>Red Hat OpenShift DevOps reoresents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure. </li> <li> <p>IBM Data Fabric empowers your teams and works across the ecosystem by connecyting data from disparate data sources in multicloud envrionments. In particular:</p> <ul> <li>Watson Knowledge Catalog provides you users with a catalog tool for intelligent, self-service discovery of data, models. </li> <li>Watson Query provides data consumers with a universal query engine that executes distributed and virtualized queries across databases, data warehouses, data lakes, and streaming data without additional manual changes, data movement or replication. </li> </ul> </li> </ul>"},{"location":"supplychain/supplychain/#core-application-systems","title":"Core application systems","text":"<p>The core application systems can be in-house applications, cloud services, IBM or competitive applications. The core applications provide data through the foundational technologies (such as API management that provides monitoring and security). They can also respond to automated actions based on business rules or from other layers in the system.</p>"},{"location":"supplychain/supplychain/#supply-assurance-platform","title":"Supply Assurance Platform","text":"<p>Each of the core application systems provides data and responds to events through the foundational technologies. </p> <p>Supply assurance control tower. Gartner describes supply chain technology as a central hub as an integrated part of a broader SCM platform using these building blocks: people, process, data, organization and technology. The idea is to capture and use data to provide enhanced real-time visibility and in-depth analysis. IBM Supply Chain Control Tower, powered by industry-leading AI, provides actionable visibility to orchestrate your end-to-end supply chain network, identify and understand the impact of external events to predict disruptions, and take actions based on recommendations to mitigate the upstream and downstream effects. </p> <p>Fulfillment optimization. Gartner describes the fulfillment forecasting approach, which provides a more accurate view of consumer fulfillment choices. This approach enables retailers to accurately gauge the right amount of inventory required to meet demand in stores, distribution centers and other inventory holding notes throughout the retailer\u2019s network. IBM offers IBM Sterling Intelligent Promising  provides shoppers with greater certainty, choice and transparency across their buying journey. </p> <p>Inventory analysis and AI. This cognitive analytic engine enhances existing order management systems. It determines the best location from which to fulfill an order, based on business rules, cost factors, and current inventory levels and placement. IBM includes IBM Sterling Fulfillment Optimizer with Watson as a component of IBM Sterling Intelligent Promising.</p> <p>Supply risk management. Gartner describes the key tenets of supply chain risk management enhance resilience and improve competitiveness. </p> <ul> <li>Market disruptions include: natural disasters, pandemics, political uncertainty, economic upheaval, cyber and terrorist attacks, third-party or supplier threats, and rapid swings in consumer preferences and demand.</li> <li>Supply chain transformation also complicates supply chain risk management \u2010 lean, but complex and globally dispersed operations add risks, and the supply chain risk management process is vital in wringing out efficiencies and costs.</li> </ul> <p>Inventory visibility. This processes inventory supply and demand activity to provide accurate and real-time global visibility across selling channels. For this solution, IBM offers Sterling Inventory Visibility, which is included with IBM Sterling Intelligent Promising. </p> <p>Planning and analytics. Businesses need to evolve their planning and analysis strategies to include continuous, integrated planning. This means creating a single source of truth to streamline planning, manage performance and build alignment across the enterprise. In this category IBM offers:</p> <ul> <li>IBM Planning Analytics with Watson to streamline and integrate financial and operational planning across the enterprise.</li> <li>IBM Maximo MRO Inventory Optimization to help you optimize your maintenance, repair and operations (MRO) inventory by providing an accurate, detailed picture of performance.</li> </ul> <p>Sustainability. To put your sustainability goals into action, you need a strategy and solutions that are specific to your needs. Businesses can approach sustainability initiatives by:</p> <ul> <li>Strategic partnership with Red Hat and IBM.</li> <li>Using environmental, social and governance (ESG) reporting to integrate data silos. Find new opportunities to drive positive change across your operations with Envizi.</li> <li>To manage assets, you can create a lower-emissions business. Use automation to track resource usage, reduce costs and improve services with IBM Maximo Application Suite.</li> </ul>"},{"location":"supplychain/supplychain/#action-guide","title":"Action guide","text":"<p>From a high-level perspective, several main steps are suggested for your organization to drive innovation and move toward a digital supply chain.</p>"},{"location":"supplychain/supplychain/#automation","title":"Automation","text":"<ul> <li>Accelerate automation in extended workflows </li> <li>Amp up AI to make workflows smarter </li> <li>Cultivate collaborative ecosystems </li> </ul>"},{"location":"supplychain/supplychain/#sustainability","title":"Sustainability","text":"<ul> <li>Link environmental and social initiatives with business solutions </li> <li>Optimize workflows with AI to manage carbon, waste, energy, and water consumption </li> <li>Compete with new sustainable products and services </li> </ul>"},{"location":"supplychain/supplychain/#modernization","title":"Modernization","text":"<ul> <li>Architect modern infrastructures </li> <li>Scale hybrid cloud platforms </li> <li>Increase awareness of cybersecurity vulnerabilities and solutions</li> </ul> <p>For specific steps on this approach, see The Action Guide details in Own your transformation survey of 1500 CSCOs across 24 industries.</p>"},{"location":"supplychain/supplychain/#next-steps","title":"Next steps","text":"<p>See Action Guide section in each of the use cases:</p> <ul> <li>Perfect order</li> <li>Intelligent order</li> <li>Demand risk</li> <li>Loss and waste management</li> <li>Product timeliness</li> <li>Returns</li> <li>Disaster readiness</li> </ul>"},{"location":"supplychain/supplychain/#references","title":"References","text":"<ul> <li>McKinsey: How COVID-19 is reshaping supply chains</li> <li>Harvard Business Review: Three steps to prepare your supply chain for the next crisis</li> <li>Gartner: What is a Supply Chain Control Tower and what's needed to deploy one</li> <li>Gartner: Fulfillment Forecasting: The Key to Optimizing Retail Inventory Positioning</li> <li>Gartner: Supply Chain Risk Management (SCRM): What &amp; Why Is It Important</li> <li>IBM Institute for Business Value: Own your transfomation</li> <li>IBM Institute for Business Value: Balancing sustainability and profitability</li> <li>What is sustainability in business?</li> </ul>"},{"location":"supplychain/supplychain/#downloads","title":"Downloads","text":"<p>View and download all of the Inventory Optimization diagrams shown in previous sections in our open source tooling site.</p> <ul> <li>PowerPoint Solution Overview: Open Solution Overview</li> <li>PowerPoint Reference Architecture: Open Workflow Diagrams</li> <li>DrawIO: Open Schematic Diagrams</li> </ul>"},{"location":"supplychain/supplychain/#contributors","title":"Contributors","text":"<ul> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Mike Lee, Principal Integration Technical Specialist, IBM</li> <li>James Stewart, Principle Account Technical Leader, IBM</li> <li>Bruce Kyle, Sr Solution Architect, IBM Client Engineering</li> <li>Mahesh Dodani, Principal Industry Engineer, IBM Technology</li> <li>Thalia Hooker, Senior Principal Specialist Solution Architect, Red Hat</li> <li>Rajeev Shrivastava, Account Technical Lead, IBM</li> <li>Ashok Iyengar, Executive Cloud Architect, IBM</li> <li>Karl Cama, Chief Architect, Red Hat</li> <li>Jeric Saez, Senior Solution Architect, IBM</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> <li>Ramesh Yerramsetti, Customer Success Architect, IBM</li> </ul>"},{"location":"supplychain/timeliness/","title":"Product timeliness","text":"<p>Foods and ingredients expire or become unusable at some point. Manufactured parts and goods also experience decay and deterioration. Each of these may be measured.</p> <p>In the food industry, you may see different terms and date types on the packaging, the following are common practices and their meanings according to the USDA and FDA as explained in Expiring Products \u2013 Food &amp; Ingredients by Michigan State University:</p> <ul> <li>Best if used by/before: this term is NOT an expiration date, rather it\u2019s the date where the food may be at its best quality </li> <li>Use by: on any food OTHER than infant formula, this term is NOT an expiration date, rather it\u2019s the date where the food may be at its best quality. If \u201cused by\u201d is present on infant formula, it\u2019s an expiration date, and you should discard the infant formula after the use by date.  </li> <li>Sell by: this term is NOT an expiration date, rather it\u2019s the date where the food may be at its best quality </li> <li>Freeze by: this term is NOT an expiration date, rather it\u2019s the date where the food should be frozen to maintain the best quality </li> <li>Expiration or EXP: this is an expiration date and usually only found on infant formula and some baby foods </li> <li>Guaranteed fresh: this term is NOT an expiration date, rather it\u2019s the date where the food may be at its best quality </li> </ul> <p>Other products, particularly pharmaceuticals, must be used within specific time frames to be warantted or to be considered safe and effective. </p> <p>Manufactured parts may be warranteed for specific periods or number of uses. For example, manufactured parts must be replaced when outside of certain tolerances.</p> <p>Some products may still continue to be useful beyond their expiration date or use. </p> <p>Products such as seasonal goods may not have, or require an expiriation date. In these cases the product should ideally be sold at the most relevant time within the calendar year. Once the seasonal timeframe has passed, the goods can still be sold, but demand and price can reduce. For any goods remaining once the season and demand has passed, the retailer must decide whether to discount the goods, pass to a third-party or retain and attempt to sell in the next season.</p> <p>For a comprehensive inventory solution overview, see Inventory Optimization.</p>"},{"location":"supplychain/timeliness/#business-problem","title":"Business problem","text":"<p>Product liability claims can wreck a business. </p> <p>Defects and liability risks affect product manufacturers, designers, distributors, wholesalers, and retailers. Physical or digital items that omit adequate warnings, injure a third party, or cause a wrongful death can lead to lawsuits for any party involved in the product.</p>"},{"location":"supplychain/timeliness/#use-cases","title":"Use Cases","text":"<p>How should a business track and respond to issues?</p> <ul> <li>Responding to Shelf life defines a set of actions to be taken with products that expire on specific dates and must be removed from use or sale. For example, food can expire and need to be removed from shelves. </li> <li>Relevance defines a set of actions that can be taken when products are no longer relevance, possibly due to seasonality, but that may be still have some benefit. For example, seasonality of clothing can be warehoused until the following year.</li> </ul> <p>In both shelf life and relevance, you will want to plan and to take proactive steps in anticipation of product expirations.</p>"},{"location":"supplychain/timeliness/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <ul> <li>Protect public safety by automating tracking and remediating items for shelf life and relevance</li> <li>Mitigate risks through specific actions, such as product removal, replacement, work order tracking to demonstrate the product is handled with due care</li> </ul> <p>Drivers</p> <ul> <li>Improve Public safety through automated shelf life tracking and management</li> <li>Reduce product liability claims through automation and AI</li> <li>Demonstrate due diligence for safety</li> <li>Identify and manage stock with reducing relevance to reduce financial loss</li> </ul>"},{"location":"supplychain/timeliness/#responses","title":"Responses","text":"<p>Many goods offered in retail and supplier businesses expire. </p> Business problem Solution Manual processes, limited capabilities of inventory management tools, and global operations pose a challenge for enterprises to manage and act on inventory and mitigate disruptions to meet actual demand. Monitor and manage network inventory availability and anticipate actions due to expiration of products with alerts and recommended actions. The lack of pertinent product information (remaining shelf life, ambient temperature, etc.) and poor data flow across partners lead to inefficient inventory management, wastage and lost sales. Gain detailed visibility into inventory characteristics at each location \u2013 e.g., by remaining shelf life, time-since-harvested. Maintain freshness by acting on alerts received when items are at risk. Visibility into actions needed and alternatives to anticipate and respond to inventory as items approach end of shelf life. Provide actionable tasks, work orders, visibility for workers and supply chain partners to remove end of life items. Proactively replace items in response to product timeliness."},{"location":"supplychain/timeliness/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Proactively protect public safety</li> <li>Mitigate product liability claims</li> <li>Demonstrate due diligence for safety</li> <li>Reduce financial loss due to reduced relevance</li> </ul>"},{"location":"supplychain/timeliness/#solution-overview","title":"Solution overview","text":"<p>This solution focuses on Automation and Modernization in our Action Guide as shown in the following diagram:</p> <ul> <li>Create a world-class sensing and risk-monitoring operation. </li> <li>Accelerate automation in extended workflows</li> <li>Amp up AI to make workflows smarter</li> <li>Modernize for modern infrastructures, scale hybrid cloud platforms</li> </ul> <p></p> <p>The solution uses the following technologies, which can be grouped into three main categories as shown in the following diagram:</p> <ul> <li>Core application systems. Often customer-provided technologies, such as order management, facilities management. These systems can be stand-alone applications, on premises and cloud services, databases. </li> <li>Foundational infrastructure. The Red Hat/IBM solution is built on RedHat OpenShift. Data is routed through API management. Events are routed through Business Automation tools such as Business Automation Workshop. </li> <li>Inventory Optimization platform</li> </ul>"},{"location":"supplychain/timeliness/#solution-principles","title":"Solution Principles","text":"<p>True end-to-end visibility. Remove data silos and create a unified view across supply chain data with a standard data platform. Personalized dashboards and insights provide a 360-degreee view of KPIs and significant events.</p> <p>Manage by exception. Detect, display, and prioritize work tasks in real time. This allows clients to sense and react to issues quickly while managing risks and disruptions in a supply chain proactively.</p> <p>Intelligent workflows. Actionable workflows can be customized to meet unique requirements and process steps required to automate actions within source transactional systems. Make informed decisions with a supply chain virtual assistant that provides responses to issues based on a client\u2019s supply chain data using natural language search.</p>"},{"location":"supplychain/timeliness/#shelf-life","title":"Shelf life","text":"<p>The following diagram shows the schematic for the shelf life use case.</p> <p></p> <p>Shelf life steps:</p> <ol> <li>Inventory Control Tower hueristics determines product inventory is near its \"Use by\" date</li> <li>Inspects current inventory</li> <li>Notifies the Inventory Controller to take action</li> <li>Creates replenishment order</li> <li>Engages partners (Charity, recycler) to remediate expired or near expired stock</li> </ol>"},{"location":"supplychain/timeliness/#relevance","title":"Relevance","text":"<p>The following diagram shows the schematic for the relevance use case.</p> <p></p> <p>Relevance steps:</p> <ol> <li>Demand Intelligence determines requirements for seasonal, product trends and time sensitive future inventory <li>Demand Intelligence informs Control Tower of future inventory to meet seasonal and product trends <li>Control Tower collects Inventory (3a) and Supply Chain Intelligence (3b) information to understand current position and ability to meet future demand <li>Control Tower determines current and future Inventory does not match inventory timeliness requirements for business and automates changes to Inventory via Fulfilment Optimiztion <li>For any non-automated changes, alerts Colleague to take remediation action <li>Colleague runs \u201cwhat-if\u201d analysis in Control Tower to determine best course of actions using Inventory and Demand data <li>Colleague triggers Business Automation to remediate stock levels using a combination of options, including: <ol> <li> Adjust product orders     <li> Managing inventory held at existing stores or by moving existing inventory     <li> Select alternative products"},{"location":"supplychain/timeliness/#use-case-and-benefit","title":"Use case and benefit","text":"Use Case The Problem The Solution The Benefits and Implications Automated processes Manual input and follow up Business automation provides a systematic way to notifications, documentation of notifications, and creation of work orders. Actions follow a consistent business process and can be easily updated as needs change Expiring products Expiring item situations lead to lost revenue and decreased brand / retailer loyalty. Control Tower monitors inventory levels at all locations in a client\u2019s network and creates items in the work queue when revenue is at risk. When drilling down on the item, users can see where they have available inventory and receive recommendations about how much inventory can and should be ordered for replacement based on demand. Action can be taken directly from the Control Tower user interface. Expiring product situations are efficiently managed and OOS are avoided with minimal human intervention. API Management Separation of systems, control and monitoring of access, providing consistent user authentication and security between platforms. API Manages the access and permissions required for data between systems. Improved security, monitoring of frequency of access between systems. Improved system maintainability."},{"location":"supplychain/timeliness/#action-guide","title":"Action Guide","text":"<p>From a high-level perspective, there are several main steps your organization can take to drive innovation and move toward a digital supply chain:</p> <ul> <li>Automation</li> <li>Sustainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automation Create a world-class sensing and risk-monitoring operation Integrate data from multiple systems to get enterprise-wide view of changes in inventory demand. Monitor and analyze near real-time data Automation Accelerate automation in extended workflows Provide actionable tasks, work orders, visibility for workers and supply chain partners to remove end of life items. Proactively replace items in response to product timeliness. Automation Amp up AI to make workflows smarter For expiring products, Control Tower monitors inventory levels at all locations in a client\u2019s network and creates items in the work queue when revenue is at risk. When drilling down on the item, users can see where they have available inventory and receive recommendations about how much inventory can and should be ordered for replacement based on demand. Sustainability Include sustainability commitments in decision making Decision making includes sustainability in handing items being removed from stock. Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point."},{"location":"supplychain/timeliness/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p>"},{"location":"supplychain/timeliness/#core-systems","title":"Core systems","text":"<p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security.</p> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.</p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"supplychain/timeliness/#integration-services","title":"Integration services","text":"<p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</p> <p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing.</p>"},{"location":"supplychain/timeliness/#supply-assurance-platform","title":"Supply assurance platform","text":"<p>IBM Supply Chain Control Tower provides actionable visibility to orchestrate your end-to-end supply chain network, identify and understand the impact of external events to predict disruptions, and take actions based on recommendations to mitigate the upstream and downstream effects.</p> <p>IBM Sterling Intelligent Promising provides shoppers with greater certainty, choice and transparency across their buying journey. It includes:</p> <ul> <li>IBM Sterling Fulfillment Optimizer with Watson to determine the best location from which to fulfill an order, based on business rules, cost factors, and current inventory levels and placement</li> <li>Sterling Inventory Visibility to processes inventory supply and demand activity to provide accurate and real-time global visibility across selling channels.</li> </ul> <p>IBM Planning Analytics with Watson streamlines and integrates financial and operational planning across the enterprise.</p> <p>Envizi simplifies the capture, consolidation, management, analysis and reporting of your environmental, social and governance (ESG) data.</p>"},{"location":"supplychain/timeliness/#similar-use-cases","title":"Similar use cases","text":"<p>See:</p> <ul> <li>Inventory management</li> <li>Demand risk</li> <li>Loss and waste management</li> <li>Intelligent order</li> <li>Returns</li> <li>Disaster readiness</li> </ul> <p>For a comprehensive supply chain overview, see Supply Chain Optimization.</p>"},{"location":"supplychain/timeliness/#downloads","title":"Downloads","text":"<p>View and download all of the Inventory Optimization diagrams shown in previous sections in our open source tooling site.</p> <ul> <li>PowerPoint Solution Overview: Open Solution Overview</li> <li>PowerPoint Reference Architecture: Open Workflow Diagrams</li> <li>DrawIO: Open Schematic Diagrams</li> </ul>"},{"location":"supplychain/timeliness/#contributors","title":"Contributors","text":"<ul> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Mike Lee, Principal Integration Technical Specialist, IBM</li> <li>James Stewart, Principle Account Technical Leader, IBM</li> <li>Bruce Kyle, Sr Solution Architect, IBM Client Engineering</li> <li>Mahesh Dodani, Principal Industry Engineer, IBM Technology</li> <li>Thalia Hooker, Senior Principal Specialist Solution Architect. Red Hat</li> <li>Jeric Saez, Senior Solution Architect, IBM</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> </ul>"},{"location":"sustainableenterprise/assetlifecycle/","title":"Asset lifecycle management","text":"<p>By driving greater efficiencies in managing assets, organizations can extend asset life, reduce Capex and maintenance expenses, and improve technician productivity. You\u2019ll find these needs across critical industries: energy and utilities, chemical and petroleum, auto, aerospace and defense, as well as electronics, retail, consumer packaged goods, and government.</p> <p>By using this cradle-to-grave method allows the company to integrate with environmental reporting processes and incentivize sustainability improvement across the organization. Gain insights into its global waste profile, the company can better engage the waste management team, supply chain team, equipment manufacturers and waste\u00a0haulers.\u00a0</p>"},{"location":"sustainableenterprise/assetlifecycle/#use-cases","title":"Use cases","text":"<p>Organizations want to enhance the tracking of assets and associated material components across the enterprise. Through improved visibility into job activities and an asset's lifecycle, the organization can better understand and drive change using data-driven insights. </p>"},{"location":"sustainableenterprise/assetlifecycle/#background","title":"Background","text":"<p>As outlined in a Pennsylvania State University study, \"Hazardous Waste is any Waste that has been designated, characterized or otherwise regulated as hazardous by applicable laws or regulations in a country, state, region, or locality. Examples can include a corrosive liquid or a flammable solvent.</p> <p>\"Product End of Life Management (PELM) refers to the demanufacturing, dismantling, reuse, reclamation, recycling, shredding, treatment and / or disposal of end-of-life (EOL) IT equipment and parts.\"</p>"},{"location":"sustainableenterprise/assetlifecycle/#business-problem","title":"Business problem","text":"<p>The business problem consists of lifecycle asset management and proper asset disposal.</p> <p>Automate procurement, maintenance to support more than tens of thousands or assets and improves inspector productivity and reduce safety incidents.</p> <p>Then when the asset is at end of life:</p> <ul> <li>Management (including monitor the collection, transportation, recycling treatment or disposal) of hazardous and nonhazardous special waste that comes from internal operations (e.g., facility operations, on-site waste water treatment, research, manufacturing processes, and environmental remediation programs). Such byproducts are reduced and recycle when possible, followed by treatment and disposal (incinerated and/or sent to landfills).</li> <li>Second are products or their parts and components that have reached their intended end of life (a.k.a PELM). These materials may be generated from your own operations or by your clients. These PELM materials may still have value and can be refurbished and resold or, may be disassembled and their parts harvested for reuse or scrapped. </li> </ul> <p>The output of the scrapping process may involve recoverable materials (e.g., ferrous and non-ferrous metals, precious metals, glass, plastics, etc.) that have intrinsic value and are sold in secondary commodity markets, or waste materials (e.g., non-recyclable packaging material, non-recyclable plastics, shredder fluff, etc.) that have no value and require proper treatment or disposal. </p> <p>Waste materials may be considered hazardous or nonhazardous depending on government designation.</p> <p>Often the disposal of hazardous wastes are contracted through third parties.</p> <p>As one example, for the waste that is generated, IBM focuses on preventing pollution through a comprehensive, proactive waste management program. IBM's waste management practices, in order of preference, are: (1) prevention and source reduction, (2) reuse, (3) recycling, (4) recovery, (5) other treatment, and (6) land disposal.</p>"},{"location":"sustainableenterprise/assetlifecycle/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <ul> <li>Tracking tens of thousands of assets, maintenance windows, and work orders during an asset's lifetime.</li> <li>Integration with procurement systems, building maintenance systems, supply chain systems.</li> <li>Predicting the possible failure and taking mitigating action to prevent asset failure.</li> <li>Waste management team manually gathers disposal data from individual haulers for each facility in the organization.</li> <li>Manual data collection method limits internal data validation to sporadic certification spot-checks.</li> <li>No real-time visibility into hazardous waste lifecycles or raw materials to actively engage with vendors and ensure hazardous waste compliance and forecasting.</li> <li>Field teams manage the end-to-end lifecycle and disposal process in spreadsheets.</li> <li>The recycling process is manually managed by external regional haulers.</li> </ul> <p>Business drivers</p> <ul> <li>Extend asset life, reduce Capex and maintenance expenses, and improve technician productivity</li> <li>Meeting strict compliance with regional hazardous waste management standards</li> <li>Setting and meeting environmental waste targets</li> <li>Streamline replacement work order management and asset tracking process</li> <li>Tracability and controls of waste disposal</li> <li>Advise on automation replacement work-orders to engage with waster haulers for reconcilation</li> <li>Streamline collection, tracking and reporting of hazardout materials</li> <li>Monitor and forecast asset replacement and waste targets</li> </ul>"},{"location":"sustainableenterprise/assetlifecycle/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Improved management of regulatory compliance and third-party audit certification.</li> <li>Enhanced monitoring by tracking against zero waste targets and diversion rate per location.</li> <li>Enhanced visibility of entire lifecycle from consumption to responsible disposal and recycling.</li> <li>Improved visibility, monitoring and management of assets and consumables including hazardous and non-hazardous waste.</li> <li>Gaining both global view and site level view of consolidated environmental sensitive cradle-to-grave equipment and parts.</li> </ul>"},{"location":"sustainableenterprise/assetlifecycle/#solution-overview","title":"Solution overview","text":"<p>The solution shown in Figure 1 uses components that can be grouped into three main categories as shown in the following diagram:</p> <ul> <li>Core application systems. Often customer-provided technologies, such as order management, facilities management. These systems can be stand-alone applications, on premises and cloud services, databases. </li> <li>Foundational infrastructure. The Red Hat/IBM solution is built on Red Hat OpenShift. Data is routed through API management. Events are routed through Business Automation tools such as Business Automation Workshop.</li> <li>Sustainable enterprise systems acts to coordinate facilities management with workplace management backed by sustainability reporting.</li> </ul> <p></p> <p>Figure 1. Overall view of sustainable facilities solution.</p> <p>The sustainable enterprise works within the existing enterprise infrastructure.</p> <p></p> <p>Figure 2. Sustainable enterprise works within existing digital infrastructure.</p>"},{"location":"sustainableenterprise/assetlifecycle/#logical-diagrams","title":"Logical diagrams","text":"<p>Figure 3. The personas and technologies that provide a platform for some of the biggest potential breakthroughs in the managing sustainability assets.</p>"},{"location":"sustainableenterprise/assetlifecycle/#architecture","title":"Architecture","text":"<p>The figures in this section show the interaction of sustainability data to your customer systems.</p>"},{"location":"sustainableenterprise/assetlifecycle/#asset-life-management","title":"Asset life management","text":"<p>Assets module contains applications that are designed to manage the assets that are owned or leased by your company from purchase to salvage, from the beginning to the end of the life cycle for an asset.</p> <p></p> <p>Figure 4. Schematic diagram of the asset life cycle management case.</p> <p>Asset life cycle management workflow steps:</p> <ol> <li>Create workflow to manage asset including:     a. Asset purchase approval     b. Procurement     c. Asset inventory</li> <li>Add asset in asset management system including location, relationships to related assets (parts or assemblies)</li> <li>Create job plan of steps to be taken to perform preventative maintenance or replacement </li> <li>Monitor asset health, receive sensor data</li> <li>Predict asset failures</li> <li>Provide work order and tracking to respond to maintenance and replacement</li> <li>Dispose of item at asset end of life</li> </ol>"},{"location":"sustainableenterprise/assetlifecycle/#asset-disposal","title":"Asset disposal","text":"<p>In this figure, asset management software provides data and AI for decision making to drive efficiencies in asset disposal.</p> <p></p> <p>Figure 5. Schematic diagram of the asset disposal use case.</p> <ol> <li>Asset identified to be replaced, either proactively or due to asset failure</li> <li>Asset replacement decision by asset manager</li> <li>Work order created to:     a. Order replacement asset ordered     b. Work order created to replace asset     c. Completion of work order reported     d. Assess disposal, recycle process work order provided to regional hauler</li> <li>Regional hauler     a. Coordinate recycle or hazardous disposal documentation with regional hauler     b. Use Transparent Supply to track hazardous waste disposal      c. In locations where local processing of waste is not possible, store wastes and product end-of-life materials in properly managed storage facilities, as allowed by law</li> <li>Sustainable supply provides data to the control tower to provide compliance reporting on hazardous waste disposal</li> <li>Sustainability officer monitor and forecast against zero waste targets</li> </ol>"},{"location":"sustainableenterprise/assetlifecycle/#action-guide","title":"Action Guide","text":"<p>From a high-level perspective, the Action Guide represents a future state for organizations considering a comprehensive commitment. The idea is to outline a set steps that can be prioritized to reach that future state by adding new functionality to your existing systems.</p> <ul> <li>Automation</li> <li>Sustainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automation Automate the collection of sustainability data Reduce manual processing of data Automation Amp up AI to make workflows smarter Participants add their data and supporting documents like certifications to the ledger and control who is allowed to see what. Once added to the ledger, data cannot be manipulated, changed or deleted. Participants can track materials and products from source to end customer and, ultimately, the consumer. Sustainability Include sustainability data in decision making Integrate sustainability metrics in supply chain, facility management, and data center operations Sustainability Track sustainability data within your supply chain Engage vendors and partners to provide sustainability data as part of your purchasing requirements Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point. Modernization Modernize application deployment and operations practices Include DevOps best practices to deploy, monitor, and maintain applications <p>For specific steps on this approach, see The Action Guide details in Own Your Transformation survey of 1500 CSCOs across 24 industries.</p>"},{"location":"sustainableenterprise/assetlifecycle/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p>"},{"location":"sustainableenterprise/assetlifecycle/#core-systems","title":"Core systems","text":"<p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security.</p> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.</p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"sustainableenterprise/assetlifecycle/#integration-services","title":"Integration services","text":"<p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</p> <p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing. </p> <p>IBM Data Fabric empowers your teams and works across the ecosystem by connecyting data from disparate data sources in multicloud envrionments. In particular, Watson Knowledge Catalog provides you users with a catalog tool for intelligent, self-service discovery of data, models. Watson Query provides data consumers with a universal query engine that executes distributed and virtualized queries across databases, data warehouses, data lakes, and streaming data without additional manual changes, data movement or replication. </p>"},{"location":"sustainableenterprise/assetlifecycle/#sustainable-enterprise-systems","title":"Sustainable enterprise systems","text":"<p>Envizi simplifies the capture, consolidation, management, analysis and reporting of your environmental, social and governance (ESG) data.</p> <p>IBM TRIRIGA harnesses the power of data and AI to infuse sustainability into your real estate and facilities management operations.</p> <p>IBM Maximo Application Suite (MAS) delivers greater sustainability through intelligent asset management, monitoring, predictive maintenance, computer vision, safety and reliability, all in a single platform..</p> <p>IBM Turbonomic monitors resource consumption of applications within the data center. It provides FinOps engineering teams the ability to ensure your applications are performing efficiently, allowing cloud and ITOps teams to cut cloud spend and multiply ROI.</p> <p>Transparent Supply provides supply chain management with a robust traceability solution.</p>"},{"location":"sustainableenterprise/assetlifecycle/#references","title":"References","text":"<ul> <li>IBM Announces 21 Environmental Sustainability Goals</li> <li>Pennsylvania State University Selecting and Evaluating Environmentally Responsible Suppliers of Hazardous, Nonhazardous Special Waste and Product End-Of-Life Management Services</li> <li>IBM Blog Uncover real savings from enterprise asset management no matter the industry</li> <li>IBM Blog How to create more sustainable operations \u2013 one asset at a time</li> <li>IBM Supplier requirements</li> </ul>"},{"location":"sustainableenterprise/assetlifecycle/#contributors","title":"Contributors","text":"<ul> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Mahesh Dodani, Principal Industry Engineer, IBM Technology</li> <li>Thalia Hooker, Senior Principal Specialist Solution Architect, Red Hat</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> <li>Eric Singsaas, Account Technical Lead, IBM Technology</li> <li>Mike Lee, Principal Integration Technical Specialist, IBM</li> <li>Rajeev Shrivastava, Account Technical Lead, IBM</li> <li>Bruce Kyle, Sr Solution Architect, IBM Client Engineering</li> </ul>"},{"location":"sustainableenterprise/energy/","title":"Smart energy","text":"<p>Energy represents a significant cost in the business.</p> <p>According to a paper recommended by the United States Department of Energy:</p> <p>One of the most lucrative areas for improving bottom line profitability is related to an organization`s costs for utilities and energy. Such things as gas, electric, water, and telephones are treasure chests of cost reduction opportunities. In the past, these items have been viewed as a fixed expense or basic mundane commodity. In recent years, these items have become a large portion of product cost and now must be examined on a continual basis. A formal income improvement program to capture and report on the savings is a requirement for remaining competitive in a global economy. \u2014 From Utility and energy cost containment</p>"},{"location":"sustainableenterprise/energy/#use-cases","title":"Use cases","text":"<ul> <li>Impact the costs due to regrigerators, heating and cooling, and IT</li> <li>Connected edge devices</li> <li>Provide data to demonstrate energy cost containment results</li> <li>Implement price controls and contractor benchmarking</li> <li>Automate maintenance, repair, and operations tracking</li> </ul>"},{"location":"sustainableenterprise/energy/#background","title":"Background","text":"<p>Most companies are committed to actions that make your operations more sustainable. At this point, some companies have set and published specific targets.</p> <p>Businesses are seeking to balance the long-term imperative to protect the planet with the short-term need to preserve the bottom line. </p>"},{"location":"sustainableenterprise/energy/#business-problem","title":"Business problem","text":"<p>Businesses need a data-driven solution to demonstrate energy cost savings. Intelligent decision making of specific steps to determine costs of energy providers, capital improvement projects, energy conservation, modernization of infrastructure.</p> <p>How do you determine the ROI on these steps to drive your sustainability goals?</p>"},{"location":"sustainableenterprise/energy/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <p>According to an IBM Institute of Business Value study, 48% of CEOs across industries say increasing sustainability is one of the highest priorities for their organization in the next two to three years. However, 51% also cite sustainability as among their greatest challenges in that same timeframe, with hurdles, such as:</p> <ul> <li>Lack of data insights</li> <li>Unclear ROI</li> <li>Technology barriers, as hurdles. </li> </ul> <p>For these CEOs, scaling business with modern infrastructure can often be one of the barriers to achieving sustainability goals.</p> <p>Business Drivers</p> <ul> <li>Energy represents a significant cost in the business</li> <li>Demonstrate ROI with data-driven energy cost savings</li> <li>Investment in energy cost savings can have ROI</li> <li>Facility maintenance responses can provide energy cost savings</li> </ul>"},{"location":"sustainableenterprise/energy/#responses","title":"Responses","text":"Business Problem Solution Manual processes for assembling energy reporting data Automate data collection activities, such as energy billing reporting and consumption Assembing sensor data in near real time Provide data systems to collect, manage, and respond to anomalies Energy reporting silos Automate data collection activities across silos Sensors provide too much data Scale sensor data collection using event streaming and anomolie detection using AI Energy consumption disclosure IBM participates in various external disclosures, such as the Carbon Disclosure Project (CDP), Global Reporting Initiative (GRI) and the Sustainability Accounting Standards Board (SASB) Index."},{"location":"sustainableenterprise/energy/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Data-based selection of energy saving priorities</li> <li>Demonstratable returns on energy savings</li> <li>Improved facilities management</li> </ul>"},{"location":"sustainableenterprise/energy/#solution-overview","title":"Solution overview","text":"<p>The solution shown in Figure 1 uses components that can be grouped into three main categories as shown in the following diagram:</p> <ul> <li>Core application systems. Often customer-provided technologies, such as order management, facilities management. These systems can be stand-alone applications, on premises and cloud services, databases. </li> <li>Foundational infrastructure. The Red Hat/IBM solution is built on Red Hat OpenShift. Data is routed through API management. Events are routed through Business Automation tools such as Business Automation Workshop.</li> <li>Sustainable enterprise systems acts to coordinate facilities management with workplace management backed by sustainability reporting.</li> </ul> <p></p> <p>Figure 1. Overall view of sustainable facilities solution.</p> <p>The sustainable enterprise works within the existing enterprise infrastructure.</p> <p></p> <p>Figure 2. Sustainable enterprise works within existing digital infrastructure.</p>"},{"location":"sustainableenterprise/energy/#logical-diagrams","title":"Logical diagrams","text":"<p>Figure 3. The personas and technologies that provide a platform for some of the biggest potential breakthroughs in managing a sustainable enterprise.</p>"},{"location":"sustainableenterprise/energy/#architecture","title":"Architecture","text":"<p>The figures in this section show the interaction of enterprise systems with sustainability enterprise platform systems.</p>"},{"location":"sustainableenterprise/energy/#energy-cost-containment","title":"Energy cost containment","text":"<p>The following diagram shows how systems work together to provide data for decision making in the energy cost containment scenario.</p> <p></p> <p>Figure 4. Schemantic diagram for energy cost containment use case.</p> <p>Energy cost containment workflow steps:</p> <ol> <li>Developer and administrator publish edge computing applications to sensor and edge devices. <li>Collect sensor and energy consumption (energy, refrigerators, HVAC) across the enterprise. Unusual data can be from a piece of equipment that no longer functions, a walk-in refrigerator door being left open, HVAC that is providing temps outside of nominal ranges <li>Sensors report to Intelligent Assets and Facilities Management software that provides alerts on abnormal behavior of data from sensors <li>Alerts are sent to Sustainability Control Tower that takes actions to remediate consumption <li>Business automation provides consistent ways of handling alerts by: <ol> <li>Sending work orders to facility and asset management software <li>Updating inventory management for spoiled goods as needed  <li>Facility management software provides work orders, tracks the completion  <li>Facility manager is updated on the work orders and successful completion of the remediation steps <li>Sustainability Manager reviews KPI, update energy consumption metrics, sets energy consumption goals"},{"location":"sustainableenterprise/energy/#action-guide","title":"Action Guide","text":"<p>From a high-level perspective, the Action Guide represents a future state for organizations considering a comprehensive commitment. The idea is to outline a set steps that can be prioritized to reach that future state by adding new functionality to your existing systems.</p> <ul> <li>Automation</li> <li>Sustainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automation Reduce manual data processing Automate energy data collection between finance and facility management systems Automation Advance the quality of capital, facility and environmental projects Integrate data from multiple systems to get enterprise-wide view to capture and evaluate occupancy to align usage with business requirements and objectives. Automation Optimize real estate portfolios Centralize and integrate critical information at an enterprise level, giving organizations the ability to make the most cost-effective decisions Automation Amp up AI to make workflows smarter Sustainability Create digital twin of your facility. Mirror and monitor building systems and troubleshoot problems before wasting resources on unnecessary or inaccurate repairs. Sustainability Include sustainability data in decision making Integrate sustainability metrics in supply chain, facility management, and data center operations. Sustainability Decrease energy costs Consolidate and measure energy cost reporting and provide systems to manage cost savings Sustainability Increase Green IT in your data center Identify and measure application needs, shut down servers when they are not needed Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point. Modernization Modernize application deployment and operations practices Include DevOps best practices to deploy, monitor, and maintain applications <p>For specific steps on this approach, see The Action Guide details in Own Your Impact: Practical Pathways to Transformational Sustainability survey of 3,000 CEOs worldwide, that reveals sustainability's emergence onto the mainstream corporate agenda.</p>"},{"location":"sustainableenterprise/energy/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p>"},{"location":"sustainableenterprise/energy/#core-systems","title":"Core systems","text":"<p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security.</p> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.</p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"sustainableenterprise/energy/#integration-services","title":"Integration services","text":"<p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</p> <p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing.</p> <p>IBM Data Fabric empowers your teams and works across the ecosystem by connecyting data from disparate data sources in multicloud envrionments. In particular, Watson Knowledge Catalog provides you users with a catalog tool for intelligent, self-service discovery of data, models. Watson Query provides data consumers with a universal query engine that executes distributed and virtualized queries across databases, data warehouses, data lakes, and streaming data without additional manual changes, data movement or replication. </p> <p>IBM Edge Application Manager provides you with edge computing features to help you manage and deploy workloads from a management hub cluster to remote instances of OpenShift Container Platform or other Kubernetes-based clusters.</p>"},{"location":"sustainableenterprise/energy/#sustainable-enterprise-systems","title":"Sustainable enterprise systems","text":"<p>Envizi simplifies the capture, consolidation, management, analysis and reporting of your environmental, social and governance (ESG) data.</p> <p>IBM TRIRIGA harnesses the power of data and AI to infuse sustainability into your real estate and facilities management operations.</p> <p>IBM Maximo Application Suite (MAS) Infuse sustainability into your asset management by harnessing the power of data and AI.</p> <p>IBM Turbonomic monitors resource consumption of applications within the data center. It provides FinOps engineering teams the ability to ensure your applications are performing efficiently, allowing cloud and ITOps teams to cut cloud spend and multiply ROI.</p> <p>Transparent Supply provides supply chain management with a robust traceability solution.</p>"},{"location":"sustainableenterprise/energy/#references","title":"References","text":"<ul> <li>IBM journey to more sustainable facilities: IBM as client zero</li> <li>IBM Institute for Business Value Balancing sustainability and profitability</li> <li>What is sustainability in business?</li> <li>IBM Institute for Business Value Sustainability at a turning point </li> <li>Forbes: 15 Simple Ways For Businesses To Start Saving Energy</li> <li>IBM Energy and climate</li> <li>IBM's 21 goals for environmental sustainability</li> <li>IBM ESG Report</li> </ul>"},{"location":"sustainableenterprise/energy/#contributors","title":"Contributors","text":"<ul> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Mahesh Dodani, Principal Industry Engineer, IBM Technology</li> <li>Thalia Hooker, Senior Principal Specialist Solution Architect, Red Hat</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> <li>Eric Singsaas, Account Technical Lead, IBM Technology</li> <li>Mike Lee, Principal Integration Technical Specialist, IBM</li> <li>Rajeev Shrivastava, Account Technical Lead, IBM</li> <li>Bruce Kyle, Sr Solution Architect, IBM Client Engineering</li> </ul>"},{"location":"sustainableenterprise/greenit/","title":"Green IT","text":"<p>Green computing (also known as green IT or sustainable IT) is the design, manufacture, use and disposal of computers, chips, other technology components and peripherals in a way that limits the harmful impact on the environment, including reducing carbon emissions and the energy consumed by manufacturers, data centers and end-users. Green computing also encompasses choosing sustainably sourced raw materials, reducing electronic waste and promoting sustainability through the use of renewable resources.</p> <p>To maximize the value from cloud, FinOps provides the way for teams to manage their cloud costs, where everyone takes ownership of their cloud usage supported by a central best-practices group. Cross-functional teams in Engineering, Finance, Product, etc work together to enable faster product delivery, while at the same time gaining more financial control and predictability.</p>"},{"location":"sustainableenterprise/greenit/#use-cases","title":"Use cases","text":"<ul> <li>Develop forecasts and cost models for operations in the cloud.</li> <li>Reduce energy consumption and optimizes heating, ventilation and cooling in data center.</li> <li>Provide data to purchasing departments for green computing decision making.</li> <li>Turn off compute that is not in use, including central processing units (CPUs) and peripheral equipment such as printers.</li> <li>Identify and move workloads to serveless compute that can provide efficiency in compute costs.</li> <li>Develop data for FinOps to operate effectively across multiple disciplines.</li> </ul>"},{"location":"sustainableenterprise/greenit/#background","title":"Background","text":"<p>Every aspect of modern information technology \u2014 from the smallest chip to the largest data center \u2014 carries a carbon price tag, and green computing seeks to reduce that carbon price tag. Technology makers play a role in green computing, as do the corporations, organizations, governments and individuals that use technology. </p> <p>\u201cThe energy demands and carbon output of computing and the entire ICT sector must be dramatically moderated if climate change is to be slowed in time to avoid catastrophic environmental damage,\u201d according to a report published by the Association for Computing Machinery. </p>"},{"location":"sustainableenterprise/greenit/#business-problem","title":"Business problem","text":"<p>Digital transformation puts increasing pressure on IT to finally get the performance-cost challenge right.</p> <p>The consequences of not solving the performance versus cost problem are environmental as well as financial.</p> <p>Everyone gets it\u2014the sustainable business must consume cloud and IT resources efficiently.</p>"},{"location":"sustainableenterprise/greenit/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <ul> <li>Competing priorities across the IT organization make alignment between teams difficult</li> <li>Increasing</li> </ul> <p>Business drivers</p> <ul> <li>Cost of compute</li> <li>Develop forecasts and cost models for operations</li> <li>Turn off compute that is not in use</li> <li>Develop data for FinOps to operate effectively across multiple disciplines</li> <li>Manage compute clusters efficiently</li> </ul>"},{"location":"sustainableenterprise/greenit/#responses","title":"Responses","text":"<p>Data centers, server rooms and data storage areas have a significant opportunity to run more efficiently.</p> <p>In such areas, setting up hot and cold aisles is an important step toward greener computing because it reduces energy consumption and optimizes heating, ventilation and cooling. When automated systems designed to control temperature and similar conditions are combined with hot and cold aisles, emissions are further lowered. Cost savings from reducing energy use may eventually be realized, as well.</p> <p>One simple step toward efficiency is to make sure things are turned off. Central processing units (CPUs) and peripheral equipment such as printers should be powered down when not in use. Scheduling blocks of time for specific tasks like printing means peripherals are only in use when they are needed.</p> <p>Purchasing departments have a role to play in green computing, too. Choosing equipment that will last and consumes the least amount of energy necessary for the task to be performed are both ways to reduce the carbon footprint of IT. Notebooks use less energy than laptops, and laptops use less energy than desktop computers, for example.</p> <p>FinOps is an evolving cloud financial management discipline and cultural practice that enables organizations to get maximum business value by helping engineering, finance, technology, and business teams to collaborate on data-driven spending decisions.8 The FinOps Foundation recognized that traditional operations in the cloud are too siloed in their approach to how cloud spend is managed and controlled. As we\u2019ve previously noted, these (over)spending decisions have environmental implications</p>"},{"location":"sustainableenterprise/greenit/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Reduce energy costs in the data center</li> <li>Reduce your carbon footprint by optimizing the data center</li> <li>Optimize cloud costs </li> <li>FinOps aims to build cross-functionally to maximize business value</li> <li>Safely reduce your cloud and data center consumption</li> <li>Provide data to demonstrate business value of using renewable energy, improving heating and cooling, demanding hardware efficiency, efficient cloud resources</li> </ul>"},{"location":"sustainableenterprise/greenit/#solution-overview","title":"Solution overview","text":"<p>The solution shown in Figure 1 uses components that can be grouped into three main categories as shown in the following diagram:</p> <ul> <li>Core application systems. Often customer-provided technologies, such as order management, facilities management. These systems can be stand-alone applications, on premises and cloud services, databases. </li> <li>Foundational infrastructure. The Red Hat/IBM solution is built on Red Hat OpenShift. Data is routed through API management. Events are routed through Business Automation tools such as Business Automation Workshop.</li> <li>Sustainable enterprise systems acts to coordinate facilities management with workplace management backed by sustainability reporting.</li> </ul> <p></p> <p>Figure 1. Overall view of sustainable facilities solution.</p> <p>The sustainable enterprise works within the existing enterprise infrastructure.</p> <p></p> <p>Figure 2. Sustainable enterprise works within existing digital infrastructure.</p>"},{"location":"sustainableenterprise/greenit/#logical-diagrams","title":"Logical diagrams","text":"<p>Figure 3. The personas and technologies that provide a platform for some of the biggest potential breakthroughs in managing a sustainable enterprise.</p>"},{"location":"sustainableenterprise/greenit/#architecture","title":"Architecture","text":"<p>The figures in this section show the interaction of enterprise systems with Green IT use case.</p>"},{"location":"sustainableenterprise/greenit/#green-it_1","title":"Green IT","text":"<p>The following diagram shows how systems work together to provide data for decision making in the Green IT scenario.</p> <p></p> <p>Figure 4. Schemantic diagram for Green IT use case.</p> <p>Green IT workflow steps:</p> <ol> <li>Financial systems provide cost data for cloud and data center resources</li> <li>Application Resource Management system suggests or automates resource allocations based on actual usage and best practices</li> <li>Application Resource Management report application and resource usage to Data Fabric</li> <li>Financial systems report energy and resource billing information to Data Fabric</li> <li>Data Fabric provides a consistent data heirarchy to Intelligent Assets and Facility Management</li> <li>Sensors in data center provide data to event streaming service</li> <li>Data center data is streamed to Sustainability Control Tower</li> <li>Intelligent Facility and Asset Manager updates Sustainability Control Tower in near-real time</li> <li>Sustainability manager, facilities manager, and public can review Green IT data</li> </ol>"},{"location":"sustainableenterprise/greenit/#action-guide","title":"Action Guide","text":"<p>From a high-level perspective, the Action Guide represents a future state for organizations considering a comprehensive commitment. The idea is to outline a set steps that can be prioritized to reach that future state by adding new functionality to your existing systems.</p> <ul> <li>Automation</li> <li>Sustainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automization Automate compute resources <ul><li>Automate dynamic resourcing actions so that applications and the infrastructure they run on continuously manages to SLOs that correlate to business success<li>Operate at the lowest cost possible without having to worry about end-user experiences through automation that dynamically scales and resizes workloads to optimize resource consumption<li>Reduce consumption immediately and continuously by ensuring workloads only consume what they need to perform Automation Amp up AI to make workflows smarter Use AI and compute consumption to identify usage patterns and provide appropriate responses Sustainability Increase Green IT in your data center Identify and measure application needs, shut down servers when they are not needed Sustainability Establish FinOps practice Provide the way for teams to manage their cloud costs, where everyone takes ownership of their cloud usage supported by a central best-practices group Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point. Modernization Modernize application deployment and operations practices Include DevOps best practices to deploy, monitor, and maintain applications <p>For specific steps on this approach, see The Action Guide details in Own Your Transformation survey of 1500 CSCOs across 24 industries.</p>"},{"location":"sustainableenterprise/greenit/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p>"},{"location":"sustainableenterprise/greenit/#core-systems","title":"Core systems","text":"<p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security.</p> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes and update compute inventory, prevent configuration drift and integrated with ITSM.</p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"sustainableenterprise/greenit/#integration-services","title":"Integration services","text":"<p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</p> <p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing.</p> <p>IBM Data Fabric empowers your teams and works across the ecosystem by connecyting data from disparate data sources in multicloud envrionments. In particular, Watson Knowledge Catalog provides you users with a catalog tool for intelligent, self-service discovery of data, models. Watson Query provides data consumers with a universal query engine that executes distributed and virtualized queries across databases, data warehouses, data lakes, and streaming data without additional manual changes, data movement or replication. </p>"},{"location":"sustainableenterprise/greenit/#sustainable-enterprise-systems","title":"Sustainable enterprise systems","text":"<p>Envizi simplifies the capture, consolidation, management, analysis and reporting of your environmental, social and governance (ESG) data.</p> <p>IBM TRIRIGA harnesses the power of data and AI to infuse sustainability into your real estate and facilities management operations.</p> <p>IBM Maximo Application Suite (MAS) Infuse sustainability into your asset management by harnessing the power of data and AI.</p> <p>IBM Turbonomic monitors resource consumption of applications within the data center. It provides FinOps engineering teams the ability to ensure your applications are performing efficiently, allowing cloud and ITOps teams to cut cloud spend and multiply ROI.</p> <p>Transparent Supply provides supply chain management with a robust traceability solution.</p>"},{"location":"sustainableenterprise/greenit/#references","title":"References","text":"<ul> <li>What is Green Computing?</li> <li>State of FinOps 2023</li> <li>IBM journey to more sustainable facilities: IBM as client zero</li> <li>IBM Institute for Business Value Balancing sustainability and profitability</li> <li>What is sustainability in business?</li> <li>IBM Institute for Business Value Sustainability at a turning point </li> <li>Accelerating FinOps &amp; Sustainable IT</li> <li>Forrester: Total Economic Impact of IBM Turbonomic Application Resource Management</li> </ul>"},{"location":"sustainableenterprise/greenit/#contributors","title":"Contributors","text":"<ul> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Mahesh Dodani, Principal Industry Engineer, IBM Technology</li> <li>Thalia Hooker, Senior Principal Specialist Solution Architect, Red Hat</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> <li>Eric Singsaas, Account Technical Lead, IBM Technology</li> <li>Mike Lee, Principal Integration Technical Specialist, IBM</li> <li>Rajeev Shrivastava, Account Technical Lead, IBM</li> <li>Bruce Kyle, Sr Solution Architect, IBM Client Engineering</li> </ul>"},{"location":"sustainableenterprise/sustainableenterprise/","title":"Environmental management system","text":"<p>Companies are using sustainability commitments as a way to affect the bottom line.</p> <p>Organizations have an unprecedented opportunity to capture and analyze their data and develop greater operational resiliency and effectiveness. These data insights give companies the tools needed to</p> <ul> <li>React quickly to change</li> <li>Care for their employees and occupants</li> <li>Make strategic space and occupancy decisions</li> <li>Contain energy costs</li> <li>Improve the efficiency in data centers and cloud</li> </ul> <p>One executive described it this way:</p> <p>Sustainability promotes business, supports the needs of our customers, generates profitability, and contributes to a better world. This is how we see our transformative role. \u2014 Fausto Ribeiro, CEO of Banco do Brasi</p> <p>An enterprise-wide environmental management system sets environmental goals that address the significant environmental aspects and impacts the operation. It drives continual improvement of the organization's environmental performance.</p> <p>Yet, a study of business leaders says:</p> <ul> <li>Executives cite inadequate data (41%) as a top barrier holding back Environmental, Social and Governance (ESG) progress.</li> <li>Only 4 in 10 surveyed consumers feel they have enough data to make environmentally sustainable purchasing (41%) or employment (37%) decisions.</li> </ul>"},{"location":"sustainableenterprise/sustainableenterprise/#use-cases","title":"Use cases","text":"<p>To identify, effectively manage and minimize the potential environmental impact of an organization's operations, companies are adopting environmental management system. It covers hardware product design, manufacturing, data centers, real estate operations, procurement, logistics, asset recovery services, and business services.</p> <p>In this series of articles, we provde architectures and action plans around key use cases:</p> <ul> <li>Sustainable supply chain</li> <li>Smart energy</li> <li>Green IT</li> <li>Sustainable facilities</li> <li>Asset lifecyle</li> </ul>"},{"location":"sustainableenterprise/sustainableenterprise/#background","title":"Background","text":"<p>Most companies are committed to actions that make your operations more sustainable. At this point, some companies have set and published specific targets. How can companies document, manage the steps taken, and report on driving toward the bold sustainability commitments?</p> <p>Businesses are seeking to balance the long-term imperative to protect the planet with the short-term need to preserve the bottom line. </p>"},{"location":"sustainableenterprise/sustainableenterprise/#business-problem","title":"Business problem","text":"<p>The data needed to combine sustainability into an organization are already in existing systems. But those data need to be organized and presented in ways that help you make informed decisions through the lens of sustainability. And once decisions are made, the steps to be taken can be automated to provide consistant, measurable actions.</p> <p>In the end, your organization can demonstrate its commitments to sustainability, while improving the bottom line.</p> <p>A global IBM Institute for Business Value (IBV) study, The ESG ultimatum: Profit or perish, of executives and consumers reveals that while an increased focus on environmental sustainability remains a top priority for consumers and business executives, inadequate data is a key challenge for both groups when it comes to achieving personal and corporate ESG goals. According to the study, companies are investing in ESG and see it as good for business:</p> <ul> <li>76% of surveyed executives say ESG is central to their business strategy.</li> <li>Almost 3 in 4 surveyed executives (72%) view ESG as a revenue enabler rather than cost center, suggesting that contrary to popular opinion, ESG and profitability are not at odds.</li> <li>76% of executive respondents agree or strongly agree that their organization focuses on achieving ESG outcomes, not just reporting requirements.</li> </ul>"},{"location":"sustainableenterprise/sustainableenterprise/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <p>There are several challenges to overcome in the pursuit of becoming a truly sustainable business:</p> <ul> <li>Customer readiness. While the mindset around sustainability is shifting, no business can afford to be left behind, and few can financially afford to be too far ahead of the appetite for sustainable offerings. Co-creating a sustainable future requires a deep understanding of your customers and having partners with the right relationships and ecosystems to bring them along on the journey.</li> <li>Cost. Implementing sustainable business practices typically requires higher upfront investments. In the short term, it will often be cheaper to stick with the status quo. Some organizations will need help building an investment case to show how immediate investment will result in more durable profitability over the long run.</li> <li>Systemic inertia. While sustainability is an important goal, it often isn\u2019t seen as more important than other key priorities that may provide benefits sooner. Many businesses plan in ten-year increments, so while a 2050 commitment is good, it often isn\u2019t enough to drive sufficient action in this decade, from a planning standpoint. It comes back to reframing risks as opportunities and building the case that acting on sustainability now is necessary to achieving future sustainability in business.</li> <li>Lack of tools, insights and expertise. Being unprepared to develop a corporate sustainability vision, strategy and framework is a monumental risk. Companies may lack the ability to implement sustainable solutions or even know where to start. Sustainability in business is evolving and so are the answers. Every business needs an ecosystem of innovation partners to help them reinvent the world and create a sustainable future.</li> </ul> <p>Drivers</p> <ul> <li>Sustainability is good for business.</li> <li>Reduce energy costs.</li> <li>Improve cost effectiveness of data centers.</li> <li>Include ESG criteria in supply chain decision making.</li> <li>Prioritize environmental improvement projects.</li> </ul>"},{"location":"sustainableenterprise/sustainableenterprise/#responses","title":"Responses","text":"Business Problem Solution Automating ESG Reporting Capture and manage over quantitative and qualitive data types to support your expanding sustainability reporting requirements to frameworks and reporting schemes Manage competing needs with constrained resources <ul><li>Predict when important assets and building systems will require maintenance, refurbishment or replacement. <li>Document the impact on productivity and building value when systems fail. Incorporate sustainability options into customer decison making Connect your strategy with day-to-day operations to embed sustainability into your business transformation Create a lower-emissions business Build intelligent asset management into operations. Document and improve business bottom line Identify, measure, and respond with day-to-day decisions supporting sustainability outcome"},{"location":"sustainableenterprise/sustainableenterprise/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Reduced energy costs</li> <li>Improved utilization of office and warehouse space</li> <li>Improved energy efficiency in your data centers</li> <li>Measurable improvement in sustainability in your supply chain</li> <li>Sustainable buildings helps create a greener environment</li> </ul>"},{"location":"sustainableenterprise/sustainableenterprise/#solution-overview","title":"Solution overview","text":"<p>The solution shown in Figure 1 uses components that can be grouped into three main categories as shown in the following diagram:</p> <ul> <li>Core application systems. Often customer-provided technologies, such as order management, facilities management. These systems can be stand-alone applications, on premises and cloud services, databases. </li> <li>Foundational infrastructure. The Red Hat/IBM solution is built on Red Hat OpenShift. Data is routed through API management. Events are routed through Business Automation tools such as Business Automation Workshop.</li> <li>Sustainable enterprise systems acts to coordinate facilities management with workplace management backed by sustainability reporting.</li> </ul> <p></p> <p>Figure 1. Overall view of sustainable facilities solution.</p> <p>The sustainable enterprise works within the existing enterprise infrastructure.</p> <p></p> <p>Figure 2. Sustainable enterprise works within existing digital infrastructure.</p>"},{"location":"sustainableenterprise/sustainableenterprise/#logical-diagrams","title":"Logical diagrams","text":"<p>Figure 3. The personas and technologies that provide a platform for some of the biggest potential breakthroughs in the supply chain.</p>"},{"location":"sustainableenterprise/sustainableenterprise/#key-use-cases","title":"Key use cases","text":"<p>See:</p> <ul> <li>Sustainable supply. Enable intelligent, end-to-end supply chain visibility and transparency.</li> <li>Smart energy. Reduce the energy costs in your enterprise.</li> <li>Green IT. Manage cloud costs, reduce energy consumption in your data centers.</li> <li>Sustainable facilities. Improve your organization's capital project management capabilities.</li> <li>Asset lifecycle management. Track assets and associate material components, track the handling of hazardous waste, and monitor and forecast against zero waste targets.</li> </ul> <p>For a supply chain overview, see Supply chain optimization.</p>"},{"location":"sustainableenterprise/sustainableenterprise/#action-guide","title":"Action Guide","text":"<p>Rather than pigeonholing ESG into the realm of compliance and reporting, leaders that elevate its role can drive engagement, inspire innovation, improve operations\u2014and unify ecosystem partners around shared strategic goals.</p> <p>From a high-level perspective, the Action Guide represents a future state for organizations considering a comprehensive commitment. The idea is to outline a set steps that can be prioritized to reach that future state by adding new functionality to your existing systems.</p> <ul> <li>Automation</li> <li>Sustainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automation Advance the quality of capital, facility and environmental projects Integrate data from multiple systems to get enterprise-wide view to capture and evaluate occupancy to align usage with business requirements and objectives Automation Optimize real estate portfolios Centralize and integrate critical information at an enterprise level, giving organizations the ability to make the most cost-effective decisions Automation Amp up AI to make workflows smarter Automation Automate the collection of sustainability data Reduce manual processing of data Sustainability Create digital twin of your facility Mirror and monitor building systems and troubleshoot problems before wasting resources on unnecessary or inaccurate repairs Sustainability Include sustainability data in decision making Integrate sustainability metrics in supply chain, facility management, and data center operations. Sustainability Decrease energy costs Consolidate and measure energy cost reporting and provide systems to manage cost savings Sustainability Increase Green IT in your data center Identify and measure application needs, shut down servers when they are not needed Sustainability Establish FinOps practice Provide the way for teams to manage their cloud costs, where everyone takes ownership of their cloud usage supported by a central best-practices group Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point Modernization Modernize application deployment and operations practices Include DevOps best practices to deploy, monitor, and maintain applications <p>For specific steps on this approach, see The Action Guide details in Own Your Impact: Practical Pathways to Transformational Sustainability survey of 3,000 CEOs worldwide, that reveals sustainability's emergence onto the mainstream corporate agenda.</p>"},{"location":"sustainableenterprise/sustainableenterprise/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p>"},{"location":"sustainableenterprise/sustainableenterprise/#core-systems","title":"Core systems","text":"<p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security.</p> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.</p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"sustainableenterprise/sustainableenterprise/#integration-services","title":"Integration services","text":"<p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</p> <p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing.</p> <p>IBM Data Fabric empowers your teams and works across the ecosystem by connecyting data from disparate data sources in multicloud envrionments. In particular, Watson Knowledge Catalog provides you users with a catalog tool for intelligent, self-service discovery of data, models. Watson Query provides data consumers with a universal query engine that executes distributed and virtualized queries across databases, data warehouses, data lakes, and streaming data without additional manual changes, data movement or replication. </p> <p>IBM Edge Application Manager provides you with edge computing features to help you manage and deploy workloads from a management hub cluster to remote instances of OpenShift Container Platform or other Kubernetes-based clusters.</p>"},{"location":"sustainableenterprise/sustainableenterprise/#sustainable-enterprise-systems","title":"Sustainable enterprise systems","text":"<p>Envizi simplifies the capture, consolidation, management, analysis and reporting of your environmental, social and governance (ESG) data.</p> <p>IBM TRIRIGA harnesses the power of data and AI to infuse sustainability into your real estate and facilities management operations.</p> <p>IBM Maximo Application Suite (MAS) Infuse sustainability into your asset management by harnessing the power of data and AI.</p> <p>IBM Turbonomic monitors resource consumption of applications within the data center. It provides FinOps engineering teams the ability to ensure your applications are performing efficiently, allowing cloud and ITOps teams to cut cloud spend and multiply ROI.</p> <p>Transparent Supply provides supply chain management with a robust traceability solution.</p>"},{"location":"sustainableenterprise/sustainableenterprise/#references","title":"References","text":"<ul> <li>IBM journey to more sustainable facilities: IBM as client zero</li> <li>IBM Institute for Business Value Balancing sustainability and profitability</li> <li>What is sustainability in business?</li> <li>IBM Institute for Business Value Sustainability at a turning point </li> <li>IBM Institute for Business Value Own Your Impact: Practical Pathways to Transformational Sustainability</li> <li>Research Insights Meet the 2020 consumers driving change </li> <li>Bloomberg Intelligence ESG assets may hit $53 trillion by 2025</li> <li>2022 Last Mile Logistics Trends: Sustainable Sustainability</li> <li>Sustainability in Facilities Management</li> <li>IDC MarketScape: Worldwide Integrated Workplace Management System 2020\u20132021 Vendor Assessment</li> <li>Unlock opportunities with an integrated workplace management system</li> <li>Sustainable asset management with IBM Maximo Application Suite</li> <li>World Economic Forum Why buildings are the foundation of an energy-efficient future</li> <li>IBM's Global environmental management system</li> <li>IBM Institute for Business Value The ESG ultimatum: Profit or perish</li> </ul>"},{"location":"sustainableenterprise/sustainableenterprise/#next-steps","title":"Next steps","text":"<p>See:</p> <ul> <li>Sustainable supply</li> <li>Smart energy</li> <li>Green IT</li> <li>Sustainable facilities</li> <li>Asset lifecycle management</li> </ul> <p>For a comprehensive supply chain overview, see Supply Chain Optimization.</p>"},{"location":"sustainableenterprise/sustainableenterprise/#contributors","title":"Contributors","text":"<ul> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Mahesh Dodani, Principal Industry Engineer, IBM Technology</li> <li>Thalia Hooker, Senior Principal Specialist Solution Architect, Red Hat</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> <li>Eric Singsaas, Account Technical Lead, IBM Technology</li> <li>Mike Lee, Principal Integration Technical Specialist, IBM</li> <li>Rajeev Shrivastava, Account Technical Lead, IBM</li> <li>Bruce Kyle, Sr Solution Architect, IBM Client Engineering</li> </ul>"},{"location":"sustainableenterprise/sustainablefacility/","title":"Sustainable facility","text":"<p>Companies are setting environmental commitments for facilities, such as:</p> <ul> <li>Achieving net zero green house gas (GHG) emissions by 2030 </li> <li>Diverting 90% of nonhazardous waste (by weight) from landfill and incineration by 2025</li> </ul> <p>Real estate plays a critical role in meeting these targets. Buildings account for 40% of the annual carbon dioxide emissions globally.</p> <p>More than 80% of CEOs expect sustainability investments to produce improved business results in the next five years.</p> <p>The key to a successful sustainability strategy is balancing the environmental drivers with market demands.</p> <p>Real estate is typically one of the largest expenses for an organization. Now an organization can make the most out of every square inch and meter of space.</p> <p>The World Economic Forum explains, energy consumption in the built environment accounted for 40% of global energy use and 33% of global energy-related CO2 emissions in 2019. In Europe alone, more than 220 million existing buildings \u2013 or 75% of the building stock \u2013 are energy-inefficient, with many relying on fossil fuels for heating and cooling. </p> <p>They continue:</p> <p>The more innovative, more efficient and cheaper way is to equip buildings with the digital tools that allow them to automatically adjust heating, lighting and other systems to the number of people present at any given time, using real-time data analysis.</p> <p>Executing on this mission involves a staggering number of decisions, ranging from room configurations and leasing terms, to capital project planning and facilities maintenance.</p>"},{"location":"sustainableenterprise/sustainablefacility/#use-cases","title":"Use cases","text":"<p>Manage competing needs with constrained resources. </p> <p>Improve an organization\u2019s capital project management capabilities.</p> <p>Manage buildings to encourage sustainability through the real estate lifecycle. Specifically:</p> <ul> <li>Include sustainability criteria in space acquisition and disposals (ISO 14000 as an example)</li> <li>Prioritize sustainability in construction and renovation projects</li> <li>Enable energy efficiency with green leases</li> <li>Reduce waste by integrating portfolio data</li> <li>Reach carbon footprint goals with space optimization</li> <li>Extend asset life through improved maintenance and assessment</li> <li>Maintain compliance to regulations and manage remediation with integrated Environmental, Health, and Safety (EHS/HSE) processes.</li> </ul> <p>Minimize the environmental impact of your operations.</p> <ul> <li>Cut energy use.</li> <li>Minimize/combine facility management work orders.</li> <li>Use data from sensors, such as HVAC, refrigerator and freezer temperatures.</li> <li>Identify and manage energy usage.</li> <li>Green IT, reducing environmental impact of your data center.</li> </ul>"},{"location":"sustainableenterprise/sustainablefacility/#background","title":"Background","text":"<p>Roughly 40% of the global share of annual carbon dioxide emissions comes from buildings. The UN Environment Program states that if nothing is done, greenhouse gas (GHG) emissions from buildings will more than double in the next 20 years.</p> <p>To meet targets for green house gas (GHG) emissions, buildings and cities must shift from being part of the problem to being part of the solution. </p>"},{"location":"sustainableenterprise/sustainablefacility/#business-problem","title":"Business problem","text":"<p>There is a constant tension between using every square inch of space, reducing real estate operating costs, and creating outstanding occupant experiences.</p> <p>IDC in its Worldwide Integrated Workplace Management System 2020\u20132021 Vendor Assessment reports:</p> <p>Now organizations must prioritize creating safe and productive environments for their employees, students, and customers. In the short term, organizations had to take measures such as allowing a dramatic shift to working remotely where possible, modifying floor plans for social distancing, reducing building capacity, implementing new schedules, and deploying novel technology. Real estate and facility professionals have turned to a variety of \"return-to-work solutions\" for space reservations, health attestation, contact tracing, proximity monitoring, wayfinding, cleaning, communications, and so on.</p> <p>Organizations have an unprecedented opportunity to capture and analyze their data and develop greater operational resiliency and effectiveness. These data insights give companies the tools needed to react quickly to change, care for their employees and occupants, and make strategic space and occupancy decisions.</p> <p>Sustainability promotes business, supports the needs of our customers, generates profitability, and  contributes to a better world. This is how we see our transformative role. \u2014 Fausto Ribeiro, CEO of Banco do Brasi</p>"},{"location":"sustainableenterprise/sustainablefacility/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <p>There are several challenges to overcome in the pursuit of becoming a truly sustainable business:</p> <ul> <li>Customer readiness. While the mindset around sustainability is shifting, no business can afford to be left behind, and few can financially afford to be too far ahead of the appetite for sustainable offerings. Co-creating a sustainable future requires a deep understanding of your customers and having partners with the right relationships and ecosystems to bring them along on the journey.</li> <li>Cost. Implementing sustainable business practices typically requires higher upfront investments. In the short term, it will often be cheaper to stick with the status quo. Some organizations will need help building an investment case to show how immediate investment will result in more durable profitability over the long run.</li> <li>Systemic inertia. While sustainability is an important goal, it often isn\u2019t seen as more important than other key priorities that may provide benefits sooner. Many businesses plan in ten-year increments, so while a 2050 commitment is good, it often isn\u2019t enough to drive sufficient action in this decade, from a planning standpoint. It comes back to reframing risks as opportunities and building the case that acting on sustainability now is necessary to achieving future sustainability in business.</li> <li>Lack of tools, insights and expertise. Being unprepared to develop a corporate sustainability vision, strategy and framework is a monumental risk. Companies may lack the ability to implement sustainable solutions or even know where to start. Sustainability in business is evolving and so are the answers. Every business needs an ecosystem of innovation partners to help them reinvent the world and create a sustainable future.</li> </ul> <p>Drivers</p> <ul> <li>Capture and calculate carbon emissions data, report it according to regulations and set targets to drive action.</li> <li>Reduce energy costs.</li> <li>Include ESG criteria in decision making.</li> <li>Prioritize environmental improvement projects.</li> <li>Improve an capital project management capabilities.</li> <li>Use AI models to \u201csqueeze\u201d more insights from the existing sensors.</li> </ul>"},{"location":"sustainableenterprise/sustainablefacility/#responses","title":"Responses","text":"Business Problem Solution Automating ESG Reporting Capture and manage over quantitative and qualitive data types to support your expanding sustainability reporting requirements to frameworks and reporting schemes Manage competing needs with constrained resources <ul><li>Predict when important assets and building systems will require maintenance, refurbishment or replacement. <li>Document the impact on productivity and building value when systems fail. Incorporate sustainability options into customer decison making Connect your strategy with day-to-day operations to embed sustainability into your business transformation Create a lower-emissions business Build intelligent asset management into operations. Data exists, but may be in silos Standardize, integrate, and centralize data. Availability and transparency of data to bring them to top-of-mind in decision making"},{"location":"sustainableenterprise/sustainablefacility/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Improved utilization of office and warehouse space</li> <li>Improved energy efficiency</li> <li>Increased asset life</li> <li>Improved maintenance </li> <li>Sustainable buildings helps create a greener environment</li> </ul>"},{"location":"sustainableenterprise/sustainablefacility/#solution-overview","title":"Solution overview","text":"<p>The solution shown in Figure 1 uses components that can be grouped into three main categories as shown in the following diagram:</p> <ul> <li>Core application systems. Often customer-provided technologies, such as order management, facilities management. These systems can be stand-alone applications, on premises and cloud services, databases. </li> <li>Foundational infrastructure. The Red Hat/IBM solution is built on Red Hat OpenShift. Data is routed through API management. Events are routed through Business Automation tools such as Business Automation Workshop.</li> <li>Sustainable enterprise systems acts to coordinate facilities management with workplace management backed by sustainability reporting.</li> </ul> <p></p> <p>Figure 1. Overall view of sustainable facilities solution.</p> <p>The sustainable enterprise works within the existing enterprise infrastructure.</p> <p></p> <p>Figure 2. Sustainable enterprise works within existing digital infrastructure.</p>"},{"location":"sustainableenterprise/sustainablefacility/#logical-diagrams","title":"Logical diagrams","text":"<p>Figure 3. The personas and technologies that provide a platform for some of the biggest potential breakthroughs in managing a sustainable enterprise.</p>"},{"location":"sustainableenterprise/sustainablefacility/#architecture","title":"Architecture","text":"<p>The figures in this section show the interaction of customer systems with sustainability facilities systems.</p>"},{"location":"sustainableenterprise/sustainablefacility/#sustainable-facilities","title":"Sustainable facilities","text":"<p>The following diagram shows how data is provided for insights and decision making along with how those data can be also used in compliance and reporting.</p> <p></p> <p>Figure 4. Schematic diagram of the sustainable facilities use case.</p> <p>Sustainable facilties workflow steps:</p> <ol> <li>Collect sensor and energy consumption (energy, refrigerators, HVAC) across the enterprise. Unusual data can be from a piece of equipment that no longer functions, a walk-in refrigerator door being left open, HVAC that is providing temps outside of nominal ranges <li>Sensors report to Intelligent Assets and Facilities Management software that provides alerts. AI is used to determine abnormal behavior of data from sensors. <li>Data is sent in real time to Intelligent Assets and Facilities system to take remediation actions, provide information for space planning and utilization <li>Data is sent in real time to the Sustainability Control Tower, an enterprise-wide repository. <li>A Data Fabric provides consistent ways of pulling data from existing data, maps data heirarchies between systems, removing inconsistencies in data. <ol> <li>Intelligence Assets and Facilities software collects data. (It also provides work orders, proactive maintence, project planning.)   <li>Data is collected in Sustainability Control Tower and used for decision making across the enterprise, across multiple scenarios. <li>Sustainability Manager, Facility Managers, company execs reviews KPI, update energy consumption metrics, sets energy consumption goals. Uses the insights it generates to help informed decision-making that makes both business and environmental sense <li>Sustainability Control Tower provides reporting for compliance and to demonstrate the impacts of decisions on sustainability and the environment."},{"location":"sustainableenterprise/sustainablefacility/#action-guide","title":"Action Guide","text":"<p>From a high-level perspective, the Action Guide represents a future state for organizations considering a comprehensive commitment. The idea is to outline a set steps that can be prioritized to reach that future state by adding new functionality to your existing systems.</p> <ul> <li>Automation</li> <li>Sustainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automation Advance the quality of capital, facility and environmental projects Integrate data from multiple systems to get enterprise-wide view to capture and evaluate occupancy to align usage with business requirements and objectives. Automation Optimize real estate portfolios Centralize and integrate critical information at an enterprise level, giving organizations the ability to make the most cost-effective decisions. Automation Amp up AI to make workflows smarter <ul><li>Using self-service, automated and mobile processes for move management and reservation and hoteling workspaces.<li>Manage and monitor the assets within each structure \u2013 like HVAC systems, elevators and even the exit signs \u2013 and use AI-driven insights to perform predictive instead of reactive maintenance&lt;./ul&gt; Automation Automate ESG data collection Integrate technologies to help address the complexities of ESG data volume and types and your Hybrid IT preference. Sustainability Build a data foundation. <ol><li>Monitor and measure your operations<li>Manage assets, infrastructure and resources<li>Improve product and service quality<li>Enable sustainable operations Sustainability Streamline reporting and disclosures Use powerful emissions calculation engine and flexible reporting tools to meet strict internal and external requirements. Sustainability Accelerate decarbonization Identify energy and emissions savings opportunities and tracking progress at every stage of your journey. Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point. <p>For specific steps on this approach, see The Action Guide details in Own Your Impact: Practical Pathways to Transformational Sustainability survey of 3,000 CEOs worldwide, that reveals sustainability's emergence onto the mainstream corporate agenda.</p>"},{"location":"sustainableenterprise/sustainablefacility/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p>"},{"location":"sustainableenterprise/sustainablefacility/#core-systems","title":"Core systems","text":"<p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security.</p> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.</p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"sustainableenterprise/sustainablefacility/#integration-services","title":"Integration services","text":"<p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</p> <p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing.</p> <p>IBM Data Fabric empowers your teams and works across the ecosystem by connecyting data from disparate data sources in multicloud envrionments. In particular, Watson Knowledge Catalog provides you users with a catalog tool for intelligent, self-service discovery of data, models. Watson Query provides data consumers with a universal query engine that executes distributed and virtualized queries across databases, data warehouses, data lakes, and streaming data without additional manual changes, data movement or replication. </p> <p>IBM Edge Application Manager provides you with edge computing features to help you manage and deploy workloads from a management hub cluster to remote instances of OpenShift Container Platform or other Kubernetes-based clusters.</p>"},{"location":"sustainableenterprise/sustainablefacility/#sustainable-enterprise-systems","title":"Sustainable enterprise systems","text":"<p>Envizi simplifies the capture, consolidation, management, analysis and reporting of your environmental, social and governance (ESG) data.</p> <p>IBM TRIRIGA harnesses the power of data and AI to infuse sustainability into your real estate and facilities management operations.</p> <p>IBM Maximo Application Suite (MAS) Infuse sustainability into your asset management by harnessing the power of data and AI.</p> <p>IBM Turbonomic monitors resource consumption of applications within the data center. It provides FinOps engineering teams the ability to ensure your applications are performing efficiently, allowing cloud and ITOps teams to cut cloud spend and multiply ROI.</p> <p>Transparent Supply provides supply chain management with a robust traceability solution.</p>"},{"location":"sustainableenterprise/sustainablefacility/#references","title":"References","text":"<ul> <li>IBM journey to more sustainable facilities: IBM as client zero</li> <li>IBM Institute for Business Value Balancing sustainability and profitability</li> <li>How sustainability technologies help make facilities work better</li> <li>What is sustainability in business?</li> <li>IBM Institute for Business Value Sustainability at a turning point </li> <li>IBM Institute for Business Value Own Your Impact: Practical Pathways to Transformational Sustainability</li> <li>Unlock opportunities with an integrated workplace management system</li> <li>Sustainable asset management with IBM Maximo Application Suite</li> <li>World Economic Forum Why buildings are the foundation of an energy-efficient future</li> <li>IBM Build tomorrow\u2019s predictive enterprise today</li> <li>EY and IBM Collaborate to Address Complex ESG Challenges and Drive Value-Led Sustainability</li> </ul>"},{"location":"sustainableenterprise/sustainablefacility/#next-steps","title":"Next steps","text":"<p>See:</p> <ul> <li>Loss and waste management (coming soon)</li> <li>Product timeliness (coming soon)</li> <li>Perfect order (coming soon)</li> <li>Intelligent order (coming soon)</li> <li>Sustainable supply (coming soon)</li> </ul> <p>For a comprehensive supply chain overview, see Supply Chain Optimization.</p>"},{"location":"sustainableenterprise/sustainablefacility/#contributors","title":"Contributors","text":"<ul> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Mahesh Dodani, Principal Industry Engineer, IBM Technology</li> <li>Thalia Hooker, Senior Principal Specialist Solution Architect, Red Hat</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> <li>Eric Singsaas, Account Technical Lead, IBM Technology</li> <li>Mike Lee, Principal Integration Technical Specialist, IBM</li> <li>Rajeev Shrivastava, Account Technical Lead, IBM</li> <li>Bruce Kyle, Sr Solution Architect, IBM Client Engineering</li> </ul>"},{"location":"sustainableenterprise/transparentsupply/","title":"Transparent supply","text":"<p>A sustainable supply platform enables goods-specific transparency across multiple supply chain partners, enabling analytics by company, location, product movement and condition.</p>"},{"location":"sustainableenterprise/transparentsupply/#use-cases","title":"Use cases","text":"<ul> <li>Track the physical flow of goods across companies for new speed to insight</li> <li>Combine data that is typically siloed to leverage new business drivers</li> <li>Automate transparency across companies to reach next-generation supply chain efficiency</li> <li>Track the legacy of products, such as pharmacy and perishable items</li> </ul>"},{"location":"sustainableenterprise/transparentsupply/#business-problem","title":"Business problem","text":"<p>You can unleash the full power of data and end-to-end visibility to empower all supply chain participants and create new business value. </p> <ul> <li>Drive buyer engagement to grow your market. Engage and empower buyers in new ways and build trust in your brand by showing how a product was made and demonstrating support for ethical and sustainable production.</li> <li>Transform your business model to optimize revenue. Leverage supply chain visibility to improve predictions and forecasting with real-time data and optimize inventory dynamically. Use smart contracts to automate responsiveness up and down the chain.</li> <li>Reduce supply chain complexity, errors and cost. Combine blockchain, IoT and AI to automate digitization of supply chain data, handle dispute resolution proactively and share digital documents across companies.</li> </ul>"},{"location":"sustainableenterprise/transparentsupply/#challenges-business-drivers","title":"Challenges / Business Drivers","text":"<p>Challenges</p> <ul> <li>Companies may be wary of sharing competitive data are more willing to participate on the platform.</li> <li>Visibility into your supply chain is limited as goods move from the supplier\u2019s supplier to the customer\u2019s customer.</li> <li>Transactions are often still paper based.</li> </ul> <p>Drivers</p> <ul> <li>Enhance traceability. If a company discovers a faulty product, the blockchain enables the firm and its supply chain partners to trace the product, identify all suppliers involved with it, identify production and shipment batches associated with it, and efficiently recall it.</li> <li>Increase efficiency and speed and reducing disruptions.</li> <li>Improve financing, contracting, and international transactions.</li> <li>Reduce loss, increasing efficiency regarding waste management.</li> </ul>"},{"location":"sustainableenterprise/transparentsupply/#business-outcomes","title":"Business outcomes","text":"<ul> <li>Quality assurance. Authenticate product origins and validate their provenance to demonstrate brand differentiators to consumers.</li> <li>Improved forecasting. Improve product traceability throughout the supply chain in near real-time. Optimize with automatic replenishment.</li> <li>Reduced friction. The use of blockchain reduces the costs of dispute resolutions, product recalls, compliance and documentation sharing.</li> <li>Extensive automation. Shorten the timeframe to achieve automation through embedded intelligence and smart workflows.</li> <li>Reducing loss, increasing efficiency regarding waste management.</li> </ul>"},{"location":"sustainableenterprise/transparentsupply/#solution-overview","title":"Solution overview","text":"<p>The solution shown in Figure 1 uses components that can be grouped into three main categories as shown in the following diagram:</p> <ul> <li>Core application systems. Often customer-provided technologies, such as order management, facilities management. These systems can be stand-alone applications, on premises and cloud services, databases. </li> <li>Foundational infrastructure. The Red Hat/IBM solution is built on Red Hat OpenShift. Data is routed through API management. Events are routed through Business Automation tools such as Business Automation Workshop.</li> <li>Sustainable enterprise systems acts to coordinate facilities management with workplace management backed by sustainability reporting.</li> </ul> <p></p> <p>Figure 1. Overall view of sustainable facilities solution.</p> <p>The sustainable enterprise works within the existing enterprise infrastructure.</p> <p></p> <p>Figure 2. Sustainable enterprise works within existing digital infrastructure.</p>"},{"location":"sustainableenterprise/transparentsupply/#logical-diagrams","title":"Logical diagrams","text":"<p>Figure 3. The personas and technologies that provide a platform for some of the biggest potential breakthroughs in the supply chain.</p>"},{"location":"sustainableenterprise/transparentsupply/#architecture","title":"Architecture","text":"<p>The figures in this section show the interaction of suppliers' data to your customer systems.</p>"},{"location":"sustainableenterprise/transparentsupply/#sustainable-supply","title":"Sustainable supply","text":"<p>In this figure, suppliers' provides data for decision making to make your supply chain more sustainable</p> <p></p> <p>Figure 4. Schematic diagram of the sustainable supply use case.</p> <p>Sustainable supply workflow steps:</p> <ol> <li>Suppliers' software provides product sourcing updates through API <li>API Management provides and monitors the input of the 3rd partly logistics information <li>Your organization receives the suppliers' information and incorporates it into Sustainable Supply <li>The Supply Assurance Control Tower surfaces the end-to-end logistics supply chain to users, such as the inventory controller and logistics <ol> <li>Sourcing data is routed to Sustainability Control Tower for reporting <li>Sourcing data is routed to Supply Chain Control Tower for visitibility to Inventory Controller and others and creates work queues as needed  <li>Based on the visibility into the supply chain, the inventory controller and logistics officer can take actions to replenish supplies or to act to reduce loss <li>The Business Automation provides a consistent way for multiple systems to respond, such as <li>Update: <ol> <li>Inventory management system counts of current inventory, store inventory, and future inventory <li>Place orders in Order Management System"},{"location":"sustainableenterprise/transparentsupply/#action-guide","title":"Action Guide","text":"<p>From a high-level perspective, the Action Guide represents a future state for organizations considering a comprehensive commitment. The idea is to outline a set steps that can be prioritized to reach that future state by adding new functionality to your existing systems.</p> <ul> <li>Automation</li> <li>Sustainability</li> <li>Modernization</li> </ul> Actionable Step Implementation details Automation Automate the collection of sustainability data Reduce manual processing of data Automation Amp up AI to make workflows smarter Participants add their data and supporting documents like certifications to the ledger and control who is allowed to see what. Once added to the ledger, data cannot be manipulated, changed or deleted. Participants can track materials and products from source to end customer and, ultimately, the consumer. Sustainability Include sustainability data in decision making Integrate sustainability metrics in supply chain, facility management, and data center operations Sustainability Track sustainability data within your supply chain Engage vendors and partners to provide sustainability data as part of your purchasing requirements Modernization Modernization for modern infrastructures, scale hybrid cloud platforms The decision for a future, Kubernetes-based enterprise platform is defining the standards for development, deployment and operations tools and processes for years to come and thus represents a foundational decision point. Modernization Modernize application deployment and operations practices Include DevOps best practices to deploy, monitor, and maintain applications <p>For specific steps on this approach, see The Action Guide details in Own Your Transformation survey of 1500 CSCOs across 24 industries.</p>"},{"location":"sustainableenterprise/transparentsupply/#technology","title":"Technology","text":"<p>The following technologies offered by Red Hat and IBM can augment the solutions already in place in your organization.</p>"},{"location":"sustainableenterprise/transparentsupply/#core-systems","title":"Core systems","text":"<p>Red Hat OpenShift Kubernetes offering, the hybrid platform offering allow deployment across data centers, private and public clouds offering choices and flexible for hosting system and services. You can manage clusters and applications from a single console, with built-in security policies with Red Hat Advanced Cluster Management and Red Hat Advanced Cluster Security.</p> <p>Red Hat Ansible Automation Platform operate, scale and delegate automate IT services, track changes an update inventory, prevent configuration drift and  integrated with ITSM.</p> <p>Red Hat OpenShift DevOps represents an approach to culture, automation and platform design intended to deliver increased business value and responsiveness through rapid, high-quality service delivery. DevOps means linking legacy apps with newer cloud-native apps and infrastructure. A DevOps developer can link legacy apps with newer cloud-native apps and infrastructure.</p>"},{"location":"sustainableenterprise/transparentsupply/#integration-services","title":"Integration services","text":"<p>Red Hat OpenShift API Management is a managed API traffic control and program management service to secure, manage, and monitor APIs at every stage of the development lifecycle.</p> <p>Red Hat Intgration is a comprehensive set of integration and messaging technologies to connect applications and data across hybrid infrastructures. It is an agile, distributed, containerized, and API-centric solution. It provides service composition and orchestration, application connectivity and data transformation, real-time message streaming, change data capture, and API management.</p> <p>IBM Business Automation delivers intelligent automations quickly with low-code tooling, such as business processes automation, decisioning software, robotic process automation, process mining, workflow automation, business process mapping, Watson Orchestrate, content services, and document processing. </p> <p>IBM Data Fabric empowers your teams and works across the ecosystem by connecyting data from disparate data sources in multicloud envrionments. In particular, Watson Knowledge Catalog provides you users with a catalog tool for intelligent, self-service discovery of data, models. Watson Query provides data consumers with a universal query engine that executes distributed and virtualized queries across databases, data warehouses, data lakes, and streaming data without additional manual changes, data movement or replication. </p>"},{"location":"sustainableenterprise/transparentsupply/#sustainable-enterprise-systems","title":"Sustainable enterprise systems","text":"<p>Envizi simplifies the capture, consolidation, management, analysis and reporting of your environmental, social and governance (ESG) data.</p> <p>IBM TRIRIGA harnesses the power of data and AI to infuse sustainability into your real estate and facilities management operations.</p> <p>IBM Maximo Application Suite (MAS) Infuse sustainability into your asset management by harnessing the power of data and AI.</p> <p>IBM Turbonomic monitors resource consumption of applications within the data center. It provides FinOps engineering teams the ability to ensure your applications are performing efficiently, allowing cloud and ITOps teams to cut cloud spend and multiply ROI.</p> <p>Transparent Supply provides supply chain management with a robust traceability solution.</p>"},{"location":"sustainableenterprise/transparentsupply/#references","title":"References","text":"<ul> <li>Harvard Business Review Building a Transparent Supply Chain</li> </ul>"},{"location":"sustainableenterprise/transparentsupply/#contributors","title":"Contributors","text":"<ul> <li>Iain Boyle, Chief Architect, Red Hat</li> <li>Mahesh Dodani, Principal Industry Engineer, IBM Technology</li> <li>Thalia Hooker, Senior Principal Specialist Solution Architect, Red Hat</li> <li>Lee Carbonell, Senior Solution Architect &amp; Master Inventor, IBM</li> <li>Eric Singsaas, Account Technical Lead, IBM Technology</li> <li>Mike Lee, Principal Integration Technical Specialist, IBM</li> <li>Rajeev Shrivastava, Account Technical Lead, IBM Technology</li> <li>Bruce Kyle, Sr Solution Architect, IBM Client Engineering</li> <li>Jeric Saez, Senior Solution Architect, IBM Client Engineering</li> </ul>"}]}